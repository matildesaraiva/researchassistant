{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c4b3db2-13fb-42d0-9eee-86cce2e031b5",
   "metadata": {},
   "source": [
    "Data Collection and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "327de972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c32db80-a5e4-478b-835d-e92e7ab511f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1829268383.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    add python-dotenv\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import PyPDF2\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "347674e0-e31d-4403-877b-4336625973d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-xGZoo8wC4Y2cwJjKUExwUUY0cgD8e572StmHJOW_DJT3BlbkFJy4nN2oTfGVJPjkxgv5z0ImGLIjJzFlOBHDhaLfy2oA\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the OpenAI API key\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "print(openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "23d6d2cc-3bd7-4302-b33a-871dc2df31d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Sequence' from 'typing_extensions' (c:\\users\\lenovo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\typing_extensions.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\lenovo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\openai\\__init__.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_os\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m override\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_types\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NOT_GIVEN, NoneType, NotGiven, Transport, ProxiesTypes\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m file_from_path\n",
      "File \u001b[1;32mc:\\users\\lenovo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\openai\\types\\__init__.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Batch \u001b[38;5;28;01mas\u001b[39;00m Batch\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image \u001b[38;5;28;01mas\u001b[39;00m Image\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model \u001b[38;5;28;01mas\u001b[39;00m Model\n",
      "File \u001b[1;32mc:\\users\\lenovo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\openai\\types\\batch.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Optional\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_error\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchError\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_request_counts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchRequestCounts\n",
      "File \u001b[1;32mc:\\users\\lenovo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\openai\\_models.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Any, Type, Tuple, Union, Generic, TypeVar, Callable, Optional, cast\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m date, datetime\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      8\u001b[0m     Unpack,\n\u001b[0;32m      9\u001b[0m     Literal,\n\u001b[0;32m     10\u001b[0m     ClassVar,\n\u001b[0;32m     11\u001b[0m     Protocol,\n\u001b[0;32m     12\u001b[0m     Required,\n\u001b[0;32m     13\u001b[0m     Sequence,\n\u001b[0;32m     14\u001b[0m     ParamSpec,\n\u001b[0;32m     15\u001b[0m     TypedDict,\n\u001b[0;32m     16\u001b[0m     TypeGuard,\n\u001b[0;32m     17\u001b[0m     final,\n\u001b[0;32m     18\u001b[0m     override,\n\u001b[0;32m     19\u001b[0m     runtime_checkable,\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerics\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Sequence' from 'typing_extensions' (c:\\users\\lenovo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\typing_extensions.py)"
     ]
    }
   ],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f6300bef-7011-4048-a0ac-121cc3e20648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted from Automated Road Extraction From High Resolution Satellite Images.pdf:  Procedia Technology   24  ( 2016 )  1460 – 1467 Available online at www.sciencedirect.com\n",
      "ScienceDirect\n",
      "2212-0173 © 2016 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license \n",
      "(http://creativecommons.org/licenses/by-nc-nd/4.0/).\n",
      "Peer\n",
      "-review under responsibility of the organizing committee of ICETEST – 2015\n",
      "doi: 10.1016/j.protcy.2016.05.180 \n",
      "International Conference on Emerging Trends in Engineering, Science and Technology \n",
      "(ICETEST - 2015) \n",
      "Automated Road Extraction  From High Resolution Satellite Images  \n",
      "Jose Hormesea,*, Dr. C. Saravananb   \n",
      "aResearch  Scholar, Dept  of Computer Centre , NIT, Durgapur ,713209,  India  \n",
      "bResearch Guide, Dept of Computer  Centre,  NIT, Durgapur,713209,  India  \n",
      "Abstract  \n",
      "The importance of road extraction from satellite images arises from the fact that it greatly enhances the efficiency of map \n",
      "generation and thus can be a big help in car navigations systems or any emergency (rescue) system that needs instant maps. \n",
      "Therefore, increasing research is being dedicated and focused on the development of efficient methods to extract topographical \n",
      "meaningful features (like roads) from digital remote sensed im ages. The work deals with extraction of roads from satellite \n",
      "images. This is a challenging domain compared to extraction fr om aerial images as satellite images are noisy and of lower \n",
      "resolution.  In this method, a Vectorization Approach for the automatic method of road extraction is being used where the image  \n",
      "is segmented to identify the road network regions followed by a d ecision making and continuity procedure to correctly detect the \n",
      "roads and the Vectorization step to identify the line segments or curved segments which represents the road. This method may be  \n",
      "employed for obtaining information for feeding large -scale Geographic Information System. In the automatic method of road \n",
      "ex\n",
      "traction the extracted roads are converted into road vectors in order to use these vector road maps in GIS. A semi -automated \n",
      "sc\n",
      "heme is used for scenarios where fully automated system fails. A combination of both methods can be devised for a full \n",
      "fledged real business scenario  \n",
      "© 2016 The Authors.  Published by E lsevier Ltd. \n",
      "Peer-review under responsibility of the organizing committee of ICETEST – 2015 . \n",
      "Keywords: Remote Sensed Images; Vectorization, Geographic Information System  \n",
      "1. Introduction  \n",
      "Satellite imagery  consists of photographs of Earth  or other planets made by means of artificial satellites. Satellite \n",
      "i\n",
      "mages have many applications in agriculture, geology, fo restry, biodiversity conservation, regional planning, \n",
      "education, intelligence  and warfare. Images can be in visible colours and in other spectra.  \n",
      " \n",
      "* Corresponding author. Tel : +919495064249  \n",
      "E-mail address: josehormese@gmail.com  \n",
      " \n",
      "© 2016 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license \n",
      "(http://creativecommons.org/licenses/by-nc-nd/4.0/).\n",
      "Peer\n",
      "-review under responsibility of the organizing committee of ICETEST – 20151461  Jose Hormese and C. Saravanan  /  Procedia Technology   24  ( 2016 )  1460 – 1467 \n",
      "The rapid  development  of sensor  tech nology  has enabled  higher  resolutions  for the remote  sensed  images namely \n",
      "th\n",
      "e satellite images  (e.g. Quick -Bird images  have  ground  resolution  of 0.6m).  Extensive  investigations  have  been  \n",
      "conducted  in the past two decades  to reliably  extract features  like roads from  these highly  accurate  images. Satellite \n",
      "imagery can be combined with vector or raster data in a GIS  provided that the imagery has been spatially rectified so \n",
      "th\n",
      "at it will properly align with other data sets.  \n",
      "To identify  roads from  high resolution  remote  sensing  images  and to distinguish  it with other  objects  like \n",
      "buildings,  rivers  and woods,  the color  information,  usually  in four or more  spectral  bands  can be used as an \n",
      "important  feature.  Between  the diversity  of methods,  the decision  to choose  one or other  one depends  on the balance  \n",
      "between  speed,  accuracy  and complexity  of the computer  algorithm.  Even  more,  such expected  accuracy  can be \n",
      "related  to the quality  of input  data,  in terms of resolution  of the digital  image.  The satellite images  can be \n",
      "represented  as raster  images  and digital raster  images  can be class ified as portrayals  of scenes,  with imperfect  \n",
      "renditions  of objects.  Imperfections in an imag e result  from  the imaging  system,  signal  noise,  atmospheric  scatter  \n",
      "and shadows.  Thus,  the task of identifying  and extracting  the desired  information  or features  from  a raster  image  is \n",
      "based  on a criteria  developed  to determine  a particular  feature  (based  on its characteristics  within  any raster  image),  \n",
      "while  ignoring  the presence  of other  features  and imperfections in the image.  Automatic  methods  of extraction  are \n",
      "more  complex  than Semi- automatic  methods  of extraction.  Automatic  methods  of extraction  require  ancillary  \n",
      "information,  as compared  to Semi -automatic  methods  that extract roads based  on information  from  the input  image.  \n",
      "2. Rela ted Works  \n",
      "Semi automated  and automated road  extraction from satellite images  can save time and labour to a great  degree  \n",
      "in updating road spatial  database.  Various road extraction  approaches  have  been  developed.  An Integrated  Method  \n",
      "for Urban  Main  Road  Centerline  Extraction  is based  on spatial  class ification  to segment  the images  based  on road \n",
      "and non road groups  [1]. A multistage  framework  was designed  to extract road networks  based  on probabilistic  \n",
      "SVMs and salient  features  [2]. Pixel  based  methods  can be based  to class ify road detection  where Edge  Detecto rs \n",
      "can be used to extract  potential  road points  [3]. In the  statistical  inference  method, linear  features  are modelled  as a \n",
      "Markov  point  process  or a geometric -stochastic  model  on the road width,  direction,  intensity  and background  \n",
      "intensity  and maximum  a posteriori  probability  is used to estimate  the road network  [4]. Besides  these, active  \n",
      "contour  models,  known  as snakes,  are also used in semi  automatic  road extraction  [5].  \n",
      "The semi automatic  methods  of road extraction  require  some  road seeds  as starting  points,  which  are in general  \n",
      "provided  by users  and road segments  evolve  under  a certain  model.  Further, these  methods  use black -and-white  \n",
      "aerial  photographs  or the panchromatic  band  of high-resolution  satellite images  and therefore  the geometric  \n",
      "characteristics  of the roads alo ne play a critical  role. Various road extraction  algo rithms  have  been  proposed  over the \n",
      "past decades.  Mena  [6] and Das et al.  presented  overviews  of road detection  methods  in this area. Quackenbush  [7] \n",
      "gave  a review  of linear feature  extraction from imagery  which  can be used for road extraction.  Automatic road \n",
      "detection  method  tests were  devised  by Mayer  et al. [8].  Based  on the level of road knowledge  used,  Poullis  and \n",
      "You [9]  class ified road detection  methods  into three  categories:  1) pixel -based;  2) region -based;  and 3) knowledge -\n",
      "based.  The pixel -based  methods  depend  on the information obtained from the  pixels.  Line [10],  [11],  and ridge  [12],  \n",
      "[13] detectors  are used to extract  potential  road points.  Road  points are then  connected  to produce  road segments  \n",
      "and also used as input to a higher  level processing  phase.   \n",
      "Road  extraction  methods  have been  proposed  by several  authors  from different  viewpoints.  Based  on \n",
      "homogeneous  polygonal  areas around  each pixel,  Hu et al. [14]  defined  the pixel  footprint to extract  road areas.  \n",
      "Similarly,  Zhang  et al. [15]  applied this detector  to extract  roads in urban  areas.  Movaghati  et al. [16] applied  \n",
      "particle  filtering  and Kalman filtering  to extract road networks . Road  intersection  extraction  from  remotely  sensed \n",
      "im\n",
      "ages was also studied  [17]–[19].  Although  road intersection extraction  alone cannot  be a complete  road network  \n",
      "generation, it  is useful  for understanding  road network  topologies  and higher  processing.  \n",
      "In the semi automatic method the active contour model is being employed for  the extraction of roads.  In the active \n",
      "co\n",
      "ntour model initial points (seed points) have to be specified from which the roads are detected. Depending on the \n",
      "length of the road to be extracted appropriate seed points ha ve to be g iven. The idea is  to plot the active contour \n",
      "between the seed points. After filtering the image with a Gau ssian filter, the potential of the image is being computed \n",
      "and it illustrates the influence of the edges of the informati on that has to be extracted. Even when the points don’t 1462   Jose Hormese and C. Saravanan  /  Procedia Technology   24  ( 2016 )  1460 – 1467 \n",
      "fall in the centre line of the road, they help to the program to get an idea of the direc tion. So, data between them can \n",
      "be interpolated. Since new initialization snake points are being interpo lated , when the distances between the points \n",
      "are larger than an amount of pixels, the order that the user initialize the points is impor tant. This is because it is \n",
      "depended on the direction and distance between these gi ven points to interpolate new points between them. So, the \n",
      "Active Contour algorithm  is implemented in a consecutive sequence such that the total segment of the road, within \n",
      "th\n",
      "e points selected by the user, will be extracted. The first s tep is to specify the seed po ints along the road to be \n",
      "extracted from a given satellite image. The next step  is to find the Gradien t and Potential of the satel lite image which \n",
      "h\n",
      "elps in identifying the roads edges. Then the contours are identified through interpolation based on  the seed points \n",
      "given by the user. After the contours are identified the next step is to evolve the active contour (snake) along the \n",
      "seed points specified by the user which represents the road surface.  \n",
      "3. Advantages  and D isadvantages  of the Present  Works  \n",
      " In the semi  automatic  extraction tec hniques  once  the initial  points  or seed points  are given  which  gives  a \n",
      "preliminary  approximation  of the feature  and hence  feature  extraction  can be done  more  accurately.  So it gives  \n",
      "better  results  for any type of satellite  images.  This saves time and human effort in extracting roads from images. The  \n",
      "draw back is that manual  intervention  is needed  initially  to perform  the computation.  Approximations  have  to be \n",
      "provided  by the operator.  If the points  are not given  the Interpolation  Routine  can generate  points  out of track. With  \n",
      "high quality  images  object  identification and feature  extraction  can be done  effectively.  It helps  in better  decision -\n",
      "makings  for Urban  Planning,  Traffic  Management  and Vehicle  Nav igation.  The Disadvantage  is that the output  is \n",
      "purely  dependent  on the Resolution  of the images,  Input  road characteristics  and variations.   \n",
      "A given input satel lite image pertaining to an ordinary area is shown as follows:  \n",
      " \n",
      " \n",
      " \n",
      "Fig.1. Input Image  \n",
      " \n",
      "Using the Semi automatic method the Gradient of sm oot hed image and potential is obtained as follows:  \n",
      " \n",
      " \n",
      "                                                                                              \n",
      "Fig.2. Gradient and Potential  \n",
      " \n",
      "After the Gradient and Potential are obtained, th e Active Contour operation is applied.  \n",
      "  1463  Jose Hormese and C. Saravanan  /  Procedia Technology   24  ( 2016 )  1460 – 1467 \n",
      " \n",
      "                                                                            \n",
      "Fig.3. Active Contour  \n",
      " \n",
      "In the Semi Automatic approach the Active Contour detects th e road from the given initial seed points.  \n",
      " \n",
      "4. Vectorization Approach to Road Extraction  \n",
      " \n",
      "With the increasing resolution of remote sensing imag es,  road network can be displayed as continuous and \n",
      "homogeneity regions with a certain width rather than trad itional thin lines. Therefore, road network extraction from \n",
      "large scale images refers to reliable road  su rface detection which identifies ro ad segments from the RS Images. A \n",
      "novel automatic road network d etectio n approach based on the combination of segmentation and vectorization is \n",
      "explain\n",
      "ed, which includes three main steps:  (i) the image is segmented to roughly  identify the road network regions; \n",
      "(ii) the decision making and continuity procedure to co rrectly detect the roads and ( iii) the Vectorization step to \n",
      "identify the line segments or curved segments which repr esent the roads segment. Lastly, the results from QuickBird \n",
      "images demonstrate the correctness and efficiency of the proposed process. These steps can be explained using the \n",
      "following flow diagram as shown : \n",
      " \n",
      " \n",
      " \n",
      "Fig.4. Automated Road Extraction Flow Diagram  \n",
      " \n",
      "The Proposal is to develop  an automatic  method  for road extraction from high resolution satellite images. In  the \n",
      "vectorization approach  of road extraction  no seed points  have  to be given.  The vectorization approach is an \n",
      "auto\n",
      "matic method in extracting road segments from satel lite images. The method adopted is to identify the road \n",
      "segments which are represented as continuous line segments as the  road could be of any arbitrary shape. The start \n",
      "and end poin\n",
      "ts of each line segment is  identified and the road segments in the image are correctly extracted. The  \n",
      "first stage  is to identify  the road network  regions  using  segmentation.  Then  a decision  makin g and continuity  \n",
      "procedure  is being  performed  in order  to correctly  detect the road.  As a road  may be of different shapes like straight  \n",
      "roads  or curved  ones the vectorization step identifies the line segment  which  corresponds  to the road.  The method  1464   Jose Hormese and C. Saravanan  /  Procedia Technology   24  ( 2016 )  1460 – 1467 \n",
      "automatically  identifies the road segments in a high  resolution  satellite image  where  the output  is highly  dependent  \n",
      "on the image.  \n",
      "The first stage of the automatic method is to classify t he road from the given satellite image using segmentation \n",
      "which consist of two stages namely the Null Gradient  Orientation method and the Edge Detection method. The Null \n",
      "Gradient Orientation method finds out the gradient at all pixels and then the eigen transform is being performed on \n",
      "the tensor and the result is such that there  will be two eigen values and two eigen  v ectors. The method is to choose \n",
      "those pixels for which at least one eigen value has a minimum which corresponds to a road. And also the Eigen  \n",
      "vector corresponding to minimum Eigen  value gives the direction of road. The n ext stage is the edge detection using \n",
      "the Canny Operator which identifies the edges which shows superior results compared to other edge detection \n",
      "methods. Also two threshold values are fine tuned so that the small connected elements are rejected and only the \n",
      "large connected areas are chosen.  In th e second stage the task is to d etect the line segments and the Hough \n",
      "Transform is being used to detect th e line segments. The final stage is th e vectorization step where the road \n",
      "segments are clearly identified from the High Resolution Satellite Image.  \n",
      " \n",
      "4.1. Image Segmentation  \n",
      "In computer vision, segmentation  refers to the process of partitioning a digital image  into multiple segments  (sets \n",
      "of pixels, also known as super -pixels). The goal of segmentation is to simplify and/or change the representation of \n",
      "an i\n",
      "mage into something that is more meaningful and ea sier to analyze. Image segmen tation is typically used to \n",
      "locate objects and boundaries (lines, curv es, etc.) in images. More precisely, im age segmentation is the process of \n",
      "assigning a label to every pixel in an image such that pix els with the same label share certain visual charac teristics.  \n",
      "The result of image segmentation is a set of  segments that collectively cover th e entire image, or a set of contours  \n",
      "extracted from the image (see edge detection). Each of th e pix els in a region is similar with respect to some \n",
      "characteristic or computed property, such as color, intensity, or texture. Adjacent  regions are significantly different \n",
      "w\n",
      "ith respect to the same characteristics . \n",
      "4.2 Null Gradient Orientation  \n",
      "The Gradient of a functio n of two variables F(x,y) can be defined as  \n",
      "jyFixF\n",
      "ww\u000eww                                                                                                                                        (1 ) \n",
      "The above equation can be collected as a collection of vectors poi n ting in the direction of increasing values of F.  \n",
      "The numerical gradients (differences) can be comp u ted for functions with any number of variables.  For a Function \n",
      "of\n",
      " N variables F(x,y,z,…) the gradient can be computed as  \n",
      "  ...\u000eww\u000eww\u000eww  kzFjyFix\n",
      "                                                                                                                   (2)      \n",
      " \n",
      "The description for the above equations can be given as : FX = gradient  (F) where F is a vector returns the one -\n",
      "dimensional numerical gradient of F. FX corresponds to wF/wx, the differences in the x  (column) direction. FY \n",
      "co\n",
      "rresponds to wF/wy, the differences in the y  (row) direction. The spacing between points in each direction is \n",
      "assumed to be one.  [FX,FY,FZ,...] = gradient(F) where F has N dimensions returns the N components of the \n",
      "g\n",
      "radient of F. There are two ways to cont rol the spacing betwee n values in F:    \n",
      "A single spacing value, h, specif ies the spacing between points in ever y direction. N spacing values (h1,h2,...) \n",
      "specifies the spacing for each dime nsion of F. Scalar spacing parameters sp ecify a constant spacing for each 1465  Jose Hormese and C. Saravanan  /  Procedia Technology   24  ( 2016 )  1460 – 1467 \n",
      "dimension. Vector parameters specify the coordinates of th e values along corresponding dimensions of F. In this \n",
      "case, the length of the vector must match the size of the corresponding dimension. In vector calculus, the gradient of \n",
      "a scalar f\n",
      "ield is a vector field  which points in the direction of the greatest rate of  increase of the scalar field and \n",
      "whose magnitude  is the greatest rate of change.  The property of Road is that there is continuity in  one direction. So \n",
      "th\n",
      "e method adopted is to find the Gradient at all pixels. The Gradient is then repr esented as a Tensor. Eigen \n",
      "Transforms are being performed on the Tensor which consists  of two eigen values and two eigen vectors. The idea is \n",
      "to choose those pixels for which at -leas t one eigen value has a minimum value which corresponds to a road. Also \n",
      "th\n",
      "e eigen vector corresponding to minimum eigen value gives the direction of road.  \n",
      " \n",
      "4.3 Edge D etectio n  \n",
      " \n",
      "Edge detection  is a fundamental tool in image processing  and computer vision, particularly in the areas of feature \n",
      "detection  and feature extraction, which aim at identifying points in a digital image  at which the image brightness  \n",
      "changes sharply or more formally has discontinuities.  In the ideal case, the result of ap ply ing an edge detector to an \n",
      "image may lead to a set of connected curves that in dicate the boundaries of objects, the boundaries of surface \n",
      "markings as well as curves that correspond to discontinuities  in surface orientation. Thus, applying an edge detection \n",
      "algorithm to an image may significantly reduce the amount of data to be processed  and may therefore filter out \n",
      "information that may be regarded as less relevan t, while preserving the important structural proper ties of an image. \n",
      "If\n",
      " the edge detection step is successful, the subsequent tas k of interpreting the information contents in the original \n",
      "image may therefore be substantially simplified. However, it is not always possible to ob tain such ideal edges from \n",
      "real life images of moderate complexity. Edges extracted from non -trivial images are often hampered by \n",
      "f\n",
      "ragmentation, meaning that the edge curves are not connected, missing edge segments as well as false edges  not \n",
      "co\n",
      "rresponding to interesting phenomena in the image – thus complicating the subsequent task of interpreting the \n",
      "i\n",
      "mage data.   \n",
      "The edge detection algorithm being em plo yed is the Canny Edge Detection Algorithm which consists of multi \n",
      "stages.   \n",
      "The algorithm runs in 5 separate steps:  \n",
      "x Smoothing: Blurring of the image to remove noise.  \n",
      "x Finding gradients: The edges should be marked where th e gradients of the image has large magnitudes.  \n",
      "x Non-maximum suppression: Only local maxima should be marked as edges.  \n",
      "x Double thresholding: Potential edges are determined by thresholding.  \n",
      "x Edge tracking by hysteresis: Final ed ges are determined by suppressing all ed ges that are not connected to a very \n",
      "certain (strong) edge.  \n",
      " \n",
      "4.4 Decision Making based on Continuity  \n",
      " \n",
      "A lot of edges are proved not to  be roa ds through the procedure  of edge detection. Therefore road following or \n",
      "track\n",
      "ing is one of the most important steps in road detec tion. The major goal of road tracking is to eliminate road -\n",
      "like but non -road pixels. Hough Transforms are being us e d to perform this step. In automated  analysis of digital \n",
      "i\n",
      "mages, a sub -problem often arises of detecting si mple shapes, such as straight lines, circles or ellipses. In many \n",
      "cases an edge detector  can be used as a  pre-processing stage to obtain image poin ts or image pixels that are on the \n",
      "desired curve in the image space. Due to  imperfections in either the image data or the edge detector, however, there \n",
      "may be missing points or pixels on the desired curves as well as spatial deviations between the ideal \n",
      "line/circle/e llipse and the noisy edge points as they are obtained f rom the edge detector. For these reasons, it is often \n",
      "non-trivial to group the extracted edge features to an appr op riate set of lines, circles or ellipses. The purpose of the \n",
      "Hough transform is to address this problem by making it possible to perform groupings of edge points into object \n",
      "candidates by performing an explicit voting proced ure over a set of parameterized image objects.  \n",
      " \n",
      "4.5 Vector Representation of Roads  1466   Jose Hormese and C. Saravanan  /  Procedia Technology   24  ( 2016 )  1460 – 1467 \n",
      "Vector graphics  is the use of geometrical primitives  such as point s, lines, curves, and shapes or polygon(s), which \n",
      "are all based on\n",
      " mathematical equations, to represent images  in computer graphics. Vector graphics formats are \n",
      "co\n",
      "mplementary to raster graphics, which is the re presentation of images as an array of pixels, as it is typically used \n",
      "f\n",
      "or the representation of photographic images. Computer disp lays are made up from grids of small rectangular cells \n",
      "called pixels. The picture is built up from these cells.  The smaller and closer the cell s a re together, the better the \n",
      "quality of the image, but the bigger the file needed to store th e data. If the number of pixels is kept constant, the size \n",
      "of each pixel will grow and the image becomes grainy (pixellated) when  magnified, as the resolution of the eye \n",
      "enab\n",
      "les it to pick out individual pixels.  \n",
      "Vector graphics files store the lines, shapes and colours t hat make up an image as mathematical formulae. A \n",
      "vector graphics program uses these mathematical formulae to  construct the screen image, building the best quality \n",
      "image possible, given the screen resolution. The mathemati cal formulae determine where the dots that make up the \n",
      "image should be placed for the best results when displaying the image. Si nce these formulae can produce an image \n",
      "scalable to any size and detail, the quality of the image is li mited only by the resolution of the display, and the file \n",
      "size of vector data generating the image stays the same. Printing the image to paper will usually give a sharper, \n",
      "higher resolution output than prin ting it to the screen but can use exactly the same vector data file.  \n",
      " \n",
      " \n",
      "5. Results  \n",
      " \n",
      "The vectorization approach is an automatic method in  extracti ng road segments from satellite images. The \n",
      "method adopted is to identify the road segments which are represented as continu ous line segments as the road could \n",
      "be of any arbitrary shape. The start and end points of each  line segment is identified and the road segments in the \n",
      "image are correctly extracted.  \n",
      "For the above given high resolution satellite image, the output obtained is as follows : \n",
      "                                                                \n",
      " \n",
      " \n",
      "Fig.5. Output Image  \n",
      " \n",
      "The output obtained shows that the image is a tile from mapped data set. The method successfully extracted the \n",
      "road and represented it with just a single line segment. The method gave the correct results as there were no \n",
      "occlusions in the road . \n",
      " \n",
      "6. Conclusion  \n",
      " \n",
      "In the automatic method of road extraction no seed poi n ts have to be given. The method automatically identifies \n",
      "the road segments in a high resolution satellite image wher e the output is highly dependent on the image. This \n",
      "method is more suited in rural areas than in urban areas where man -made objects are less and it is poss ible to detect \n",
      "the roads more easily. Through the segmentation, decision  making based on continuity and vectorization procedure \n",
      "the raster satellite images can be converted to vector repr esentation and it is possible to extract roads from satellite \n",
      "images. In the case of complex road structures and also in  the case of occlusions the semi automated method gave \n",
      "better results than an automated method. The significance  of the automated method is that human labour can be \n",
      "minimized to a very large extent. F or a real large scale road extraction  work, a combination of both methods is being \n",
      "prop\n",
      "osed. The first stage employs the u tility of the automatic method where the road segments are identified and for 1467  Jose Hormese and C. Saravanan  /  Procedia Technology   24  ( 2016 )  1460 – 1467 \n",
      "identifying the missing parts of the road the semi automatic method is being employed. The combination of both the \n",
      "methods will save time as well as reduce human labour to a very large extent . \n",
      " \n",
      "References  \n",
      "[1] Wenzhong  Shi, Zelang  Miao,  and Johan  Debayle,  “ An Integrated  Method  for Urban  Main  Road  Centerline  Extraction  from  Optical  Remote  \n",
      "Sensed  Imagery ”, IEEE  Transactions  on Geo-science and Remote  Sensing,  Vol. 52, No. 6, June 2014.  \n",
      "[2] S. Das, T. T. Mirnalinee,  and K. Varghese,  “Use of salient features  for the design  of a multistage  framework  to extract  roads  from  high \n",
      "resolution  multispectral  satellite  images, ” I EEE  Trans.  Geosci.  Remote  Sens.,  vol. 49, no. 10, pp. 3906 –3931,  Oct. 2011.  \n",
      "[3] C. Unsalan  and B. Sirmacek,  “Road  network  detection  using  probabilistic  and graph  theoretical  methods, ” I EEE  Trans.  Geosci.  Remote  Sens.,  \n",
      "vol. 50, no. 11, pp. 4441– 4453,  Nov.  2012.  \n",
      "[4] Barzohar,  Mein,  Cooper,  David  B. “Automatic  finding  of main  roads  in aerial  images  by using  geometric  - stochastic  models and estimation ”, \n",
      "IEEE  Computer  Vision  and Pattern  Recognition , pp. 459-464, 1993.  \n",
      "[5] Grün,  A., Li, H., “Semi-automatic  linear  feature  extraction  by dynamic  programming  and LSB-snakes ”, Photogrammetric  Engineering  & \n",
      "Remote Sensing , Vol. 63, No. 8, pp. 985-995, 1997.  \n",
      "[6] J. B. Mena,  “State  of the art on automatic  road extraction  for GIS update: A  novel  classification, ” P attern  Recognit.  Lett.,  vol. 24, no. 16, pp. \n",
      "3037 – 3058,  Dec. 2003.  \n",
      "[7] L. J. Quackenbush,  “A review  of techniques  for extracting  linear  features  from  imagery, ” Ph otogramm.  Eng. Remote  Sens.,  vol. 70, no. 12, \n",
      "pp\n",
      ". 1383 –1392,  Dec. 2004.  \n",
      "[8] H. Mayer,  S. Hinz,  U. Bacher,  and E. Baltsavias,  “A test of automatic  road extraction  approaches, ” In t. Archives  Photogramm.,  Remote  Sens.,  \n",
      "Spatial Inf.  Sci., vol. 36, no. 3, pp. 209–214, 2006.  \n",
      "[9] C. Poullis  and S. You,  “Delineation  and geometric  modeling  of road networks, ” IS PRS J. Photogramm.  Remote  Sens.,  vol. 65, no. 2, pp. 165–  \n",
      "181, Mar.  2010.  \n",
      "[10] A. Gruen  and H. Li, “Semi -automatic  linear  feature  extraction  by dynamic programming  and LSB-snakes, ” Ph otogramm.  Eng. Remote  \n",
      "Sens.,  vol. 63, no.  8, pp. 985–995, Aug.  1997.  \n",
      "[11] F. Dell’Acqua  and P. Gamba,  “Detection  of urban  structures  in SAR images  by robust  fuzzy clustering  algorithms:  The example  of street  \n",
      "tracking, ” I EEE  Trans.  Geosci.  Remote  Sens.,  vol. 39, no. 10, pp. 2287 –2297, Oct.  2001.  \n",
      "[12] R. Nevatia  and K. Babu, “Linear  feature  extraction  and description, ”Comput.  Graph.  Image  Process.,  vol. 13, no. 3, pp. 257–269, Jul. 1980.  \n",
      "[13] K. Treash  and K. Amaratunga,  “Automatic  road detection  in gray scale aerial  images, ” AS CE J. Comput.  Civil Eng.,  vol. 14, no. 1, pp. 60–\n",
      "69, 2000.  \n",
      "[14] J. Hu, A. Razdan,  J. C. Femiani,  M. Cui, and P. Wonka,  “Road  network extraction and  intersection  detection  from  aerial  images by tracking  \n",
      "road footprints, ” I EEE Trans.  Geosci.  Remote  Sens.,  vol. 45, no. 12, pp. 4144 –4157,  Dec. 2007.  \n",
      "[15] J. Zhang,  X. Lin, Z. Liu, and J. Shen,  “Semi -automatic  road tracking by  template  matching  and distance  transformation  in urban  areas, ” In t.  \n",
      "J. Remote  Sens.,  vol. 32, no. 23, pp. 8331– 8347,  Dec. 2011.  \n",
      "[16] S. Movaghati,  A. Moghaddamjoo,  and A. Tavakoli,  “Road  extraction  from  satellite  images using  particle  filtering  and extended  Kalman  \n",
      "filtering, ” I EEE  Trans.  Geosci.  Remote  Sens.,  vol. 48, no. 7, pp. 2807 –2817,  Jul. 2010.  \n",
      "[17] A. Barsi  and C. Heipke,  “Artificial  neural  networks  for the detection  of road junctions  in aerial  images, ” IS PRS  Arch.,  vol. 34, pt. 3/W8,  pp. \n",
      "113–118, Sep. 2003.  \n",
      "[18] M. Ravanbakhsh,  C. Heipke,  and K. Pakzad,  “Road  junction  extraction from  high resolution  aerial  imagery, ” Ph otogramm.  Rec.,  vol. 23, no. \n",
      "124, pp.  405–423, Dec. 2008. \n",
      "[19] J. J. Ruiz,  T. J. Rubio,  and M. A. Urena,  “Automatic  extraction  of road intersections  from  images  based  on texture  characterisation, ” Su rvey \n",
      "Rev., vol.  43, no. 321, pp. 212–225, Jul. 2011.  \n",
      "Extracted from Convolutional Neural Networks for Large-Scale Remote Sensing Image Classification.pdf: HAL Id: hal-01369906\n",
      "https://inria.hal.science/hal-01369906\n",
      "Submitted on 13 Mar 2017\n",
      "HAL is a multi-disciplinary open access\n",
      "archive for the deposit and dissemination of sci-\n",
      "entific research documents, whether they are pub-\n",
      "lished or not. The documents may come from\n",
      "teaching and research institutions in F rance or\n",
      "abroad, or from public or private research centers.L’archive ouverte pluridisciplinaire HAL , est\n",
      "destinée au dépôt et à la diffusion de documents\n",
      "scientifiques de niveau recherche, publiés ou non,\n",
      "émanant des établissements d’enseignement et de\n",
      "recherche français ou étrangers, des laboratoires\n",
      "publics ou privés.\n",
      "Convolutional Neural Networks for Large-Scale Remote\n",
      "Sensing Image Classification\n",
      "Emmanuel Maggiori, Y uliya T arabalka, Guillaume Charpiat, Pierre Alliez\n",
      "T o cite this version:\n",
      "Emmanuel Maggiori, Y uliya T arabalka, Guillaume Charpiat, Pierre Alliez. Convolutional Neural\n",
      "Networks for Large-Scale Remote Sensing Image Classification. IEEE T ransactions on Geoscience and\n",
      "Remote Sensing, 2017, 55, pp.645-657. ￿10.1109/tgrs.2016.2612821￿. ￿hal-01369906￿IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 1\n",
      "Convolutional Neural Networks for Large-Scale\n",
      "Remote Sensing Image Classiﬁcation\n",
      "Emmanuel Maggiori, Student member, IEEE, Yuliya Tarabalka, Member, IEEE,\n",
      "Guillaume Charpiat, and Pierre Alliez\n",
      "Abstract —We propose an end-to-end framework for the dense,\n",
      "pixelwise classiﬁcation of satellite imagery with convolutional\n",
      "neural networks (CNNs). In our framework, CNNs are directly\n",
      "trained to produce classiﬁcation maps out of the input images.\n",
      "We ﬁrst devise a fully convolutional architecture and demonstrate\n",
      "its relevance to the dense classiﬁcation problem. We then address\n",
      "the issue of imperfect training data through a two-step training\n",
      "approach: CNNs are ﬁrst initialized by using a large amount\n",
      "of possibly inaccurate reference data, then reﬁned on a small\n",
      "amount of accurately labeled data. To complete our framework\n",
      "we design a multi-scale neuron module that alleviates the common\n",
      "trade-off between recognition and precise localization. A series\n",
      "of experiments show that our networks take into account a large\n",
      "amount of context to provide ﬁne-grained classiﬁcation maps.\n",
      "Index Terms —Classiﬁcation, satellite images, convolutional\n",
      "neural networks, deep learning.\n",
      "I. I NTRODUCTION\n",
      "THE ANALYSIS of remote sensing images is of\n",
      "paramount importance in many practical applications,\n",
      "such as precision agriculture and urban planning. Recent\n",
      "technological developments have signiﬁcantly increased the\n",
      "amount of available satellite imagery. Notably, the constel-\n",
      "lation of Pl ´eiades satellites produces high spatial resolution\n",
      "images that cover the whole Earth in less than a day. The\n",
      "large-scale nature of these datasets introduces new challenges\n",
      "in image analysis. In this paper we address the problem of\n",
      "pixelwise classiﬁcation of satellite imagery.\n",
      "There is a vast literature on classiﬁcation approaches that\n",
      "take into account the spectrum of every individual pixel to\n",
      "assign it to a certain class. Alternatively, more advanced\n",
      "techniques combine information from a few neighboring pixels\n",
      "to enhance the classiﬁers’ performance, often referred to as\n",
      "spectral-spatial classiﬁcation. These approaches rely on the\n",
      "separability of the different classes based on the spectrum\n",
      "of a single pixel or of some neighboring pixels. In a large-\n",
      "scale setting, however, these approaches are not effective.\n",
      "On the one hand, current large-scale satellite imagery does\n",
      "not use high spectral resolution sensors, making it difﬁcult\n",
      "to distinguish object classes solely by their spectrum. On\n",
      "the other hand, due to the large spatial extent covered by\n",
      "the datasets, classes have a considerable internal variability,\n",
      "which further challenges the class separability when simply\n",
      "E. Maggiori, Y . Tarabalka and P. Alliez are with Univerist ´e Cˆote d’Azur,\n",
      "TITANE team, Inria, 2004 Route des Lucioles, BP93 06902 Sophia Antipolis\n",
      "Cedex, France. E-mail: emmanuel.maggiori@inria.fr.\n",
      "G. Charpiat is with Tao team, Inria Saclay– ˆIle-de-France, LRI, B ˆat. 660,\n",
      "Universit Paris-Sud, 91405 Orsay Cedex, France.\n",
      "Manuscript received ...; revised ...observing the spectral signatures of a restricted neighborhood.\n",
      "We argue that a more thorough understanding of the context\n",
      "such as, e.g., the shape of objects, is required to aid the\n",
      "classiﬁcation process.\n",
      "Convolutional neural networks (CNNs) [1] are therefore\n",
      "gaining attention, due to their capability to automatically\n",
      "discover relevant contextual features in image categorization\n",
      "problems. CNNs consist of a stack of learned convolution\n",
      "ﬁlters that extract hierarchical contextual image features, and\n",
      "are a popular form of deep learning networks. They are already\n",
      "outperforming other approaches in various domains such as\n",
      "digit recognition [2] and natural image categorization [3].\n",
      "Our goal is to devise an end-to-end framework to clas-\n",
      "sify satellite imagery with CNNs. The context of large-scale\n",
      "satellite image classiﬁcation introduces certain challenges that\n",
      "we must address in order to turn CNNs into a relevant\n",
      "classiﬁcation tool. Notably, we must (1) design a speciﬁc\n",
      "neural network architecture for our problem, (2) acquire large-\n",
      "scale training data and handle its eventual inaccuracies, and\n",
      "(3) generate high-resolution output classiﬁcation maps.\n",
      "1) CNN architecture: CNNs are commonly used for image\n",
      "categorization , i.e., for assigning the entire image to a class\n",
      "(e.g., a digit [1] or an object category [3]). In remote sensing,\n",
      "the equivalent problem is to assign a category to an entire\n",
      "image patch, such as ‘residential’ or ‘agricultural’ area. Our\n",
      "context differs in that we wish to conduct a dense pixelwise\n",
      "labeling. We must thus design a CNN that outputs a per-pixel\n",
      "classiﬁcation and not just a category for the entire input.\n",
      "2) Imperfect training data: A sensitive point regarding\n",
      "CNNs is the amount of training data required to properly learn\n",
      "the network parameters. A large source of free-access maps\n",
      "is OpenStreetMap, a collaborative online mapping platform,\n",
      "but the availability of data is highly variable between areas.\n",
      "In some areas, the coverage is very limited or nonexistent,\n",
      "and an irregular misregistration is prevalent throughout the\n",
      "maps. As we focus on the large-scale application of CNNs for\n",
      "classiﬁcation, we must explore the use of imperfect training\n",
      "data in order to make our framework applicable to a wide\n",
      "range of geographic areas.\n",
      "3) High-resolution output: The power of CNNs to take a\n",
      "large context to conduct predictions comes at the price of\n",
      "losing resolution for the output. This is because some degree\n",
      "of downsampling of the feature maps along the network is\n",
      "required in order to increase the amount of context without\n",
      "an excessive number of learnable parameters. Such coarse\n",
      "resolution translates into a fuzzy aspect around object edgesIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 2\n",
      "and corners. One of our challenges is then to alleviate this\n",
      "trade-off.\n",
      "A. Related Work\n",
      "We now review classiﬁcation methods and the use of CNNs\n",
      "in remote sensing.\n",
      "In the context of spectral classiﬁcation, decision trees [4],\n",
      "artiﬁcial neural networks [5], [6] and support vector ma-\n",
      "chines [7] are some of the approaches that have been ex-\n",
      "plored, both for multispectral and hyperspectral image analy-\n",
      "sis. Spectral-spatial methods [8] use contextual information to\n",
      "regularize the classiﬁcation maps. Different approaches have\n",
      "been presented, for example, Liao et al. [9] sequentially apply\n",
      "morphological ﬁlters to model different kinds of structural\n",
      "information and Tarabalka et al. [10] model spatial interactions\n",
      "with a graphical model. Neural networks have also been used\n",
      "for spectral-spatial classiﬁcation. In this direction, Kurnaz\n",
      "et al. [11] use such network to classify the concatenated\n",
      "spectrum of pixels inside a sliding window, in order to label\n",
      "multispectral images. In a similar fashion, Lloyd et al. [12]\n",
      "compute a textural feature which is concatenated to the pixel\n",
      "spectrum vector, prior the a neural network classiﬁcation. Lu\n",
      "and Weng [13] provide a comprehensive survey on classiﬁca-\n",
      "tion methods.\n",
      "In remote sensing, CNNs have been used to individually\n",
      "classify the pixels of hyperspectral images. This was achieved\n",
      "by performing convolutions in the 1D domain of the spectrum\n",
      "of each pixel [14], [15], [16]. Alternatively, a spectral-spatial\n",
      "approach has been taken by convolving in the 1D ﬂattened\n",
      "spectrum vector of a group of adjacent pixels [17], [18].\n",
      "Note however that these approaches do not learn spatial\n",
      "contextual features such as the typical shape of the objects\n",
      "of a class. Recent works have incorporated convolutions on\n",
      "the spatial domain after extracting the principal components\n",
      "of the hyperspectral image [19], [20], [21], and the idea of\n",
      "reasoning at multiple spatial scales has also been exploited,\n",
      "notably for hyperspectral classiﬁcation [22], [23] and image\n",
      "segmentation [24]. Let us remark that convolutional neural\n",
      "networks have also been used for other remote sensing appli-\n",
      "cations, such as road tracking [25], object detection [26] and\n",
      "land use classiﬁcation [27], [28].\n",
      "Mnih [29] proposed a speciﬁc architecture to learn large-\n",
      "extent spatial contextual features for aerial image labeling.\n",
      "It is derived from common image categorization networks\n",
      "by increasing the output size of the ﬁnal layer. Instead of\n",
      "outputting a single value to indicate the category, the ﬁnal layer\n",
      "produces an entire dense classiﬁcation patch. This network\n",
      "successfully learns contextual spatial features to better distin-\n",
      "guish the object classes. However, this patchwise procedure\n",
      "has the disadvantage of introducing artifacts on the border of\n",
      "the classiﬁed patches. Moreover, the last layer of the network\n",
      "introduces an unnecessarily large number of parameters, ham-\n",
      "pering its efﬁciency.\n",
      "B. Contributions\n",
      "We now summarize our contributions to address the issues\n",
      "presented before and provide then a framework for satellite\n",
      "image classiﬁcation with CNNs.1) Fully convolutional architecture: We ﬁrst analyze the\n",
      "CNN architecture proposed by Mnih [29] and the fact that it\n",
      "has a fully connected layer, i.e., connected to allthe outputs of\n",
      "the previous layer, to produce the output classiﬁcation patches.\n",
      "We point out that this architectural decision hampers both its\n",
      "accuracy and efﬁciency.\n",
      "We then propose a new network architecture that is fully\n",
      "convolutional , i.e., that only involves a series of convolution\n",
      "and deconvolution operations to produce the output clas-\n",
      "siﬁcation maps. This architecture solves the issues of the\n",
      "previous patch-based approach by construction. While such a\n",
      "fully convolutional architecture imposes further restrictions to\n",
      "the neuronal connections than the fully connected approach,\n",
      "these restrictions reduce the number of trainable parameters\n",
      "without losing generality. It has been seen multiple times in\n",
      "the literature that reducing the number of parameters under\n",
      "sensible assumptions often implies a simpler error surface\n",
      "and helps reaching better local minima. For example, con-\n",
      "volutional networks have fewer connections than multi-layer\n",
      "perceptrons but perform better in practice for visual tasks [1],\n",
      "and Mnih [29] showed that adding too many layers to a\n",
      "network resulted in poorer results.\n",
      "We compare the fully convolutional vs fully connected\n",
      "approaches on a dataset of publicly available aerial color\n",
      "images over Massachusetts [29] created with the speciﬁc\n",
      "purpose of evaluating CNN architectures.\n",
      "2) Two-step training approach: To deal with the imper-\n",
      "fections in training data we propose a two-step approach.\n",
      "First, we train our fully convolutional neural network on raw\n",
      "OpenStreetMap data to discover the generalities of the dataset.\n",
      "Second, we ﬁne-tune the resulting neural networks for a few\n",
      "iterations under a small piece of manually labeled image. Our\n",
      "hypothesis is that, once the network is pre-trained on large\n",
      "amounts of imperfect data, we can boost its performance by\n",
      "“showing” it a small amount of accurate labels. Our approach\n",
      "is inspired by a common practice in deep learning: taking\n",
      "pre-trained networks designed to solve one problem and ﬁne-\n",
      "tuning them to another problem.\n",
      "3) Multi-scale architecture: We design a speciﬁc neuron\n",
      "module that processes its input at multiple scales, while\n",
      "keeping a low number of parameters. This alleviates the\n",
      "aforementioned trade-off between the amount of context taken\n",
      "and the resolution of the classiﬁcation maps. Our overall\n",
      "approach constitutes then an end-to-end framework for satellite\n",
      "image labeling with CNNs. We evaluate it on a Pl ´eiades image\n",
      "dataset over France, where the associated OpenStreetMap data\n",
      "is signiﬁcantly inaccurate.\n",
      "C. Organization of the Paper\n",
      "In the next section an introduction to convolutional neural\n",
      "networks is presented. In Section III the fully convolutional\n",
      "architecture is described and evaluated. Section IV presents\n",
      "the two-step training approach and the multi-scale architec-\n",
      "ture, in order to use CNNs as an end-to-end framework for\n",
      "satellite image classiﬁcation. Finally, conclusions are drawn\n",
      "in Section V.IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 3\n",
      "II. C ONVOLUTIONAL NEURAL NETWORKS\n",
      "In machine learning an artiﬁcial neural network is a system\n",
      "of interconnected neurons that pass messages to each other.\n",
      "Neural networks are used to model complex functions and, in\n",
      "particular, as frameworks for classiﬁcation. In this work we\n",
      "deal with the so-called feed-forward networks, whose graph\n",
      "of message passing between neurons is acyclic [30].\n",
      "An individual neuron takes a vector of inputs x=x1:::x n\n",
      "and performs a simple operation to produce an output a. The\n",
      "most common neuron is deﬁned as follows:\n",
      "a=\u001b(wx+b); (1)\n",
      "where xdenotes a weight vector, ba scalar known as bias and\n",
      "\u001ban activation function. The weight vectors and the biases are\n",
      "parameters that deﬁne the function computed by a network,\n",
      "and the goal of training is to ﬁnd the optimal values for\n",
      "these parameters. When using at least one layer of nonlinear\n",
      "activation functions, one can prove that a sufﬁciently large\n",
      "network can represent any function, suggesting the expres-\n",
      "sive power of neural networks. The most common activation\n",
      "functions are sigmoids, hyperbolic tangents and rectiﬁed linear\n",
      "units (ReLU) [3]. ReLUs are known to offer some practical\n",
      "advantages in the convergence of the training procedure.\n",
      "Even though any function can be represented by a sufﬁ-\n",
      "ciently large single layer of neurons, it is common to organize\n",
      "them in a set of stacked layers that transform the outputs of the\n",
      "previous layer and feed it to the next layer. This encourages\n",
      "the networks to learn hierarchical features, doing low-level\n",
      "reasoning in the ﬁrst layers and performing higher-level tasks\n",
      "in the last layers. For this reason, the ﬁrst and last layers are\n",
      "often referred to as lower and upper layers respectively.\n",
      "In an image categorization problem, the input of our net-\n",
      "work is an image (or a set of features derived from an image),\n",
      "and the goal is to predict the correct label associated with the\n",
      "image. Finding the optimal neural network classiﬁer reduces\n",
      "to ﬁnding the weights and biases that minimize a loss L\n",
      "between the predicted values and the target values in a training\n",
      "set. If there is a set Lof possible classes, the labels are\n",
      "typically encoded as a vector of length jLjwith value ‘1’\n",
      "at the position of the correct label and ‘0’ elsewhere. The\n",
      "network has then as many output neurons as possible labels. A\n",
      "softmax normalization is performed on top of the last layer to\n",
      "guarantee that the output is a probability distribution, i.e., the\n",
      "values for every label are between zero and one and add to\n",
      "one. The multi-label problem is then seen as a regression on\n",
      "the desired output label vectors.\n",
      "The loss function Lquantiﬁes the misclassiﬁcation by\n",
      "comparing the target label vectors y(i)and the predicted label\n",
      "vectors ^ y(i), forntraining samples i= 1:::n . In this work\n",
      "we use the common cross-entropy loss, deﬁned as:\n",
      "L=\u00001\n",
      "nnX\n",
      "i=1jLjX\n",
      "k=1y(i)\n",
      "klog ^y(i)\n",
      "k: (2)\n",
      "The cross-entropy loss has fast convergence rates when train-\n",
      "ing neural networks (compared with, for instance, the Eu-\n",
      "clidean distance between yand^ y) and is numerically stable\n",
      "when coupled with softmax normalization [30].Note that in the special case of binary labeling we can\n",
      "produce only one output (with targets ‘1’ for positive and\n",
      "‘0’ for negative). In this case a sigmoid normalization and\n",
      "cross-entropy loss are analogously used, albeit a multi-class\n",
      "framework can also be used for two classes.\n",
      "Once the loss function is deﬁned, the parameters (weights\n",
      "and biases) that minimize the loss must be solved for. Solving\n",
      "is achieved by gradient descent by computing the derivative\n",
      "@L\n",
      "@w iof the loss function with respect to every parameter wi,\n",
      "and updating the parameters with a learning rate \u0015as follows:\n",
      "wi wi+\u0015@L\n",
      "@wi: (3)\n",
      "The derivatives@L\n",
      "@w iare obtained by backpropagation , which\n",
      "consists in explicitly computing the derivatives of the loss with\n",
      "respect to the last layer’s parameters and using the chain rule\n",
      "to recursively compute the rest of the derivatives. In practice,\n",
      "learning is performed by stochastic gradient descent , i.e., by\n",
      "estimating the loss (2) on a small subset of the training set,\n",
      "referred to as a mini-batch.\n",
      "Despite the fact that neural networks can represent very\n",
      "complex functions, the epigraph of the loss function Lcan\n",
      "be highly non-convex, making the optimization difﬁcult via a\n",
      "gradient descent approach. To regularize this loss and improve\n",
      "training, convolutional neural networks (CNNs) [1] are a\n",
      "special type of neural networks that impose restrictions that\n",
      "make sense in the context of image processing. In these\n",
      "networks, every neuron is associated to a spatial location (i;j)\n",
      "with respect to the input image. The output aijassociated with\n",
      "location (i;j)is then computed as follows:\n",
      "aij=\u001b((W\u0003X)ij+b); (4)\n",
      "where Wdenotes a kernel with learned weights, Xthe input\n",
      "to the layer and ‘\u0003’ the convolution operation. Note that this\n",
      "is a special case of the neuron in Eq. 1 with the following\n",
      "constraints:\n",
      "\u000fThe connections only extend to a limited spatial neigh-\n",
      "borhood determined by the kernel size;\n",
      "\u000fThe same ﬁlter is applied to each location, guaranteeing\n",
      "translation invariance.\n",
      "Typically multiple convolution kernels are learned in every\n",
      "layer, interpreted as a set of spatial feature detectors. The\n",
      "responses to every learned ﬁlter are therefore known as a\n",
      "feature map .\n",
      "Departing from the traditional fully connected layer, in\n",
      "which every neuron is connected to all outputs of the previous\n",
      "layer, a convolutional layer dramatically reduces the number\n",
      "of parameters by enforcing the aforementioned constraints.\n",
      "This results in a regularized loss function, easier to optimize,\n",
      "without losing much generality.\n",
      "Note that the convolution kernels are actually three-\n",
      "dimensional because, in addition to their spatial extent, they go\n",
      "through all the feature maps in the previous layers, or through\n",
      "all the bands in the input image. Since the third dimension\n",
      "can be inferred from the previous layer it is rarely speciﬁed\n",
      "in architecture descriptions, only the two spatial dimensions\n",
      "being usually mentioned.IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 4\n",
      "In addition to convolutional layers, state-of-the-art networks\n",
      "such as Imagenet [3] involve some degree of downsampling,\n",
      "i.e., a reduction in the resolution of the feature maps. The\n",
      "goal of downsampling is to increase the so-called receptive\n",
      "ﬁeld of the neurons, which is the part of the input image\n",
      "that neurons can “see”. For the predictions to take into\n",
      "account a large spatial context, the upper layers should have\n",
      "a large receptive ﬁeld. This is achieved either by increasing\n",
      "the convolution kernel sizes or by downsampling feature\n",
      "maps to a lower resolution. The ﬁrst alternative increases the\n",
      "number of parameters and memory consumption, making the\n",
      "training and inference processes prohibitive. State-of-the-art\n",
      "CNNs tend then to keep the kernels small and add some\n",
      "degree of downsampling instead. This can be accomplished\n",
      "either by including pooling layers (e.g., taking the average or\n",
      "maximum of adjacent locations) or by introducing a so-called\n",
      "stride , which amounts to skip some convolutions through, e.g.,\n",
      "applying the ﬁlter once every four locations.\n",
      "Classiﬁcation networks typically contain a fully connected\n",
      "layer on top of the convolutions/pooling. This layer is designed\n",
      "to have as many outputs as labels, and produces the ﬁnal\n",
      "classiﬁcation scores.\n",
      "The overall success of CNNs lies mostly in the fact the\n",
      "the networks are forced by construction to learn hierarchical\n",
      "contextual translation-invariant features, which are particularly\n",
      "useful for image categorization.\n",
      "III. CNN S FOR DENSE CLASSIFICATION\n",
      "In this work we address the problem of dense classiﬁ-\n",
      "cation, i.e., not just the categorization of an entire image,\n",
      "but a full pixelwise labeling into the different categories. We\n",
      "ﬁrst describe an existing approach, the patch-based network,\n",
      "point out its limitations and propose a fully convolutional\n",
      "architecture that addresses these limitations. We restrict our\n",
      "experiments to the binary labeling problem for the building\n",
      "vsnot building classes, but our approach is extensible to\n",
      "an arbitrary number of classes following the formulation\n",
      "described in Section II.\n",
      "A. Patch-based Network\n",
      "To perform dense classiﬁcation of aerial imagery, Mnih\n",
      "proposed a patch-based convolutional neural network [29].\n",
      "Training and inference are performed patch-wise: the network\n",
      "takes as input a patch of an aerial image, and generates\n",
      "as output a classiﬁed patch. The output patch is smaller,\n",
      "and centered in the input patch, to take into account the\n",
      "surrounding context for more accurate predictions. The way to\n",
      "create dense predictions is to increase the number of outputs of\n",
      "the last fully connected classiﬁcation layer, in order to match\n",
      "the size of the target patch.\n",
      "Fig. 1(a) illustrates the patch-based architecture from [29].\n",
      "The network takes 64\u000264patches (on color images of 1m2\n",
      "spatial resolution) and predicts 16\u000216centered patches of the\n",
      "same resolution. Three convolutional layers learn 64, 112 and\n",
      "80 convolution kernels, of 12\u000212,4\u00024and3\u00023spatial\n",
      "dimensions, respectively. The ﬁrst convolution is strided (one\n",
      "(a) Color\n",
      " (b) Patch-based\n",
      " (c) FCN\n",
      "Fig. 2: The patch-based predictions exhibit artifacts on the\n",
      "patch borders while the FCN prevents them by construction.\n",
      "convolution every four pixels), which implies a downsampling\n",
      "with factor 4.\n",
      "After the three convolutional layers, a fully connected layer\n",
      "transforms the high-level features of the last convolutional\n",
      "layer into a classiﬁcation map of 256 elements, matching the\n",
      "required 16\u000216output patch.\n",
      "Training is performed by selecting random patches from the\n",
      "training set, and grouping them into mini-batches as required\n",
      "by the stochastic gradient descent algorithm.\n",
      "B. Limitations of the Patch-based Framework\n",
      "We now point out some limitations of the patch-based\n",
      "approach discussed above, which motivate the design of an\n",
      "improved network architecture. Let us ﬁrst analyze the role\n",
      "of the last fully connected layer that constructs the output\n",
      "patches. In the architecture of Fig. 1(a), the size of the feature\n",
      "maps in the last convolutional layer (before the last fully\n",
      "connected one) is 9\u00029. The resolution of these ﬁlters is 1/4\n",
      "of the resolution of the input image, due to the 4-stride in\n",
      "the ﬁrst convolution. The output of the fully connected layer\n",
      "is, however, a full-resolution 16\u000216classiﬁcation map. This\n",
      "means that the fully connected layer does not only compute\n",
      "the classiﬁcation scores, but also learns how to upsample\n",
      "them. Outputting a full-resolution patch is then the result of\n",
      "upsampling and not of an intrinsic high-resolution processing.\n",
      "We also observe that the fully connected layer allows outputs\n",
      "at different locations to have different weights with respect\n",
      "to the previous layer. For example, the weights associated\n",
      "to an output pixel at the top-left corner of a patch can be\n",
      "different to those of a pixel at the bottom right. In other\n",
      "words, the network can learn priors on the position inside\n",
      "a patch. This makes sense in some speciﬁc contexts such as\n",
      "when labeling pictures of outdoor scenes: the system could\n",
      "learn a prior for the sky to be at the top of the image. In\n",
      "our context, however, the partition of an image into patches\n",
      "is arbitrary, hence the “in-patch location” prior is irrelevant\n",
      "since allowing different weights at different patch locations\n",
      "may yield undesirable properties. For example, feeding two\n",
      "image patches that are identical but rotated by 90 degrees\n",
      "could yield different classiﬁcation maps.\n",
      "When training the network of Fig. 1(a) we expect that, after\n",
      "processing many training cases, the fully connected layer will\n",
      "end up learning a location-invariant function. Figs. 2(a)-(b)\n",
      "illustrate a fragment of an output score map by using suchIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 5\n",
      "(a) Patch-based\n",
      "(b) Fully convolutional ( 16\u000216output)\n",
      "Fig. 1: Convolutional neural network architectures (e.g., “64@ 14\u000214” means 64 feature maps of size 14\u000214).\n",
      "an architecture. Notice the discontinuities at the border of\n",
      "the patches, which reveal that the network did not succeed\n",
      "in learning to classify pixels independently of their location\n",
      "inside the patch. While this issue is partly addressed in [29]\n",
      "by smoothing the outputs with a conditional random ﬁeld, we\n",
      "argue that avoiding such artifacts by construction is desirable.\n",
      "In addition, generating similar results regardless of image\n",
      "tiling is an important property for large-scale satellite image\n",
      "processing, and an active research topic [31], [32]. Another\n",
      "concern with the fully connected layer is that the receptive ﬁeld\n",
      "of every patch output is not centered in itself. For example, a\n",
      "prediction near the center of the output patch can “see” about\n",
      "32 pixels in every direction around it. However, the prediction\n",
      "at the top-left corner of the output patch considers a larger\n",
      "portion of the image to the bottom and to the right than to the\n",
      "top and to the left. Considering that the division into patches\n",
      "is arbitrary, this behavior is hard to justify.\n",
      "A deeper understanding of the role played by every layer of\n",
      "the network, as described in this section, motivates the design\n",
      "of a more suitable architecture from a theoretical point of view,\n",
      "with the additional goal of boosting the overall performance\n",
      "of the approach.\n",
      "C. Fully Convolutional Network\n",
      "We propose a fully convolutional neural network architec-\n",
      "ture (FCN) to produce dense predictions. We explicitly restrict\n",
      "the process to be location-independent, enforcing the outputs\n",
      "to be the result of a series of convolutions only (see Fig. 1b).\n",
      "A classiﬁcation network may be “convolutionalized” [33] as\n",
      "follows. We ﬁrst convert the fully connected layer that carries\n",
      "out the classiﬁcation to a convolutional layer. The convolution\n",
      "kernel is chosen so that its dimensions coincide with the\n",
      "previous layer. Thus, its connections are equivalent to a fully\n",
      "connected layer. The difference is that if we enlarge the input\n",
      "image, the output size is also increased, but the number of\n",
      "parameters remains constant. This may be seen as convolving\n",
      "the whole original network around a larger image to evaluate\n",
      "the output at different locations.\n",
      "To increase the resolution of the output map, we then add a\n",
      "so-called “deconvolutional” layer [33]. The goal of this layer is\n",
      "to upsample the feature maps from the previous layer, which is\n",
      "achieved by performing an interpolation from a set of nearby\n",
      "points. Such an interpolation is parametrized by a kernel\n",
      "that expresses the extent and amount of contribution from a\n",
      "pixel value to its neighboring positions, only based on their\n",
      "locations. For an effective interpolation, the kernels must be\n",
      "Fig. 3: “Deconvolution” layer for upsampling.\n",
      "large enough to overlap in the output. The interpolation is then\n",
      "performed by multiplying the values of the kernel by every\n",
      "input and adding the overlapping responses in the output. This\n",
      "process is illustrated by Fig. 3 for a 2x upsampling. Notice\n",
      "that the scaling step is performed based on a constant 4\u00024\n",
      "kernel. In our framework, and as in previous work [33], the\n",
      "interpolation kernel is another set of learnable parameters of\n",
      "the network instead of being determined a priori, e.g., setting\n",
      "them to represent a bilinear interpolation. Note also that the\n",
      "upsampled feature map has a central part computed by adding\n",
      "the contribution of two neighboring kernels and an outer\n",
      "border obtained solely by the contribution of one kernel (the\n",
      "two leftmost and rightmost output columns in Fig. 3). The\n",
      "outer border can be seen as an extrapolation of the input while\n",
      "the inner part can be seen as an interpolation. The extrapolated\n",
      "border can be cropped from the output to avoid artifacts.\n",
      "As compared to a patch-based approach, we can expect our\n",
      "fully convolutional network to exhibit the following advan-\n",
      "tages:\n",
      "\u000fElimination of discontinuities due to patch borders;\n",
      "\u000fImproved accuracy due to a simpliﬁed learning process,\n",
      "with a smaller number of parameters;\n",
      "\u000fLower execution time at inference, due to the fast GPU\n",
      "execution of convolution operations.\n",
      "Our FCN network is constructed by convolutionalizing the\n",
      "existing patch-based network depicted by Fig. 1(a). We choose\n",
      "an existing framework to beneﬁt from a mature architecture\n",
      "and to carry out a rigorous comparison. The architectural\n",
      "decisions (i.e., the choice of the number of layers and ﬁlter\n",
      "sizes) of the base network are described in [29].\n",
      "Fig. 1(b) depicts the resulting FCN. First, we pretend that\n",
      "the output patch of the original network is only of sizeIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 6\n",
      "1\u00021, thus just focusing on a single output centered in its\n",
      "receptive ﬁeld. Second, we rewrite the fully connected layer\n",
      "as a convolutional layer with one feature map and the spatial\n",
      "dimensions of the previous layer ( 9\u00029). Third, we add a\n",
      "deconvolutional layer that upsamples its input by a factor of\n",
      "4 (with a learnable kernel of size 8\u00028), in order to recover\n",
      "the input resolution. Notice that the tasks of classiﬁcation and\n",
      "upsampling are now separated.\n",
      "This new network can take input images of different sizes,\n",
      "with the output size varying accordingly. For example, during\n",
      "the training stage we wish to output patches of size 16\u000216in\n",
      "order to emulate the learning process as was done in the patch-\n",
      "based network of Fig. 1(a). For this we require a patch input\n",
      "of size 80\u000280, as in the architecture of Fig. 1(b). Notice\n",
      "that the input is larger than the original 64\u000264patches.\n",
      "This is not because we are taking more context to carry\n",
      "out the predictions, but instead because every output is now\n",
      "centered in its context. At inference time we can take inputs\n",
      "of arbitrary sizes and feed them to the network to construct\n",
      "the classiﬁcation maps, and the number of network parameters\n",
      "does not vary.\n",
      "In the deconvolutional layer illustrated in Fig. 1(b), the\n",
      "overlapping areas added to produce the output are depicted\n",
      "in gray while the excluded extrapolation is in white.\n",
      "D. Experiments on Fully Convolutional Networks\n",
      "We implemented the CNNs using the Caffe deep learning\n",
      "framework [34]. In a ﬁrst experiment we apply our approach\n",
      "to the Massachusetts Buildings Dataset [29]. This dataset\n",
      "consists of color images over the area of Boston with 1 m2\n",
      "spatial resolution, covering an area of 340 km2for training,\n",
      "9 km2for validation and 22.5 km2for testing. The images are\n",
      "labeled into two classes: building andnot building . A portion\n",
      "of an image and its corresponding reference are depicted in\n",
      "Figs. 4(a-b).\n",
      "We train the patch-based and fully convolutional networks\n",
      "(Figs. 1(a) and 1(b) respectively) for 30,000 stochastic gra-\n",
      "dient descent iterations, until we observe barely no further\n",
      "improvement on the validation set. The patches are sampled\n",
      "uniformly from the whole training set, with mini-batches of\n",
      "64 patches each and a learning rate of 0.0001. A momentum\n",
      "and an L2 parameter penalty are introduced to regularize\n",
      "the learning process and avoid overﬁtting. Momentum adds\n",
      "a fraction of the previous gradient to the current one in order\n",
      "to smooth the descent, while an L2 penalty on the learned\n",
      "parameters discourages neurons to specialize too much on\n",
      "particular training cases [30]. The weights of these regularizers\n",
      "are set to 0.9 and 0.0002 respectively. Further details on these\n",
      "so-called hyperparameters and rationale for selecting them are\n",
      "provided by Mnih [29].\n",
      "To evaluate the accuracy of the classiﬁcation we use two\n",
      "different measures: pixelwise accuracy (proportion of correctly\n",
      "classiﬁed pixels, obtained through binary classiﬁcation of the\n",
      "output probabilities with threshold 0.5) and area under the\n",
      "receiver operating characteristics (ROC) curve [35]. The latter\n",
      "quantiﬁes the relation between true and false positives at\n",
      "different thresholds, and is appropriate to evaluate the overall\n",
      "quality of the fuzzy maps.Fig. 5(a) plots the evolution of the area under ROC curve\n",
      "and pixelwise accuracy in the test set, across iterations.\n",
      "The FCN consistently outperforms the patch-based network.\n",
      "Fig. 5(b) shows ROC curves for the ﬁnal networks after\n",
      "convergence, the FCN exhibiting the best relation between true\n",
      "and false positive rates. Fig. 4(c-d) depicts some visual results.\n",
      "To further illustrate the beneﬁts of neural networks over\n",
      "other learning approaches we train a support vector machine\n",
      "(SVM) with Gaussian kernel on 1,000 randomly selected\n",
      "pixels of each class. We train on the individual pixel spec-\n",
      "tra without any feature selection. The SVM parameters are\n",
      "selected by 5-fold cross-validation, as commonly performed\n",
      "in remote sensing image classiﬁcation [10]. As shown by\n",
      "Fig. 4(e), the pixelwise SVM classiﬁcation often confuses\n",
      "roads with buildings due to the fact that their colors are similar,\n",
      "while neural networks better infer and separate the classes by\n",
      "taking into account the geometry of the context. The accuracy\n",
      "of the SVM on the Boston test dataset is 0.6229 and its area\n",
      "under ROC curve is 0.5193, i.e., signiﬁcantly lower than with\n",
      "CNNs, as shown in Fig. 5. If we wished to successfully use an\n",
      "SVM for this task, we should design and select spatial features\n",
      "(e.g., texture) and use them as the input to the classiﬁer instead.\n",
      "The ampliﬁed fragment in Fig. 2 shows that the border\n",
      "discontinuity artifacts present in the patch-based scheme are\n",
      "absent in our fully convolutional setting. This behaves as\n",
      "expected considering that the issues described in Section III-B\n",
      "are addressed by construction in the new architecture. This\n",
      "conﬁrms that imposing sensible restrictions to the connections\n",
      "of a neural network has a positive impact in the performance.\n",
      "In terms of efﬁciency the FCN also outperforms the patch-\n",
      "based CNN. At inference time, instead of carrying out the\n",
      "prediction in a small patch basis, the input of the FCN is\n",
      "simply increased to output larger predictions, better beneﬁting\n",
      "from the GPU parallelization of convolutions. The execution\n",
      "time required to classify the whole Boston 22.5 km2test set\n",
      "(performed on an Intel I7 CPU @ 2.7Ghz with a Quadro\n",
      "K3100M GPU) is 82.21 s with the patch-based CNN against\n",
      "8.47 s with the FCN. The speedup is about 10x, a relevant im-\n",
      "provement considering the large-scale processing capabilities\n",
      "required by new sensors.\n",
      "IV. E ND-TO-ENDFRAMEWORK\n",
      "In remote sensing image analysis it is a common practice to\n",
      "train classiﬁers on the spectrum of a small number (a couple\n",
      "of hundreds) of isolated sample pixels [36]. Training relies\n",
      "on the trustworthiness of the reference data and on the fact\n",
      "that classes are reliably separable simply by observing the\n",
      "spectral signature of the sampled pixels. While such training\n",
      "approaches are popular, for example, in hyperspectral image\n",
      "classiﬁcation, our goals differ as we wish to automatically\n",
      "learn contextual features that can help better identify the\n",
      "classes in satellite imagery. Our goal requires more training\n",
      "data per se , as we must show the classiﬁer the many different\n",
      "contexts in which a pixel class can be embedded, and not just\n",
      "its spectral values. In addition, is it well-known that massive\n",
      "data might be required to train neural networks, contrary to\n",
      "a common feature selection and classiﬁcation approach. ThisIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 7\n",
      "(a) Color image\n",
      " (b) Reference data\n",
      " (c) Patch-based fuzzy map\n",
      " (d) FCN fuzzy map\n",
      " (e) SVM fuzzy map\n",
      "Fig. 4: Experimental results on a fragment of the Boston dataset.\n",
      "0.5 1 1.5 2 2.5 3\n",
      "x 1040.780.80.820.840.860.880.90.920.94\n",
      "IterationsArea under ROC curve\n",
      "  \n",
      "Fully convolutional\n",
      "Patch−based\n",
      "0.5 1 1.5 2 2.5 3\n",
      "x 1040.80.820.840.860.880.90.92\n",
      "IterationsPixel−wise accuracy\n",
      "  \n",
      "Fully convolutional\n",
      "Patch−based\n",
      "(a) Performance evolution\n",
      "0 0.1 0.2 0.3 0.4 0.50.50.60.70.80.91\n",
      "False Positive rateTrue Positive rate\n",
      "  \n",
      "Patch−based\n",
      "Fully convolutional(b) ROC curves\n",
      "Fig. 5: Evaluation of patch-based and fully convolutional neural networks on the Boston test set.\n",
      "led us to analyze and address the dependency of the algorithm\n",
      "on the availability and accuracy of the training data.\n",
      "In the experiments described in Section III-D, the Mas-\n",
      "sachusetts Buildings dataset is used for training and testing.\n",
      "This dataset is a hand-corrected version of the OpenStreetMap\n",
      "(OSM) vectorial map available over the area covered by\n",
      "the images. Despite the existence of some inaccuracies in\n",
      "the reference data, the coverage of OSM in that region is\n",
      "satisfactory and the errors are minor.\n",
      "In many other areas of Earth, however, the coverage of OSM\n",
      "is limited. In the samples of Fig. 8 we observe large areas\n",
      "with missing data and a general misregistration of the vectorial\n",
      "maps with respect to the actual structures. In addition, the\n",
      "misregistration is not uniform and neighboring buildings are\n",
      "often shifted in different directions. Note that in the samples of\n",
      "Fig. 8 the buildings have been delineated in OSM based on the\n",
      "ofﬁcial French cadaster records. However, even the cadaster\n",
      "records are not always accurate up to the meter resolution.\n",
      "Furthermore, satellite images undergo a series of corrections\n",
      "before being aligned to the maps. For example, the use of\n",
      "inexact elevation models for orthorectiﬁcation might introduce\n",
      "misregistrations throughout the images. As a result, the OSM\n",
      "raw data is imperfect and thus not fully reliable.\n",
      "The reference data obtained from OSM, as shown by Fig. 8,\n",
      "provides a rough idea of the location of the buildings, but\n",
      "rarely outlines them. In such a setting, convolutional neural\n",
      "networks would hardly learn that building boundaries are\n",
      "likely to fall on visible edges, since this is not what the refer-\n",
      "ence data depicts. Under these circumstances, we expect the\n",
      "predictions not to be very conﬁdent, especially on the border of\n",
      "the objects. As we will illustrate in Section IV-C, this yields a“blobby” and overly fuzzy aspect to predictions obtained with\n",
      "the network of Section III-C on more challenging datasets.\n",
      "Our ﬁrst contribution in this section is a novel approach for\n",
      "tackling the issue of inaccurate labels for CNN training. For\n",
      "this we propose a two-step approach: 1) the network is ﬁrst\n",
      "trained on raw OSM data, 2) it is then ﬁne-tuned on a tiny\n",
      "piece of manually labeled image.\n",
      "This method provides us with a means to deal with the\n",
      "inaccuracy of training data, by increasing the conﬁdence\n",
      "and sharpness of the predictions. However, we still cannot\n",
      "expect it to provide highly precise boundaries with the fully\n",
      "convolutional architecture as described in Section III-C. This\n",
      "is because such network includes a downsampling step, re-\n",
      "quired to capture the long-range spatial dependencies that\n",
      "help recognize the classes. However, downsampling makes\n",
      "the whole system lose spatial precision, and the deconvo-\n",
      "lutional layer learns a way of naively upsampling the data\n",
      "from a restricted number of neighbors, without reincorporating\n",
      "higher-resolution information. What is lost in spatial precision\n",
      "through the network, is not recovered. This is a consequence\n",
      "of a well-known trade-off between the receptive ﬁeld (how\n",
      "much context is taken to conduct predictions) and the output\n",
      "resolution (how ﬁne is the prediction) if we wish to keep a\n",
      "reasonable number of trainable parameters [33]. Our second\n",
      "contribution is then a new architecture that incorporates infor-\n",
      "mation at multiple scales in order to alleviate this trade-off.\n",
      "Our architecture combines low-resolution long-range features\n",
      "with high-resolution local features that conduct predictions\n",
      "with a higher level of detail. This architecture, when combined\n",
      "with our two-step training approach, provides a framework that\n",
      "can be used end-to-end to classify satellite imagery.IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 8\n",
      "A. Fine-tuning\n",
      "Fine-tuning is a very common procedure in the neural\n",
      "network literature. The idea is to adapt an existing pretrained\n",
      "model to a different domain by executing a few training iter-\n",
      "ations on a new dataset. The notion of ﬁne-tuning is based on\n",
      "the intuition that low-level information/features can be reused\n",
      "in different applications, without training from scratch. Even\n",
      "when the ﬁnal classiﬁcation objective is different, it is also\n",
      "a relevant approach for initializing the learnable parameters\n",
      "close to good local minima, instead of initializing with random\n",
      "weights. After proper ﬁne-tuning, low-level features tend to be\n",
      "quite preserved from one dataset to another, while the higher\n",
      "layers’ parameters are updated to adapt the network to the new\n",
      "problem [37].\n",
      "When ﬁne-tuning, the training set for the new domain is usu-\n",
      "ally substantially smaller than the one used to train the original\n",
      "network. This is because one assumes that some generalities\n",
      "of both domains are well conveyed in the pretrained network\n",
      "(e.g., edge detectors in different directions) and the ﬁne-tuning\n",
      "phase is just needed to conduct the domain adaptation. When\n",
      "the training set used for ﬁne-tuning is very small, additional\n",
      "considerations to avoid overﬁtting are commonly taken, such\n",
      "as early stopping (executing just a few iterations on the new\n",
      "training dataset), ﬁxing the weights at the lower layers or\n",
      "reducing the learning rate.\n",
      "We now incorporate the idea of neural network ﬁne-tuning,\n",
      "in order to perform training on imperfect data. Our approach\n",
      "proceeds in two steps. In step one large amounts of training\n",
      "data are used to train a fully convolutional neural network.\n",
      "This raw training data is extracted directly from OSM, without\n",
      "any hand correction. The goal of this step is to capture the\n",
      "generalities of the dataset such as, e.g., the representative\n",
      "spectrum of object classes.\n",
      "In step two, we ﬁne-tune the network by using a small\n",
      "part of carefully labeled image. This phase is designed to\n",
      "compensate for the inaccuracy of labels obtained in step one,\n",
      "by ﬁne-tuning the network on small yet consistent target\n",
      "outputs. Assuming that most of the generalities have been\n",
      "captured during the initial training step, the ﬁne-tuning step\n",
      "should locally correct the network parameters to output more\n",
      "accurate classiﬁcations. The efforts of ﬁne-tuning are thus\n",
      "limited to manually labeling a small dataset, while the large\n",
      "inaccurate dataset is automatically extracted from OSM.\n",
      "B. Conducting Fine Predictions\n",
      "The resolution at which the networks proposed in Section III\n",
      "operate yields probability maps that, once upsampled, are\n",
      "coarse in terms of spatial accuracy. A naive way to increase\n",
      "the resolution of the network would be to use higher-resolution\n",
      "ﬁlters, which requires to increase their dimensions if we\n",
      "want to preserve the receptive ﬁeld. For example, instead of\n",
      "applying a 5\u00025ﬁlter at a fourth of the image resolution, one\n",
      "could use a 20\u000220ﬁlter at full resolution, hence covering the\n",
      "same spatial extent. However, such an increase in ﬁlter sizes\n",
      "is prohibitive, hampering the spatial and temporal efﬁciency\n",
      "of the algorithm and producing less accurate results due to the\n",
      "difﬁculty of optimizing so many parameters.Nevertheless, we observed that we do not need full-\n",
      "resolution ﬁlters to conduct accurate predictions. One requires\n",
      "a higher resolution only in the center of the convolution\n",
      "ﬁlters (assuming that the pixel we wish to predict is in the\n",
      "center of the context of interest). A large spatial extent is\n",
      "indeed required to capture contextual information, but it is\n",
      "not necessary to conduct this analysis at full resolution. For\n",
      "example, the presence of two parallel bands of grass can help\n",
      "identify a road (and distinguish it from, for instance, a building\n",
      "with a gray rooftop), but a precise localization of the grass is\n",
      "not necessary. On the contrary, at the center of the convolution\n",
      "ﬁlter, a higher-resolution analysis is required to speciﬁcally\n",
      "locate the boundary of the aforementioned road.\n",
      "Fig. 6 illustrates this observation. In Fig. 6(a) we observe\n",
      "the area around a pixel whose class we wish to predict, at full\n",
      "resolution. A ﬁlter taking such an amount of context with that\n",
      "resolution would be prohibitive in the number of parameters,\n",
      "as well as unnecessary. Fig. 6(b) depicts the same context\n",
      "at a quarter of the resolution. Notice that it is still possible\n",
      "to visually infer that there is a road. However, identifying\n",
      "the precise location of the boundaries of the road becomes\n",
      "difﬁcult. Alternatively, Fig. 6(c) depicts a small patch but at\n",
      "full resolution. We can now better locate the precise boundary\n",
      "of the object, but with so little context it is difﬁcult to identify\n",
      "that the object is indeed a road. Large ﬁlters at low resolution\n",
      "- see Fig. 6(b) or small ﬁlters at high resolution - see Fig. 6(c),\n",
      "which would both have a reasonable number of parameters,\n",
      "are bad alternatives: the ﬁrst ﬁlter is too coarse and the second\n",
      "ﬁlter is using too little context.\n",
      "We propose convolutional ﬁlters that combine multiple\n",
      "scales instead. In Fig. 6(d) the large-size low-resolution con-\n",
      "text of Fig. 6(b) is combined with the small high-resolution\n",
      "context of 6(d). This provides us with a means to simultane-\n",
      "ously infer the class by observing the surroundings at a coarse\n",
      "scale, and determine the precise boundary location by using\n",
      "a ﬁner context. This way, the amount of parameters are kept\n",
      "small while the trade-off between recognition and localization\n",
      "is alleviated.\n",
      "Les us denote bySa set of levels of detail expressed as a\n",
      "fraction of the original resolution. For example, S=f1;1=2g\n",
      "is a set comprising two-scales: full resolution and half of the\n",
      "full resolution. We denote by xsa feature map xdownsampled\n",
      "to a certain level s2S. For example, x1=2is a feature map\n",
      "downsampled to half of the original resolution. Inspired in\n",
      "Equation 1, we design a special type of neuron that adds the\n",
      "responses to a set of ﬁlters applied at different scales of the\n",
      "feature maps in the previous layer:\n",
      "a=\u001b X\n",
      "s2Swsxs+b!\n",
      ": (5)\n",
      "Notice that individual ﬁlters wsare learned for every scale\n",
      "s. Such a ﬁlter is easily implemented by using a combination\n",
      "of elementary convolutional, downsampling and upsampling\n",
      "layers. Fig. 7 illustrates this process in the case of a two-\n",
      "scale (S=f1;1=2g) module. In our implementation we\n",
      "average neighboring elements in a window for downsampling\n",
      "and perform bilinear interpolation for upsampling, but otherIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 9\n",
      "(a) Large context, high res-\n",
      "olution\n",
      "(b) Large context, low res-\n",
      "olution\n",
      "(c) Small context, high res-\n",
      "olution\n",
      "(d) Combined scales\n",
      "Fig. 6: Different types of context to predict a pixel’s class. A multi-scale context such as in (d) alleviates the trade-off between\n",
      "classiﬁcation accuracy and number of learnable parameters.\n",
      "Fig. 7: Two-scale convolutional module that simultaneously\n",
      "combines coarse large-range and ﬁne short-range reasoning.\n",
      "Fig. 8: Fragments of the Forez training set (red: building ).\n",
      "approaches are also applicable. The kernel sizes of the con-\n",
      "volutions at both scales are set to be equal (e.g., 3\u00023), yet\n",
      "the amount of context taken varies from one path to the other\n",
      "due to the different scales. The addition is an elementwise\n",
      "operation, followed by the nonlinear activation function.\n",
      "C. Experiments on the End-to-End Classiﬁcation Framework\n",
      "We conduct our experiments on a Pl ´eiades image over the\n",
      "area of Forez, France. An orthorectiﬁed color pansharpened\n",
      "version of the image is used, at a spatial resolution of 0.5\n",
      "m2. Our training subset amounts to 22.5 km2. The criterion to\n",
      "construct the training set was to choose ten 3000\u00023000 tiles\n",
      "with at least some coverage of OpenStreetMap (OSM). The\n",
      "shape ﬁles were rasterized with GDAL1to create the binary\n",
      "reference maps. Fig. 8 shows some fragments of the reference\n",
      "data. Inconsistent misregistrations and considerable omissions\n",
      "are observed all over.\n",
      "1http://www.gdal.org\n",
      "Fig. 9: Manually labeled tile for ﬁne-tuning (3000 \u00023000).\n",
      "Fig. 10: Fragment of the ﬁne-tuning tile. Red borders enclose\n",
      "building areas.\n",
      "We manually labeled a 2.25 km2tile for FCN ﬁne-tuning,\n",
      "and a different 2.25 km2tile for testing. The manual labeling\n",
      "takes about two hours for each of the tiles. The entire tuning\n",
      "tile is depicted by Fig. 9 and a close-up is shown in Fig. 10.\n",
      "The fully convolutional network (FCN) described in Section\n",
      "III-C, which was used for the Massachusetts dataset, is now\n",
      "trained with the Forez set, under a similar experimental setting.\n",
      "Note that this FCN was designed for images which have a\n",
      "1 m2resolution, while Pl ´eiades imagery features a 0.5 m2\n",
      "resolution. In order for the architectural decisions of FCN to\n",
      "be valid in our new dataset, one must preserve the receptive\n",
      "ﬁeld size in terms of meters, not pixels. We thus downsample\n",
      "Pl´eiades images prior to entering the ﬁrst layer of the FCN,\n",
      "and bilinearly upsample the output classiﬁcation maps. Even\n",
      "though a new network directly tailored to the Pl ´eiades reso-\n",
      "lution could be designed, we favor this proven architecture to\n",
      "conduct our experiments. The concepts described in this paperIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 10\n",
      "Method Accuracy AUC IoU\n",
      "FCN 0.99126 0.99166 0.48\n",
      "FCN + Fine-tuning 0.99459 0.99699 0.66\n",
      "Two-scale FCN 0.99129 0.98154 0.47\n",
      "Two-scale FCN + Fine-tuning 0.99573 0.99836 0.72\n",
      "TABLE I: Performance evaluation on the Pl ´eiades test set.\n",
      "are however general and can be used to design other networks.\n",
      "After training on the raw OSM Forez dataset, we ﬁne-tune\n",
      "the weights on the manually labeled tuning tile. The training\n",
      "hyper-parameters are kept similar in the ﬁne-tuning step, but\n",
      "an early stopping criterion interrupts it after 200 iterations.\n",
      "To assess the performance of ﬁne-tuning we use as criteria\n",
      "pixelwise accuracy and area under the ROC curve (AUC), as\n",
      "described in Section III-D. Since there are many more non-\n",
      "building pixels than building pixels in this dataset, these accu-\n",
      "racy measures might seem overly high, a well-known issue of\n",
      "pixelwise accuracy in imbalanced datsets [38]. We add then\n",
      "the intersection over union criterion (IoU), an object-based\n",
      "overlap measure typically used for imbalanced datasets [38].\n",
      "In our case it is deﬁned as the number of pixels labeled as\n",
      "buildings both in the classiﬁed image and in the ground truth,\n",
      "divided by the total amount of pixels labeled as such in either\n",
      "of them. These criteria are evaluated on the manually labeled\n",
      "test set, which is used neither for training nor for ﬁne-tuning.\n",
      "The ﬁrst two rows of Table I show that ﬁne-tuning enhances\n",
      "the quality of the predictions in terms of accuracy, AUC and\n",
      "IoU. To conﬁrm the signiﬁcance of the accuracy, a McNemar’s\n",
      "test [39] proved that the improvement is not a result of mere\n",
      "luck with a probability greater than 0.99999. Besides, the IoU\n",
      "is improved by over a third with the ﬁne-tuned network.\n",
      "Fig. 11(a-d) shows the impact of ﬁne-tuning on several\n",
      "ampliﬁed fragments of the test set. A greater conﬁdence in\n",
      "the ﬁne-tuned network predictions is observed. The objects\n",
      "exhibit better alignment to the objects of the image, albeit the\n",
      "boundaries could better line up to the underlying edges.\n",
      "Fig. 12 illustrate the ﬁrst-layer convolutional ﬁlters learned\n",
      "by the initial and ﬁne-tuned networks. We observe a combina-\n",
      "tion of low- and high-frequency ﬁlters, a behavior typically\n",
      "observed in CNNs. We also observe edge and color blob\n",
      "detectors. These ﬁlter remain unchanged after ﬁne-tuning, even\n",
      "though no constraints are introduced to enforce this. Fine-\n",
      "tuning corrects the weights in the high-level layers, which\n",
      "suggests that the initial low-level features were useful indeed,\n",
      "but the inaccuracy in the labels was introducing fuzziness in\n",
      "the upper layers of the network.\n",
      "We now evaluate the performance of a two-scale network.\n",
      "The FCN architecture described in Section III-C is replaced\n",
      "by three two-scale stacked modules, with scales S=f1;1=4g.\n",
      "We selectS= 1=4as it corresponds to the degree of\n",
      "downsampling of the original FCN network, and S= 1 is\n",
      "added to reﬁne the predictions. The three modules learn 3\u00023\n",
      "ﬁlters in both scales. The ﬁrst two modules generate 64 feature\n",
      "maps and the last module generates a single map with the\n",
      "building/non-building prediction.\n",
      "The two-scale network is trained and ﬁne-tuned in a similar\n",
      "setting as the FCN network. The results summarized in the\n",
      "last two rows of Table I show that ﬁne-tuning signiﬁcantly\n",
      "Fig. 12: First layer ﬁlters before and after ﬁne-tuning.\n",
      "enhances the classiﬁcation performance, and that the ﬁne-\n",
      "tuned two-scale network outperforms the single scale network.\n",
      "Notably, IoU goes from 0.48 to 0.72, implying that objects\n",
      "overlap with the ground truth 50% better by adding a scale\n",
      "and performing ﬁne-tuning. Note that if a scale is added but\n",
      "no ﬁne-tuning is done, there is actually a slight decrease in\n",
      "performance. A possible explanation for this is that including a\n",
      "ﬁner scale adds even more confusion to the training algorithm\n",
      "if only noisy misregistered labels are provided.\n",
      "Figs. 11(e-f) illustrate the results on visual fragments of\n",
      "the test set. The two-scale network yields classiﬁcation maps\n",
      "that better correspond to the actual image objects, and exhibit\n",
      "sharper angles and straighter lines. The entire classiﬁed test tile\n",
      "for the ﬁne-tuned two-scale network is depicted by Fig. 13c.\n",
      "The time required to generate this result corresponds to three\n",
      "hours for training on the OSM dataset, two hours to manually\n",
      "label an image tile and about a minute for ﬁne-tuning. The\n",
      "prediction of the 3000\u00023000 test tile using the hardware\n",
      "described in Section III-D takes 3:2seconds, and it grows\n",
      "linearly in the size of the image. As in Section III-D, we ran\n",
      "an SVM on the individual pixel values (see the classiﬁcation\n",
      "map in Fig. 13b). Accuracy is 0.9487 and IoU 0.19, yielding\n",
      "poorer results than the presented CNN-based approaches.\n",
      "As validated by the experiments, the issue of not having\n",
      "large amounts of high-quality reference data can be alleviated\n",
      "by providing the network with a small amount of accurate\n",
      "data in a ﬁne-tuning step. Our multi-scale neurons combine\n",
      "reasoning at different resolutions to effectively produce ﬁne\n",
      "predictions, while keeping a reasonable number of parameters.\n",
      "Such a framework can be used end-to-end to perform the\n",
      "classiﬁcation task directly from input imagery. More scales\n",
      "can be easily be added and, besides the fact of being fully\n",
      "convolutional, there are little constraints on the architecture\n",
      "itself, admitting a different number of classes, input bands or\n",
      "number of feature maps.\n",
      "V. C ONCLUDING REMARKS\n",
      "Convolutional neural networks have become a popular clas-\n",
      "siﬁer in the context of image analysis due to their potential\n",
      "to automatically learn relevant contextual features. Initially\n",
      "devised for the categorization of natural images, these net-\n",
      "works must be revisited and adapted to tackle the problem of\n",
      "pixelwise labeling in remote sensing imagery.\n",
      "We proposed a fully convolutional network architecture by\n",
      "analyzing a state-of-the-art model and solving its concerns by\n",
      "construction. Despite their outstanding learning capability, theIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 11\n",
      "(a) Color image\n",
      " (b) Reference\n",
      " (c) FCN\n",
      " (d) FCN + Fine-tuning\n",
      " (e) 2-scale FCN\n",
      " (f) 2-scale FCN +\n",
      "Fine-tuning\n",
      "Fig. 11: Classiﬁed fragments of the Pl ´eiades test image. Fine-tuning increases the conﬁdence of the predictions, and the\n",
      "two-scale network produces ﬁne-grained classiﬁcation maps.IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 12\n",
      "(a) Color pansharpened input\n",
      " (b) SVM on individual pixels\n",
      " (c) FCN (two scales + ﬁne-tuning)\n",
      "Fig. 13: Binary classiﬁcation maps on the Forez test image.\n",
      "lack of accurate training data might limit the applicability of\n",
      "CNN models in realistic remote sensing contexts. We therefore\n",
      "proposed a two-step training approach combining the use of\n",
      "large amounts of raw OpenStreetMap data and a small sample\n",
      "of manually labeled reference. The last ingredient we needed\n",
      "to provide a usable end-to-end framework for remote sensing\n",
      "image classiﬁcation was to produce ﬁne-grained classiﬁcation\n",
      "maps, since typical CNNs tend to hamper the ﬁneness of the\n",
      "output as a side effect of taking large amounts of context. We\n",
      "proposed a type of neuron module that simultaneously reasons\n",
      "at different scales.\n",
      "Experiments showed that our fully convolutional network\n",
      "outperforms the previous model in multiple aspects: the accu-\n",
      "racy of the results is improved, the visual artifacts are removed\n",
      "and the inference time is reduced by a factor of ten. The use of\n",
      "our architecture constitutes then a win-win situation in which\n",
      "no aspect is compromised for the others. This was achieved\n",
      "by analyzing the role played by every layer in the network in\n",
      "order to propose a more appropriate architecture, showing that\n",
      "a deep understanding of how CNNs work is important for their\n",
      "success. Further experimentation showed that the two-step\n",
      "training approach effectively combines imperfect training data\n",
      "with manually labeled data to capture the dataset’s generalities\n",
      "and its precise details. Moreover, the multi-scale modules\n",
      "increase the level of detail of the classiﬁcation without making\n",
      "the number of parameters explode, attenuating the trade-off\n",
      "between detection and localization.\n",
      "Our overall framework shows then that convolutional neural\n",
      "networks can be used end-to-end to process large amounts of\n",
      "satellite images and provide accurate pixelwise classiﬁcations.\n",
      "As future work we plan to extend our experiments to\n",
      "multiple object classes and study the possibility of directly\n",
      "inputting non-pansharpened imagery, in order to avoid this\n",
      "preprocessing step. We also plan to study the introduction of\n",
      "shape priors in the learning process and the vectorization of\n",
      "the classiﬁcation maps.\n",
      "ACKNOWLEDGMENT\n",
      "CNES (2012 and 2013), distri-\n",
      "bution Airbus DS / SpotImage. The authors thank CNES for\n",
      "initializing and funding the study, and providing Pl ´eiades data.REFERENCES\n",
      "[1] Yann LeCun, L ´eon Bottou, Yoshua Bengio, and Patrick Haffner,\n",
      "“Gradient-based learning applied to document recognition,” Proceedings\n",
      "of the IEEE , vol. 86, no. 11, pp. 2278–2324, 1998.\n",
      "[2] Dan Ciresan, Ueli Meier, and J ¨urgen Schmidhuber, “Multi-column deep\n",
      "neural networks for image classiﬁcation,” in IEEE CVPR , 2012, pp.\n",
      "3642–3649.\n",
      "[3] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton, “Imagenet\n",
      "classiﬁcation with deep convolutional neural networks,” in NIPS , 2012.\n",
      "[4] Annamalai Senthil Kumar and Kantilal Majumder, “Information fusion\n",
      "in tree classiﬁers,” International Journal of Remote Sensing , vol. 22,\n",
      "no. 5, pp. 861–869, 2001.\n",
      "[5] Jean Mas and Juan Flores, “The application of artiﬁcial neural networks\n",
      "to the analysis of remotely sensed data,” International Journal of Remote\n",
      "Sensing , vol. 29, no. 3, pp. 617–663, 2008.\n",
      "[6] Thomas Villmann, Erzsbet Mernyi, and Barbara Hammer, “Neural maps\n",
      "in remote sensing image analysis,” Neural Networks , vol. 16, no. 34,\n",
      "pp. 389 – 403, 2003, Neural Network Analysis of Complex Scientiﬁc\n",
      "Data: Astronomy and Geosciences.\n",
      "[7] Gustavo Camps-Valls and Lorenzo Bruzzone, “Kernel-based methods\n",
      "for hyperspectral image classiﬁcation,” IEEE Tran. Geosci. Remote\n",
      "Sens. , vol. 43, no. 6, pp. 1351–1362, 2005.\n",
      "[8] Mathieu Fauvel, Yuliya Tarabalka, Jon Atli Benediktsson, Jocelyn\n",
      "Chanussot, and James C Tilton, “Advances in spectral-spatial classi-\n",
      "ﬁcation of hyperspectral images,” Proceedings of the IEEE , vol. 101,\n",
      "no. 3, pp. 652–675, 2013.\n",
      "[9] Wenzhi Liao, Mauro Dalla Mura, Jocelyn Chanussot, Rik Bellens,\n",
      "and Wilfried Philips, “Morphological attribute proﬁles with partial\n",
      "reconstruction,” IEEE Tran. Geosci. Remote Sens. , vol. 54, no. 3, pp.\n",
      "1738–1756, 2016.\n",
      "[10] Yuliya Tarabalka and Aakanksha Rana, “Graph-cut-based model for\n",
      "spectral-spatial classiﬁcation of hyperspectral images,” in IEEE IGARSS .\n",
      "IEEE, 2014, pp. 3418–3421.\n",
      "[11] Mehmet Nadir Kurnaz, Zmray Dokur, and Tamer lmez, “Segmentation\n",
      "of remote-sensing images by incremental neural network,” Pattern\n",
      "Recognition Letters , vol. 26, no. 8, pp. 1096 – 1104, 2005.\n",
      "[12] Christopher David Lloyd, Suha Berberoglu, Paul Curran, and Peter\n",
      "Atkinson, “A comparison of texture measures for the per-ﬁeld classi-\n",
      "ﬁcation of mediterranean land cover,” International Journal of Remote\n",
      "Sensing , vol. 25, no. 19, pp. 3943–3965, 2004.\n",
      "[13] Dengsheng Lu and Qihao Weng, “A survey of image classiﬁcation\n",
      "methods and techniques for improving classiﬁcation performance,” In-\n",
      "ternational journal of Remote sensing , vol. 28, no. 5, pp. 823–870, 2007.\n",
      "[14] ME Midhun, Sarath R Nair, VT Prabhakar, and S Sachin Kumar, “Deep\n",
      "model for classiﬁcation of hyperspectral image using restricted boltz-\n",
      "mann machine,” in Proceedings of the 2014 International Conference\n",
      "on Interdisciplinary Advances in Applied Computing . ACM, 2014, p. 35.\n",
      "[15] Tong Li, Junping Zhang, and Ye Zhang, “Classiﬁcation of hyperspectral\n",
      "image based on deep belief networks,” in IEEE ICIP , 2014.\n",
      "[16] Viktor Slavkovikj, Steven Verstockt, Wesley De Neve, Soﬁe Van Hoecke,\n",
      "and Rik Van de Walle, “Hyperspectral image classiﬁcation with convo-\n",
      "lutional neural networks,” in Proceedings of the 23rd ACM international\n",
      "conference on Multimedia . ACM, 2015, pp. 1159–1162.IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 13\n",
      "[17] Yushi Chen, Xing Zhao, and Xiuping Jia, “Spectral-spatial classiﬁcation\n",
      "of hyperspectral data based on deep belief network,” IEEE J. Sel. Topics\n",
      "Appl. Earth Observ. in Remote Sens. , vol. 8, no. 6, June 2015.\n",
      "[18] Yushi Chen, Zhouhan Lin, Xing Zhao, Gang Wang, and Yanfeng Gu,\n",
      "“Deep learning-based classiﬁcation of hyperspectral data,” IEEE J. Sel.\n",
      "Topics Appl. Earth Observ. in Remote Sens. , vol. 7, no. 6, pp. 2094–\n",
      "2107, 2014.\n",
      "[19] Jun Yue, Wenzhi Zhao, Shanjun Mao, and Hui Liu, “Spectral–spatial\n",
      "classiﬁcation of hyperspectral images using deep convolutional neural\n",
      "networks,” Remote Sensing Letters , vol. 6, no. 6, pp. 468–477, 2015.\n",
      "[20] Konstantinos Makantasis, Konstantinos Karantzalos, Anastasios\n",
      "Doulamis, and Nikolaos Doulamis, “Deep supervised learning for\n",
      "hyperspectral data classiﬁcation through convolutional neural networks,”\n",
      "inIEEE IGARSS . IEEE, 2015, pp. 4959–4962.\n",
      "[21] Wenzhi Zhao and Shihong Du, “Spectral–spatial feature extraction\n",
      "for hyperspectral image classiﬁcation: A dimension reduction and deep\n",
      "learning approach,” IEEE Tran. Geosci. Remote Sens. , vol. 54, no. 8,\n",
      "pp. 4544–4554, 2016.\n",
      "[22] Wenzhi Zhao, Zhou Guo, Jun Yue, Xiuyuan Zhang, and Liqun Luo,\n",
      "“On combining multiscale deep learning features for the classiﬁcation of\n",
      "hyperspectral remote sensing imagery,” International Journal of Remote\n",
      "Sensing , vol. 36, no. 13, pp. 3368–3379, 2015.\n",
      "[23] Wenzhi Zhao and Shihong Du, “Learning multiscale and deep repre-\n",
      "sentations for classifying remotely sensed imagery,” ISPRS Journal of\n",
      "Photogrammetry and Remote Sensing , vol. 113, pp. 155–165, 2016.\n",
      "[24] Essa Basaeed, Harish Bhaskar, Paul Hill, Mohammed Al-Mualla, and\n",
      "David Bull, “A supervised hierarchical segmentation of remote-sensing\n",
      "images using a committee of multi-scale convolutional neural networks,”\n",
      "International Journal of Remote Sensing , vol. 37, no. 7, 2016.\n",
      "[25] Jun Wang, Jingwei Song, Mingquan Chen, and Zhi Yang, “Road network\n",
      "extraction: a neural-dynamic framework based on deep learning and a\n",
      "ﬁnite state machine,” International Journal of Remote Sensing , vol. 36,\n",
      "no. 12, pp. 3144–3169, 2015.\n",
      "[26] Xueyun Chen, Shiming Xiang, Cheng-Lin Liu, and Chun-Hong Pan,\n",
      "“Vehicle detection in satellite images by hybrid deep convolutional\n",
      "neural networks,” IEEE Geoscience and remote sensing letters , vol.\n",
      "11, no. 10, pp. 1797–1801, 2014.\n",
      "[27] Igor ˇSevo and Aleksej Avramovi ´c, “Convolutional neural network based\n",
      "automatic object detection on aerial images,” IEEE Geoscience and\n",
      "Remote Sensing Letters , vol. 13, no. 5, pp. 740–744, 2016.\n",
      "[28] FPS Luus, BP Salmon, F Van Den Bergh, and BTJ Maharaj, “Multiview\n",
      "deep learning for land-use classiﬁcation,” IEEE Geoscience and Remote\n",
      "Sensing Letters , vol. 12, no. 12, pp. 2448–2452, 2015.\n",
      "[29] V olodymyr Mnih, Machine learning for aerial image labeling , Ph.D.\n",
      "thesis, University of Toronto, 2013.\n",
      "[30] Christopher M Bishop, Neural networks for pattern recognition , Oxford\n",
      "university press, 1995.\n",
      "[31] Julien Michel, David Yousseﬁ, and Manuel Grizonnet, “Stable mean-\n",
      "shift algorithm and its application to the segmentation of arbitrarily large\n",
      "remote sensing images,” IEEE Tran. Geosci. Remote Sens. , vol. 53, no.\n",
      "2, pp. 952–964, 2015.\n",
      "[32] Pierre Lassalle, Jordi Inglada, Julien Michel, Manuel Grizonnet, and\n",
      "Julien Malik, “A scalable tile-based framework for region-merging\n",
      "segmentation,” IEEE Tran. Geosci. Remote Sens. , vol. 53, no. 10, pp.\n",
      "5473–5485, 2015.\n",
      "[33] Jonathan Long, Evan Shelhamer, and Trevor Darrell, “Fully convolu-\n",
      "tional networks for semantic segmentation,” in CVPR , 2015.\n",
      "[34] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan\n",
      "Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell, “Caffe:\n",
      "Convolutional architecture for fast feature embedding,” arXiv preprint\n",
      "arXiv:1408.5093 , 2014.\n",
      "[35] C `esar Ferri, Jos ´e Hern ´andez-Orallo, and Peter A Flach, “A coherent\n",
      "interpretation of AUC as a measure of aggregated classiﬁcation perfor-\n",
      "mance,” in ICML , 2011.\n",
      "[36] Yuliya Tarabalka, Mathieu Fauvel, Jocelyn Chanussot, and J ´on Atli\n",
      "Benediktsson, “Svm-and mrf-based method for accurate classiﬁcation of\n",
      "hyperspectral images,” Geoscience and Remote Sensing Letters, IEEE ,\n",
      "vol. 7, no. 4, pp. 736–740, 2010.\n",
      "[37] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson, “How\n",
      "transferable are features in deep neural networks?,” in NIPS , 2014.\n",
      "[38] Gabriela Csurka, Diane Larlus, Florent Perronnin, and France Meylan,\n",
      "“What is a good evaluation measure for semantic segmentation?.,” in\n",
      "BMVC , 2013, vol. 27, p. 2013.\n",
      "[39] Quinn McNemar, “Note on the sampling error of the difference between\n",
      "correlated proportions or percentages,” Psychometrika , vol. 12, no. 2,\n",
      "pp. 153–157, 1947.\n",
      "Emmanuel Maggiori received the Engineering de-\n",
      "gree in computer science from Central Buenos Aires\n",
      "Province National University (UNCPBA), Tandil,\n",
      "Argentina, in 2014. The same year he joined\n",
      "AYIN and STARS teams at Inria Sophia Antipolis-\n",
      "M´editerran ´ee as a research intern in the ﬁeld of\n",
      "remote sensing image processing. Since 2015, he\n",
      "has been working on his Ph.D. within TITANE team,\n",
      "studying machine learning techniques for large-scale\n",
      "processing of satellite imagery.\n",
      "Yuliya Tarabalka (S’08–M’10) received the B.S.\n",
      "degree in computer science from Ternopil Ivan\n",
      "Pul’uj State Technical University, Ukraine, in 2005\n",
      "and the M.Sc. degree in signal and image processing\n",
      "from the Grenoble Institute of Technology (INPG),\n",
      "France, in 2007. She received a joint Ph.D. degree\n",
      "in signal and image processing from INPG and in\n",
      "electrical engineering from the University of Iceland,\n",
      "in 2010.\n",
      "From July 2007 to January 2008, she was a\n",
      "researcher with the Norwegian Defence Research\n",
      "Establishment, Norway. From September 2010 to December 2011, she was a\n",
      "postdoctoral research fellow with the Computational and Information Sciences\n",
      "and Technology Ofﬁce, NASA Goddard Space Flight Center, Greenbelt, MD.\n",
      "From January to August 2012 she was a postdoctoral research fellow with\n",
      "the French Space Agency (CNES) and Inria Sophia Antipolis-M ´editerran ´ee,\n",
      "France. She is currently a researcher with the TITANE team of Inria Sophia\n",
      "Antipolis-M ´editerran ´ee. Her research interests are in the areas of image\n",
      "processing, pattern recognition and development of efﬁcient algorithms. She\n",
      "is Member of the IEEE Society.\n",
      "Guillaume Charpiat is a researcher at Inria Saclay\n",
      "(France) in the TAO team. He studied Mathemat-\n",
      "ics and Physics at the ´Ecole Normale Sup ´erieure\n",
      "(ENS Paris), and then Computer Vision and Machine\n",
      "Learning (at ENS Cachan), as well as Theoretical\n",
      "Physics. His PhD thesis, in Computer Science, ob-\n",
      "tained in 2006, was on the topic of distance-based\n",
      "shape statistics for image segmentation with priors.\n",
      "He then spent one year at the Max-Planck Institute\n",
      "for Biological Cybernetics (T ¨ubingen, Germany),\n",
      "on the topics of medical imaging (MR-based PET\n",
      "prediction) and automatic image colorization. As a researcher at Inria Sophia-\n",
      "Antipolis (France), he worked mainly on image segmentation and optimization\n",
      "techniques. Now at Inria Saclay he focuses on Machine Learning, in particular\n",
      "on building a theoretical background for neural networks.\n",
      "Pierre Alliez Pierre Alliez is Senior Researcher and\n",
      "team leader at Inria Sophia-Antipolis - Mediterranee.\n",
      "He has authored scientiﬁc publications and several\n",
      "book chapters on mesh compression, surface recon-\n",
      "struction, mesh generation, surface remeshing and\n",
      "mesh parameterization. He is an associate editor\n",
      "of the Computational Geometry Algorithms Library\n",
      "(http://www.cgal.org) and an associate editor of the\n",
      "ACM Transactions on Graphics. He was awarded\n",
      "in 2005 the EUROGRAPHICS young researcher\n",
      "award for his contributions to computer graphics\n",
      "and geometry processing. He was co-chair of the Symposium on Geometry\n",
      "Processing in 2008, of Paciﬁc Graphics in 2010 and Geometric Modeling and\n",
      "Processing 2014. He was awarded in 2011 a Starting Grant from the European\n",
      "Research Council on Robust Geometry Processing.\n",
      "Extracted from Deep Learning Based Large-Scale Automatic Satellite Crosswalk Classification.pdf: Deep Learning Based Large-Scale Automatic\n",
      "Satellite Crosswalk Classiﬁcation\n",
      "Rodrigo F. Berriel, Andr ´e Teixeira Lopes, Alberto F. de Souza, and Thiago Oliveira-Santos\n",
      "Abstract —High-resolution satellite imagery have been increas-\n",
      "ingly used on remote sensing classiﬁcation problems. One of the\n",
      "main factors is the availability of this kind of data. Even though,\n",
      "very little effort has been placed on the zebra crossing classiﬁca-\n",
      "tion problem. In this letter, crowdsourcing systems are exploited\n",
      "in order to enable the automatic acquisition and annotation of a\n",
      "large-scale satellite imagery database for crosswalks related tasks.\n",
      "Then, this dataset is used to train deep-learning-based models\n",
      "in order to accurately classify satellite images that contains or\n",
      "not zebra crossings. A novel dataset with more than 240,000\n",
      "images from 3 continents, 9 countries and more than 20 cities\n",
      "was used in the experiments. Experimental results showed that\n",
      "freely available crowdsourcing data can be used to accurately\n",
      "(97.11%) train robust models to perform crosswalk classiﬁcation\n",
      "on a global scale.\n",
      "Index Terms —Zebra crossing classiﬁcation, crosswalk classiﬁ-\n",
      "cation, large-scale satellite imagery, deep learning\n",
      "I. I NTRODUCTION\n",
      "ZEBRA crossing classiﬁcation and detection are important\n",
      "tasks for mobility autonomy. Even though, there are\n",
      "few data available on where crosswalks are in the world.\n",
      "The automatic annotation of crosswalks’ locations worldwide\n",
      "can be very useful for online maps, GPS applications and\n",
      "many others. In addition, the availability of zebra crossing\n",
      "locations on these applications can be of great use to people\n",
      "with disabilities, to road management, and to autonomous\n",
      "vehicles. However, automatically annotating this kind of data\n",
      "is a challenging task. They are often aging (painting fading\n",
      "away), occluded by vehicle and pedestrians, darkened by\n",
      "strong shadows, and many other factors.\n",
      "A common approach to tackle these problems is using the\n",
      "camera on the phones to help the visually impaired people.\n",
      "Ivanchenko et al. [1] developed a prototype of a cell phone\n",
      "application that determines if any crosswalk is visible and\n",
      "aligns the user to it. Their system requires some thresholds to\n",
      "be tuned, which may decrease performance delivered off-the-\n",
      "shelf. Another approach is a camera mounted on a car, usually\n",
      "aiming driver assistance systems. Haselhoff and Kummert [2]\n",
      "presented a strategy based on the detection of line segments.\n",
      "As discussed by the authors, their method is constrained\n",
      "to crosswalks perpendicular to the driving direction. Both\n",
      "perspectives (from a person [1] or a car [2]) present a known\n",
      "limitation: there is a maximum distance in which crosswalks\n",
      "can be detected. Moreover, these images are quite different\n",
      "from those used in the work hereby proposed, i.e., satellite\n",
      "imagery. Nonetheless, Ahmetovic et al. [3] presented a method\n",
      "combining both perspectives. Their algorithm searches for ze-\n",
      "bra crossings in satellite images. Subsequently, the candidatesare validated against Google Street View images. One of the\n",
      "limitations is that the precision of the satellite detection step\n",
      "alone, as the one proposed in this work, is \u001920% lower than\n",
      "the combined algorithm. In addition, their system takes 180ms\n",
      "to process a single image. Banich [4] presented a neural-\n",
      "network-based model, in which the author used 11 features of\n",
      "stripelets, i.e., pair of lines. The author states that the proposed\n",
      "model cannot deal with crosswalks affected by shadows and\n",
      "can only detect white crosswalks. The aforementioned meth-\n",
      "ods [1]–[4] were evaluated on manually annotated datasets that\n",
      "were usually local (i.e., within a single city or a country) and\n",
      "small (from 30 to less than 700 crosswalks).\n",
      "Another approach that has been increasingly investigated is\n",
      "the use of aerial imagery, specially satellite imagery. Heru-\n",
      "murti et al. [5] employed a circle mask template matching\n",
      "and SURF method to detect zebra crossing on aerial images.\n",
      "Their method was validated on a single region in Japan and\n",
      "the method that detects most crosswalks took 739.2 seconds\n",
      "to detect 306 crosswalks. Ghilardi et al. [6] presented a model\n",
      "to classify crosswalks in order to help the visually impaired.\n",
      "Their model is based on an SVM classiﬁer fed with manually\n",
      "annotated crosswalk regions. As most of the other related\n",
      "works, the dataset used in their work is local and small (900\n",
      "small patches, and 370 of crosswalks). Koester et al. [7]\n",
      "proposed an SVM based on HOG and LBPH features to detect\n",
      "zebra crossings in aerial imagery. Although very interesting,\n",
      "their dataset (not publicly available) was gathered manually\n",
      "from satellite photos, therefore it is relatively small (3119\n",
      "zebra crossings and \u001912500 negative samples) and local. In\n",
      "addition, their method shows a low generalization capability,\n",
      "because a model trained in one region shows low recall when\n",
      "evaluated in another (known as cross-based protocol), e.g., the\n",
      "recall goes from 95.7% to 38.4%.\n",
      "In this letter, we present a system able to automatically\n",
      "acquire and annotate zebra crossings satellite imagery, and\n",
      "train deep-learning-based models for crosswalk classiﬁcation\n",
      "in large scale. The proposed system can be used to automati-\n",
      "cally annotate crosswalks worldwide, helping systems used by\n",
      "the visually impaired, autonomous driving technologies, and\n",
      "others. This system is the result of a comprehensive study.\n",
      "This study assesses the quality of the available data, the most\n",
      "suitable model and its performance in several imagery levels\n",
      "(city, country, continent and global). In fact, this study is\n",
      "performed on real-world satellite imagery (almost 250,000\n",
      "images) acquired and annotated automatically. Given the noisy\n",
      "nature of the data, results are also compared with human\n",
      "annotated data. The proposed system is able to train models\n",
      "that achieve 97.11% on a global scale.\n",
      "2017 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including\n",
      "reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any\n",
      "copyrighted component of this work in other works.arXiv:1706.09302v2  [cs.CV]  5 Jul 2017Crosswalk Locations \n",
      "Define Paths \n",
      "Region of Interest \n",
      "Input ConvNet \n",
      "Yes No Positive Samples \n",
      "Negative Samples \n",
      " Decode and Filter \n",
      "OSM \n",
      "Overpass API \n",
      "Google Static \n",
      "Maps API \n",
      "Google Static \n",
      "Maps API Google Maps \n",
      "Directions API Fig. 1. System architecture. The input is a region (red dashed rectangle) or a set of regions of interest. Firstly, known crosswalk locations (red markers)\n",
      "are retrieved using the OpenStreetMap (OSM). Secondly, using Google Maps Directions API, paths (blue dashed arrows) between the crosswalk locations\n",
      "are deﬁned. Thirdly, these paths are decoded and the result locations are ﬁltered (only locations within the green area are accepted) in order to decrease the\n",
      "amount of wrongly annotated images. At this point, positive and negative samples can be downloaded from Google Static Maps API. Finally, this large-scale\n",
      "satellite imagery is used to train Convolutional Neural Networks (ConvNets) to perform zebra crossing classiﬁcation.\n",
      "II. P ROPOSED METHOD\n",
      "The system comprises two parts: Automatic Data Acquisi-\n",
      "tion and Annotation, and Model Training and Classiﬁcation.\n",
      "An overview of the proposed method can be seen in the\n",
      "Figure 1. Firstly, the user deﬁnes the regions of interest\n",
      "(regions where he wants to download crosswalks). The region\n",
      "of interest is given by the lower-left and the upper-right\n",
      "corners. After that, crosswalk locations within these regions\n",
      "are retrieved from the OpenStreetMap1. Subsequently, using\n",
      "the zebra crossing locations, positive and negative images\n",
      "(i.e., images that contain and do not contain crosswalks on\n",
      "it, respectively) are downloaded using the Google Static Maps\n",
      "API2. As the location of the crosswalks are known, the im-\n",
      "ages are automatically annotated. Finally, these automatically\n",
      "acquired and annotate images are used to train a Convolutional\n",
      "Neural Network from the scratch to perform classiﬁcation.\n",
      "Each process is described in details in the following subsec-\n",
      "tions.\n",
      "A. Automatic Data Acquisition and Annotation\n",
      "To automatically create a model for zebra crossing classiﬁ-\n",
      "cation, the ﬁrst step is the image acquisition. Initially, regions\n",
      "of interest are deﬁned by the user. These regions can either\n",
      "be deﬁned manually or automatically (e.g. based on a given\n",
      "address, city, etc.). The region is rectangular (with the True\n",
      "North up) and is deﬁned by four coordinate points: minimum\n",
      "latitude, minimum longitude, maximum latitude, maximum\n",
      "longitude (or South-West-North-East), i.e., the bottom-left and\n",
      "top-right corners (e.g. 40.764498, -73.981447, 40.799976, -\n",
      "73.949402 deﬁnes the Central Park, NY , USA region). For\n",
      "each region of interest, zebra crossing locations are retrieved\n",
      "from the OpenStreetMap (OSM) using the Overpass API3.\n",
      "Regions larger than 1/4 degree in either dimension are likely\n",
      "1http://www.openstreetmap.org\n",
      "2http://developers.google.com/maps/documentation/static-maps/\n",
      "3http://wiki.openstreetmap.org/wiki/Overpass APIto be split into multiple regions, as OpenStreetMap servers\n",
      "may reject these requests. Even though, most of the settled\n",
      "part of the cities around the world meets this limitation (e.g.\n",
      "the whole city of Niter ´oi, RJ, Brazil ﬁts into a single region).\n",
      "To download the zebra crossings, the proposed system uses\n",
      "the tag highway=crossing of the OSM, which is one\n",
      "of the most reliable and most used tag for this purpose. In\n",
      "possession of these crosswalk locations, the system needs to\n",
      "automatically ﬁnd locations without zebra crossing to serve\n",
      "as negative samples. As simple as it may look, negative\n",
      "samples are tricky to ﬁnd because of several factors. One\n",
      "of these is the relatively low coverage of the zebra crossing\n",
      "around the world. Another is the fact that crosswalks are\n",
      "susceptible to changes over the years. They may completely\n",
      "fade away, they can be removed and streets can change. Along-\n",
      "side these potential problems, OSM is built by volunteers,\n",
      "therefore open to contributions that may not be as accurate\n",
      "as expected. Altogether these factors indicate how noisy the\n",
      "zebra crossing locations may be. Therefore, picking up no-\n",
      "crosswalk locations must be done cautiously. The ﬁrst step\n",
      "to acquire good locations for the negative samples is to ﬁlter\n",
      "only regions that contain roads. For that, the system queries\n",
      "Google Maps Directions API for directions from a crosswalk\n",
      "to another to ensure that the points will be within a road.\n",
      "In order to lower the number of requests to the Google\n",
      "Maps Directions API, our system adds 20 other crosswalks as\n",
      "waypoints between two crosswalk points (the API has a limit\n",
      "of up to 23 waypoints and 20 ensures this limit). The number\n",
      "of waypoints does not affect the accuracy performance of the\n",
      "ﬁnal system since the same images would be downloaded but\n",
      "requiring more time. Google Maps Directions API responds\n",
      "to the request with an encoded polyline. The decoded polyline\n",
      "comprises a set of points in the requested path. The second\n",
      "step consists of virtually augmenting the number of locations\n",
      "using a ﬁxed spacing of 1:5\u000210\u00004degrees (approximately\n",
      "16 meters). All duplicate points are removed on the third step.\n",
      "Finally, the system also ﬁlters out all images too close or toofar away. Too close locations may contain crosswalks and\n",
      "could create false positives; and too far locations may have\n",
      "non-annotated crosswalks and could create false negatives.\n",
      "Therefore, locations closer than 3\u000210\u00004degrees or farther\n",
      "than 6\u000210\u00004degrees or locations outside the region requested\n",
      "are removed to decrease the occurrence of false positives and\n",
      "false negatives.\n",
      "After acquiring both positive and negative sample locations,\n",
      "the proposed system dispatches several threads to download\n",
      "the images using the Google Static Maps API. Each requested\n",
      "image is centered on the location to be requested. Also, the\n",
      "images are requested with a zoom factor equal to 20 and size\n",
      "of200\u0002225pixels. This size was empirically deﬁned to have a\n",
      "good trade-off between the image size and the ﬁeld of view of\n",
      "the area. Bigger sizes would increase the probability of having\n",
      "crosswalks away of the queried location. As a result, each\n",
      "image covers \u001922\u000225meters. Some positive and negative\n",
      "samples can be seen in the Figure 2.\n",
      "B. Model training and classiﬁcation\n",
      "Before initializing the model training, an automatic pre-\n",
      "processing operation is required. Every image downloaded\n",
      "from the Google Static Maps API contains the Google logo\n",
      "and a copyright message on the bottom. In order to remove\n",
      "these features, 25 pixels are removed from the bottom of\n",
      "each image, cropping the original images from 200\u0002225to\n",
      "200\u0002200. In possession of all cropped images, both positive\n",
      "and negative samples, the training of the model can begin.\n",
      "To tackle this large-scale problem, the proposed system uses\n",
      "a deep-learning-based model: a Convolutional Neural Network\n",
      "[8]. The architecture of the model used by the system is the\n",
      "VGG [9]. It was chosen after the evaluation of three different\n",
      "architectures: AlexNet [10], VGG and GoogLeNet [11] with\n",
      "5, 16, and 22 convolutional layers, respectively. All models\n",
      "started from pre-trained models on the ImageNet [12], i.e.,\n",
      "ﬁne-tuning. In addition, the input images were upsampled from\n",
      "200\u0002200 to256\u0002256 using bilinear interpolation and the\n",
      "subtraction of the mean of the training set was performed.\n",
      "More details on the training conﬁguration are described in the\n",
      "section III. Also, the last layer of VGG was replaced by a\n",
      "fully-connected layer comprising two neurons with randomly\n",
      "initialized weights, one for each class (crosswalk or no-\n",
      "crosswalk), and 10 times higher learning rate when compared\n",
      "to the previous layers (due to ﬁne-tuning).\n",
      "III. E XPERIMENTAL METHODOLOGY\n",
      "In this section, we present the methodology used to evaluate\n",
      "the proposed system. First, the dataset is properly introduced\n",
      "and described. Then, the metrics used to evaluate the proposed\n",
      "system are presented. Finally, the experiments are detailed.\n",
      "A. Dataset\n",
      "The dataset used in this work was automatically acquired\n",
      "and annotated using the system hereby presented. The system\n",
      "downloads satellite images using the Google Static Maps API\n",
      "and acquires the annotations using the OpenStreetMap. In\n",
      "Fig. 2. First row presents positive samples with varying layouts of crosswalks.\n",
      "Second row has some challenging positive cases (crosswalks with strong\n",
      "shadows, occlusions, aging, truncated, etc.). Third row presents different\n",
      "negative examples. Last row has some challenging negative cases.\n",
      "total, the dataset comprises 245,768 satellite images, 74,047\n",
      "images of which contain crosswalks (positive samples) and\n",
      "171,721 do not contain zebra crossings (negative samples).\n",
      "To the best of our knowledge, this is the largest satellite\n",
      "dataset for crosswalk-related tasks in the literature. In the wild,\n",
      "crosswalks can vary across different cities, different countries\n",
      "and different continents. Alongside the design variations, they\n",
      "can be presented in a variety of conditions (e.g. occluded\n",
      "by trees, cars, pedestrians; with painting fading away; with\n",
      "shadows; etc.). In order to capture all this diversity, this dataset\n",
      "comprises satellite imagery from 3 continents, 9 countries,\n",
      "and at least 20 cities. The cities were chosen considering the\n",
      "density of available annotations and the size of the city. It was\n",
      "given preference to big cities assuming that they are better\n",
      "annotated. In total, these images add up to approximately\n",
      "135,000 square kilometers, even though different images may\n",
      "partially contain a shared area. Some samples of crosswalks\n",
      "are shown in the Figure 2. A summary of the dataset can be\n",
      "seen at the Table I. It is worth noting that, even though each\n",
      "part of the dataset is named after a city, some of the selected\n",
      "regions were large enough to partially include neighboring\n",
      "towns. A more detailed description of each part of the dataset,\n",
      "region locations and scripts used for the data acquisition are\n",
      "publicly available4.\n",
      "TABLE I\n",
      "NUMBER OF IMAGES ON THE DATASETS GROUPED BY CONTINENTS\n",
      "Description Crosswalks No-Crosswalks Total\n",
      "Europe 42,554 99,461 142,015\n",
      "America 15,822 36,811 52,633\n",
      "Asia 15,671 35,449 51,120\n",
      "Total 74,047 171,721 245,313\n",
      "B. Metrics\n",
      "On this classiﬁcation task, we reported the global accuracy\n",
      "(hits per number of images) and the F1score (harmonic mean\n",
      "of precision and recall).\n",
      "4http://github.com/rodrigoberriel/satellite-crosswalk-classiﬁcationC. Experiments\n",
      "Several experiments were designed to evaluate the pro-\n",
      "posed system. Initially, three well-known Convolutional Neu-\n",
      "ral Network architectures were evaluated: AlexNet, VGG and\n",
      "GoogLeNet. The images downloaded from the Google Static\n",
      "Maps API were upsampled from 200\u0002200to256\u0002256using\n",
      "bilinear interpolation. As a data augmentation procedure, the\n",
      "images were randomly cropped ( 224\u0002224 to the VGG and\n",
      "GoogLeNet, and 227\u0002227to the AlexNet – as in the original\n",
      "networks), and the images were randomly mirrored on-the-ﬂy.\n",
      "It is known that crosswalks vary across different cities,\n",
      "countries and continents. In this context, some experiments\n",
      "were designed based on the expectation that crosswalks be-\n",
      "longing to the same locality (e.g., same city, same country,\n",
      "etc.) are more likely to present similar features. Therefore, in\n",
      "these experiments, some models were trained and evaluated on\n",
      "the same locality, and they were named with the preﬁx “intra”:\n",
      "intra-city, intra-country, intra-continent and intra-world. At the\n",
      "smaller scales (e.g., city and country) only some of the datasets\n",
      "were used (assuming the performance may be generalized), at\n",
      "higher levels all available datasets were used. These datasets\n",
      "were randomly chosen considering all cities with high anno-\n",
      "tation density. Ultimately, the intra-world experiment was also\n",
      "performed.\n",
      "Besides these intra-based experiments, in which samples\n",
      "tend to have high correlation (i.e., present more similarities),\n",
      "cross-based experiments were performed. The experiments\n",
      "named with the preﬁx “cross” are those in which the model\n",
      "was trained with a dataset and evaluated in another with\n",
      "the same level of locality, i.e., trained using data from a\n",
      "city and tested in another city. Cross-based experiments were\n",
      "performed on the three levels of locality: city, country and\n",
      "continent. Another experiment performed was the cross-level,\n",
      "i.e., the model trained using an upper level imagery (e.g., the\n",
      "world model) was evaluated in lower levels (e.g., continents).\n",
      "All the experiments aforementioned share a common setup.\n",
      "Each dataset was divided into train, validation and test sets,\n",
      "with 70%, 10% and 20%, respectively. For all the experiments,\n",
      "including the cross-based ones (cross-level included), none of\n",
      "the images in the test set were seen by the models during\n",
      "training or validation, i.e., train, validation and test sets were\n",
      "exclusive. In fact, the cross experiments used only the test-set\n",
      "of the respective dataset on which they were evaluated. This\n",
      "procedure enables fairer comparisons and conclusions about\n",
      "the robustness of the models, even though the entire datasets\n",
      "could have been used on the cross-based models. Regarding\n",
      "the training, all models were trained during 30 epochs and the\n",
      "learning rate was decreased three times by a factor of 10. The\n",
      "initial learning rate was set to 10\u00004to all three networks.\n",
      "Lastly, the annotations from the OpenStreetMap may not be\n",
      "as accurate as required due to many factors (e.g., human error,\n",
      "zebra crossing was removed, etc.). Even though, we assume\n",
      "the vast majority of them are correct and accurate. To validate\n",
      "that, an experiment using manually labeled datasets from the\n",
      "three continents (America, Europe, and Asia – 44,175 images\n",
      "in total) was performed. The experiment was designed to have\n",
      "three results: error of the automatic annotation; performanceincrease when switching from automatic labeled training data\n",
      "to manually labeled; and, correlation between the error of the\n",
      "automatic annotation and the error of the validation results.\n",
      "IV. R ESULTS\n",
      "Several experiments were performed and their accuracy and\n",
      "F1score were reported. Initially, different architectures were\n",
      "evaluated on two levels of locality. As can be seen in the\n",
      "Table II, VGG achieved the best results. It is interesting to\n",
      "notice that AlexNet, a smaller model that can be loaded into\n",
      "smaller GPUs, also achieved competitive results.\n",
      "TABLE II\n",
      "DIFFERENT ARCHITECTURES USING INTRA -BASED PROTOCOL\n",
      "Architecture Level Dataset Accuracy F1score\n",
      "AlexNetCity Milan 96.69% 94.56%\n",
      "City Turim 95.39% 92.51%\n",
      "Country Italy 96.06% 93.53%\n",
      "VGGCity Milan 97.00% 95.10%\n",
      "City Turim 96.37% 94.15%\n",
      "Country Italy 96.70% 94.66%\n",
      "GoogLeNetCity Milan 96.04% 93.41%\n",
      "City Turim 94.27% 90.78%\n",
      "Country Italy 95.22% 92.17%\n",
      "Regarding the intra-based experiments, the chosen model\n",
      "showed to be very consistent across the different levels of\n",
      "locality. The model achieved 96.9% of accuracy (on average)\n",
      "and the details of the results can be seen in the Table III.\n",
      "TABLE III\n",
      "INTRA -BASED RESULTS FOR THE VGG N ETWORK\n",
      "Level Dataset Accuracy F1score\n",
      "CityMilan 97.00% 95.10%\n",
      "Turim 96.37% 94.15%\n",
      "CountryItaly 96.70% 94.66%\n",
      "France 95.87% 93.12%\n",
      "ContinentEurope 96.72% 94.50%\n",
      "America 96.77% 94.55%\n",
      "Asia 98.61% 97.71%\n",
      "World World 97.11% 95.17%\n",
      "As expected, the cross-based models achieved a lower over-\n",
      "all accuracy when compared to the intra-based. Nevertheless,\n",
      "these models were still able to achieve high accuracies ( 94:8%\n",
      "on average, see Table IV). This can be partially explained by\n",
      "TABLE IV\n",
      "CROSS -BASED RESULTS FOR THE VGG N ETWORK\n",
      "Level Train/Val Test Accuracy F1score\n",
      "CityMilan Turim 93.47% 89.80%\n",
      "Turim Milan 95.40% 92.36%\n",
      "CountryItaly France 94.78% 91.16%\n",
      "France Italy 94.78% 91.61%\n",
      "ContinentEurope Asia 96.62% 94.33%\n",
      "Asia Europe 93.65% 88.94%inherent differences on the images between places far apart,\n",
      "i.e., different solar elevation angles cause notable differences\n",
      "on the images; the quality of the images may differ between\n",
      "cities; among other factors that tend to be captured by the\n",
      "model during the training phase.\n",
      "Cross-level experiments reported excellent results. As al-\n",
      "ready discussed, none of these models had contaminated test\n",
      "sets. Yet, on average, they achieved 96.1% of accuracy. The\n",
      "robustness of these models can be seen in the Table V, where\n",
      "all the cross-level results were summarized.\n",
      "TABLE V\n",
      "CROSS -LEVEL RESULTS FOR THE VGG N ETWORK\n",
      "TRAIN /VAL!TEST\n",
      "Cross-Level Train/Val Test Accuracy F1score\n",
      "Country!CityItaly Milan 97.17% 95.37%\n",
      "Italy Turim 96.28% 94.04%\n",
      "Continent!CountryAsia Portugal 92.71% 86.04%\n",
      "Asia Italy 94.69% 91.43%\n",
      "World!ContinentWorld Europe 96.72% 94.50%\n",
      "World America 96.66% 94.35%\n",
      "World Asia 98.65% 97.79%\n",
      "Lastly, results of manual annotations showed the proposed\n",
      "system can automatically acquire and annotate satellite im-\n",
      "agery with an average accuracy of 95:41% (4:04% false\n",
      "positive and 4:83% false negative samples), see Table VI for\n",
      "detailed automatic annotation errors. In addition, results of the\n",
      "manual annotation also showed a small improvement (2.00%\n",
      "on average, see Table VI) on the accuracy when switching\n",
      "from automatic labeled training data to manually labeled. This\n",
      "improvement is due to the decrease in noise of models trained\n",
      "using the manual labels. Some failure cases are shown in\n",
      "Figure 3. As can be seen in the Table VI, there is a correlation\n",
      "between the error of the automatic annotation and the accuracy\n",
      "of the resulting models, i.e., an increase in the annotation\n",
      "error implies in a decrease in the accuracy of models using\n",
      "automatic data. The Table VI also shows that the absolute\n",
      "differences between models validated with automatic data and\n",
      "manual data are not very high, at most 1.75% which is the\n",
      "difference for New York. This indicates that all our previous\n",
      "results of experiments evaluated with automatic data are valid,\n",
      "and would not be much different if they were evaluated with\n",
      "manually annotated data.\n",
      "TABLE VI\n",
      "IMPACT OF MANUAL ANNOTATION ON THE ACCURACY FOR THE VGG\n",
      "A: A UTOMATIC – M: M ANUAL\n",
      "DatasetAnnotation\n",
      "ErrorTrainVal / Test\n",
      "A / A A / M M / M\n",
      "Milan 2.58% 97.00% 97.71% 98.91%\n",
      "Turim 6.57% 96.37% 94.69% 98.32%\n",
      "New York 6.77% 95.49% 93.74% 96.62%\n",
      "Toyokawa 1.47% 98.16% 99.04% 99.33%\n",
      "V. C ONCLUSION\n",
      "In this letter, a scheme for automatic large-scale satellite\n",
      "zebra crossing classiﬁcation was proposed. The system au-\n",
      "Fig. 3. Failure cases. True Positive and True Negative are represented by\n",
      "green and blue markers, respectively. False Positive and False Negative are\n",
      "both represented by the red markers.\n",
      "tomatically acquires images of crosswalks and no-crosswalks\n",
      "around the world using the Google Static Maps API, Google\n",
      "Maps Directions API and OpenStreetMap. Additionally, deep-\n",
      "learning-based models are trained and evaluated using these\n",
      "automatically annotated images. Experiments were performed\n",
      "on this novel dataset with 245,768 images from 3 different\n",
      "continents, 9 countries and more than 20 cities. Experimental\n",
      "results validated the robustness of the proposed system and\n",
      "showed an accuracy of 97.11% on the global experiment.\n",
      "ACKNOWLEDGMENT\n",
      "We would like to thank UFES for the support, CAPES for\n",
      "the scholarships, and CNPq (311120/2016-4) for the grant. We\n",
      "gratefully acknowledge the support of NVIDIA Corporation\n",
      "with the donation of the Tesla K40 GPU used for this research.\n",
      "REFERENCES\n",
      "[1] V . Ivanchenko, J. Coughlan, and H. Shen, “Detecting and locating\n",
      "crosswalks using a camera phone,” in Computer Vision and Pattern\n",
      "Recognition Workshops, 2008. CVPRW’08. IEEE Computer Society\n",
      "Conference on . IEEE, 2008, pp. 1–8.\n",
      "[2] A. Haselhoff and A. Kummert, “On visual crosswalk detection for driver\n",
      "assistance systems,” in 2010 IEEE Intelligent Vehicles Symposium , June\n",
      "2010, pp. 883–888.\n",
      "[3] D. Ahmetovic, R. Manduchi, J. M. Coughlan, and S. Mascetti, “Zebra\n",
      "Crossing Spotter: Automatic Population of Spatial Databases for In-\n",
      "creased Safety of Blind Travelers,” in 17th International ACM SIGAC-\n",
      "CESS Conference on Computers & Accessibility , 2015, pp. 251–258.\n",
      "[4] J. D. Banich, “Zebra Crosswalk Detection Assisted By Neural Net-\n",
      "works,” Master’s thesis, Faculty of California Polytechnic State Uni-\n",
      "versity, California, USA, 2016.\n",
      "[5] D. Herumurti, K. Uchimura, G. Koutaki, and T. Uemura, “Urban\n",
      "Road Network extraction based on Zebra Crossing Detection from a\n",
      "very high resolution RGB aerial image and DSM data,” in Signal-\n",
      "Image Technology & Internet-Based Systems (SITIS), 2013 International\n",
      "Conference on . IEEE, 2013, pp. 79–84.\n",
      "[6] M. Ghilardi, J. Junior, and I. Manssour, “Crosswalk localization from\n",
      "low resolution satellite images to assist visually impaired people,” 2016.\n",
      "[7] D. Koester, B. Lunt, and R. Stiefelhagen, “Zebra crossing detection\n",
      "from aerial imagery across countries,” in International Conference on\n",
      "Computers Helping People with Special Needs , 2016, pp. 27–34.\n",
      "[8] Y . LeCun, B. Boser, J. S. Denker, R. E. Howard, W. Habbard, L. D.\n",
      "Jackel, and D. Henderson, “Handwritten digit recognition with a back-\n",
      "propagation network,” in Advances in Neural Information Processing\n",
      "Systems , San Francisco, CA, USA, 1990, pp. 396–404.\n",
      "[9] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\n",
      "large-scale image recognition,” arXiv preprint arXiv:1409.1556 , 2014.\n",
      "[10] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet Classiﬁcation\n",
      "with Deep Convolutional Neural Networks,” in Advances in Neural\n",
      "Information Processing Systems , 2012, pp. 1097–1105.\n",
      "[11] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,\n",
      "V . Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,”\n",
      "inProceedings of the IEEE Conference on Computer Vision and Pattern\n",
      "Recognition , 2015, pp. 1–9.\n",
      "[12] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\n",
      "Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-\n",
      "Fei, “ImageNet Large Scale Visual Recognition Challenge,” Int. Journal\n",
      "of Computer Vision , vol. 115, no. 3, pp. 211–252, 2015.\n",
      "Extracted from Deep Learning Based Oil Palm Tree Detection and Counting for High-Resolution Remote Sensing Images.pdf: remote sensing  \n",
      "Article\n",
      "Deep Learning Based Oil Palm Tree Detection\n",
      "and Counting for High-Resolution Remote\n",
      "Sensing Images\n",
      "Weijia Li1,2, Haohuan Fu1,2,3,*, Le Yu1,2and Arthur Cracknell4\n",
      "1Ministry of Education Key Laboratory for Earth System Modeling, Department of Earth System Science,\n",
      "Tsinghua University, Beijing 100084, China; liwj14@mails.tsinghua.edu.cn (W.L.);\n",
      "leyu@tsinghua.edu.cn (L.Y.)\n",
      "2Joint Center for Global Change Studies (JCGCS), Beijing 100084, China\n",
      "3National Supercomputing Center in Wuxi, Wuxi 214072, China\n",
      "4Division of Electronic Engineering and Physics, University of Dundee, Dundee DDI 4HN, UK;\n",
      "apcracknell774787@yahoo.co.uk\n",
      "*Correspondence: haohuan@tsinghua.edu.cn; Tel.: +86-158-1084-7817\n",
      "Academic Editors: Janet Nichol and Prasad S. Thenkabail\n",
      "Received: 5 November 2016; Accepted: 28 December 2016; Published: 30 December 2016\n",
      "Abstract: Oil palm trees are important economic crops in Malaysia and other tropical areas.\n",
      "The number of oil palm trees in a plantation area is important information for predicting the yield\n",
      "of palm oil, monitoring the growing situation of palm trees and maximizing their productivity,\n",
      "etc. In this paper, we propose a deep learning based framework for oil palm tree detection and\n",
      "counting using high-resolution remote sensing images for Malaysia. Unlike previous palm tree\n",
      "detection studies, the trees in our study area are more crowded and their crowns often overlap.\n",
      "We use a number of manually interpreted samples to train and optimize the convolutional neural\n",
      "network (CNN), and predict labels for all the samples in an image dataset collected through the\n",
      "sliding window technique. Then, we merge the predicted palm coordinates corresponding to the\n",
      "same palm tree into one palm coordinate and obtain the ﬁnal palm tree detection results. Based on\n",
      "our proposed method, more than 96% of the oil palm trees in our study area can be detected correctly\n",
      "when compared with the manually interpreted ground truth, and this is higher than the accuracies of\n",
      "the other three tree detection methods used in this study.\n",
      "Keywords: oil palm trees; deep learning; convolutional neural network (CNN); object detection\n",
      "1. Introduction\n",
      "Oil palm trees are important economic crops. In addition to their main use to produce palm oil,\n",
      "oil palms are also used to generate a variety of products such as plywood, paper, furniture, etc. [ 1].\n",
      "Information about the locations and the number of oil palm trees in a plantation area is important in\n",
      "many aspects. First, it is essential for predicting the yield of palm oil, which is the most widely used\n",
      "vegetable oil in the world. Second, it provides vital information to understand the growing situation of\n",
      "palm trees after plantation, such as the age or the survival rate of the palm trees. Moreover, it informs\n",
      "the development of irrigation processes and maximizes productivity [2].\n",
      "Remote sensing has played an important role in various studies on oil palm productivity, the age\n",
      "of oil palm trees and oil palm mapping, etc. [ 3–8]. In recent years, high-resolution remote sensing\n",
      "images have become increasingly popular and important for many applications including automatic\n",
      "palm tree detection. Previous palm tree or tree crown detection research has usually been based\n",
      "on traditional methods in the computer vision domain. For instance, a tree detection–delineation\n",
      "algorithm was designed for high-resolution digital imagery tree crown detection, which is based\n",
      "Remote Sens. 2017 ,9, 22; doi:10.3390/rs9010022 www.mdpi.com/journal/remotesensingRemote Sens. 2017 ,9, 22 2 of 13\n",
      "on the local maximum ﬁlter and the analysis of local transects extending outward from a potential\n",
      "tree apex [ 9]. Shafri et al. [ 10] presented an approach for oil palm tree extraction and counting from\n",
      "high spatial resolution airborne imagery data, which is composed of many parts including spectral\n",
      "analysis, texture analysis, edge enhancement, segmentation process, morphological analysis and blob\n",
      "analysis. Ke et al. [ 11] reviewed various methods for automatic individual tree-crown detection and\n",
      "delineation from passive remote sensing, including local maximum ﬁltering, image binarization, scale\n",
      "analysis, and template matching, etc. Srestasathiern et al. [ 12] used semi-variogram computation and\n",
      "non-maximal suppression for palm tree detection from high-resolution multi-spectral satellite images.\n",
      "Moreover, some researchers have also applied machine learning-based methods to palm tree\n",
      "detection studies. Malek et al. [ 2] used a scale-invariant feature transform (SIFT) and a supervised\n",
      "extreme learning machine classiﬁer to detect palm trees from unmanned aerial vehicle (UAV) images.\n",
      "Manandhar et al. [ 13] used circular autocorrelation of the polar shape matrix representation of an image\n",
      "as the shape feature and a linear support vector machine to standardize and reduce dimensions of\n",
      "the feature. This study also used a local maximum detection algorithm on the spatial distribution of\n",
      "standardized features to detect palm trees. Previous palm tree or tree crown detection studies have\n",
      "focused on detecting trees that are not very crowded and have achieved good detection results for their\n",
      "study areas. However, the performance of some of these methods would deteriorate when detecting\n",
      "palm trees in some of the regions of our study area. For instance, the local maximum ﬁlter based\n",
      "method [ 9] cannot detect palm trees correctly in regions where the trees are very young and small, as\n",
      "the local maximum of each ﬁlter does not locate around the apex of young palm trees. The template\n",
      "matching method [ 10] is not suitable for regions where palm trees are very crowded and where their\n",
      "crowns overlap.\n",
      "The convolutional neural network (CNN), a widely used deep learning model, has achieved\n",
      "great performance in many studies in the computer vision ﬁeld, such as image classiﬁcation [ 14,15],\n",
      "face recognition [ 16,17], and pedestrian detection [ 18,19], etc. In recent years, deep learning based\n",
      "methods have also been applied to hyperspectral image classiﬁcation [ 20,21], large-scale land cover\n",
      "classiﬁcation [ 22], scene classiﬁcation [ 23–25], and object detection [ 26,27], etc. in the remote sensing\n",
      "domain and achieved better performance than traditional methods. For instance, Chen et al. [ 20]\n",
      "introduced the concept of deep learning and applied the stacked autoencoder method to hyperspectral\n",
      "remote sensing image classiﬁcation for the ﬁrst time. Li et al. [ 22] built a classiﬁcation framework for\n",
      "large-scale remote sensing image processing and African land cover mapping based on the stacked\n",
      "autoencoder. Zou et al. [ 24] proposed a deep belief network based feature selection method for\n",
      "remote sensing scene classiﬁcation. Chen et al. [ 26] proposed a hybrid deep convolutional neural\n",
      "network for vehicle detection in high-resolution satellite images. Vakalopoulou et al. [ 27] proposed\n",
      "an automated building detection framework from very high-resolution remote sensing data based on\n",
      "deep convolutional neural networks.\n",
      "In this paper, we introduce the deep learning based method to oil palm tree detection for the ﬁrst\n",
      "time. We propose a CNN based framework for the detection and counting of oil palm trees using\n",
      "high-resolution remote sensing images from Malaysia. The detection and counting of oil palm trees\n",
      "in our study area is more difﬁcult than for the previous palm detection research mentioned above,\n",
      "as the trees are very crowded and their crowns often overlap. In our proposed method, we collect\n",
      "a number of manually interpreted training and test samples for training the convolutional neural\n",
      "network and calculating the classiﬁcation accuracy. Secondly, we optimize the convolutional neural\n",
      "network through tuning its main parameters to obtain the best CNN model. Then, we use the best\n",
      "CNN model obtained previously to predict the labels for all the samples in an image dataset that are\n",
      "collected through the sliding window technique. Finally, we merge the predicted palm tree coordinates\n",
      "corresponding to the same palm tree (spatial distance less than a certain threshold) into one coordinate,\n",
      "and obtain the ﬁnal palm tree detection results. Compared with the manually interpreted ground\n",
      "truth, more than 96% of the oil palm trees in our study area can be detected correctly, which is higher\n",
      "than the accuracies of the other three tree detection methods used in this study. The detection accuracyRemote Sens. 2017 ,9, 22 3 of 13\n",
      "of our proposed method is affected, to some extent, by the limited number of our manually interpreted\n",
      "samples. In our future work, more manually interpreted samples will be collected to further improve\n",
      "the overall performance of our proposed method.\n",
      "The rest of this paper is organized as follows. Section 2 presents the study area and the datasets\n",
      "of this research; Section 3 describes the ﬂowchart and the details of our proposed method; Section 4\n",
      "provides the detection results of our proposed method and the performance comparison with other\n",
      "methods; and Section 5 presents some important conclusions of this research.\n",
      "2. Study Area and Datasets\n",
      "In this research, a QuickBird image acquired on 21 November 2006 is used. The QuickBird satellite\n",
      "has one panchromatic (Pan) band with 0.6-m spatial resolution and four multi-spectral (MS) bands\n",
      "with 2.4-m spatial resolution. The Gram–Schmidt (GS) spectral sharpening fusion method [ 28], which\n",
      "is implemented in the ENVI software (version 5.3, Exelis Visual Information Solutions, Boulder, CO,\n",
      "USA), was employed to integrate Pan and MS bands to obtain a higher sharpness and spectral quality\n",
      "(0.6-m spatial resolution, four bands) dataset for further image processing and analysis.\n",
      "The study area of this research is located in the south of Malaysia, as shown in Figure 1. The manually\n",
      "interpreted samples used in this study were collected from two typical regions of our study area\n",
      "(denoted by the blue rectangles in Figure 1). To evaluate the performance of our proposed method,\n",
      "we selected another three representative regions in our study area (denoted by the red squares in\n",
      "Figure 1) and compared the detected images of these regions with the ground truth collected by\n",
      "manual interpretation.\n",
      "Remote Sens. 2017 , 9, 22  3 of 13 \n",
      " number of our manually interpreted samples. In  our future work, more manually interpreted \n",
      "samples will be collected to further improve the overall performance of our proposed method. \n",
      "The rest of this paper is organized as follows. Section 2 presents the study area and the datasets \n",
      "of this research; Section 3 describes the flowchart  and the details of our proposed method; Section 4 \n",
      "provides the detection results of our proposed me thod and the performance comparison with other \n",
      "methods; and Section 5 presents some im portant conclusions of this research. \n",
      "2. Study Area and Datasets \n",
      "In this research, a QuickBird image acquired on  21 November 2006 is used. The QuickBird \n",
      "satellite has one panchromatic (Pan) band with 0.6-m spatial resolution and four multi-spectral (MS) bands with 2.4-m spatial resolution. The Gram–Sch midt (GS) spectral sharpening fusion method \n",
      "[28], which is implemented in the ENVI software (v ersion 5.3, Exelis Visual  Information Solutions, \n",
      "Boulder, CO., USA), was employed to integrate Pan and MS bands to obtain a higher sharpness and \n",
      "spectral quality (0.6-m spatial resolution, four bands) dataset for further image processing and analysis. \n",
      "The study area of this research is located in the south of Malaysia, as shown in Figure 1. The \n",
      "manually interpreted samples used in this study were collected from two typical regions of our study area (denoted by the blue re ctangles in Figure 1). To evaluate the performance of our proposed \n",
      "method, we selected another three representative regions in our study area (denoted by the red \n",
      "squares in Figure 1) and compared the detected im ages of these regions with the ground truth \n",
      "collected by manual interpretation. \n",
      " \n",
      "Figure 1.  The study area of this research in the south of Peninsular Malaysia. The blue  rectangles \n",
      "show the two regions from which the manua lly interpreted samples are collected. The red squares \n",
      "show the three selected regions for evaluati ng the performance of our proposed method. \n",
      "3. Methods \n",
      "3.1. Overview \n",
      "The flowchart of our proposed method is shown in  Figure 2. First, th e convolutional neural \n",
      "network [14] was implemented based on the Tens orflow framework [29]. We used a number of \n",
      "training samples collected previously by manual interpretation to train the CNN, and calculated the \n",
      "classification accuracy based on a number of te st samples collected independently of training \n",
      "samples. The main parameters of the CNN (e.g., the number of kernels in the first convolutional \n",
      "Figure 1. The study area of this research in the south of Peninsular Malaysia. The blue rectangles show\n",
      "the two regions from which the manually interpreted samples are collected. The redsquares show the\n",
      "three selected regions for evaluating the performance of our proposed method.\n",
      "3. Methods\n",
      "3.1. Overview\n",
      "The ﬂowchart of our proposed method is shown in Figure 2. First, the convolutional neural\n",
      "network [ 14] was implemented based on the Tensorﬂow framework [ 29]. We used a number of\n",
      "training samples collected previously by manual interpretation to train the CNN, and calculated the\n",
      "classiﬁcation accuracy based on a number of test samples collected independently of training samples.Remote Sens. 2017 ,9, 22 4 of 13\n",
      "The main parameters of the CNN (e.g., the number of kernels in the ﬁrst convolutional layer, the\n",
      "number of kernels in the second convolutional layer and the number of hidden units in the fully\n",
      "connected layer) were adjusted continuously until we found the best combination of parameters of\n",
      "which the overall accuracy was the highest on our test samples. By tuning the parameters, we achieved\n",
      "the best CNN model and saved it for further use. Secondly, the image dataset for palm tree detection\n",
      "was collected through the sliding window technique (the window size is 17 \u000217 and the sliding\n",
      "step is three pixels). Then, we used the best CNN model obtained previously to predict the label for\n",
      "each sample in the image dataset. Thirdly, for all samples that were predicted as “palm tree” class,\n",
      "we merged the coordinates corresponding to the same palm tree sample (spatial distance less than\n",
      "a certain threshold) into one coordinate, and obtained the ﬁnal palm tree detection results.\n",
      "Remote Sens. 2017 , 9, 22  4 of 13 \n",
      " layer, the number of kernels in the second convol utional layer and the number of hidden units in the \n",
      "fully connected layer) were adjusted continuo usly until we found the best combination of \n",
      "parameters of which the overall accuracy was th e highest on our test samples. By tuning the \n",
      "parameters, we achieved the best CNN model and saved it for further use. Secondly, the image \n",
      "dataset for palm tree detection was collected th rough the sliding window technique (the window \n",
      "size is 17 × 17 and the sliding step is three pi xels). Then, we used the best CNN model obtained \n",
      "previously to predict the label for each sample in the image dataset. Thirdly, for all samples that were predicted as “palm tree” class, we merged th e coordinates corresponding to the same palm tree \n",
      "sample (spatial distance less than a certain thre shold) into one coordinate, and obtained the final \n",
      "palm tree detection results. \n",
      " \n",
      "Figure 2. The flowchart of our proposed method. \n",
      "3.2. CNN Training and Parameter Optimization \n",
      "The LeNet convolutional neural network used in this study is constructed of two convolutional \n",
      "layers, two pooling layers and a fully connected laye r, as shown in Figure 3. The input to the fully \n",
      "connected layer is the set of all features maps at the layer below. The fully connected layers \n",
      "correspond to a traditional multilayer perception constructed by a hidden layer and a logistic regression layer. We use the Rectif ied Linear Unit (ReLU) as the activation function of the CNN. In \n",
      "this research, we manually interpreted 5000 palm tree samples and 4000 background samples from two regions of our study area (denoted by the blue rectangles in Figure 1). Then, we randomly select \n",
      "7200 of these samples as the training dataset of the convolutional neural network, and the other 1800 \n",
      "samples as its test dataset. Only a sample with a palm located at its center will be labeled as “palm \n",
      "tree”. Each sample corresponds to 17 × 17 pixels  with three bands (Red, Green and Blue) selected \n",
      "from the original four bands. The main parameters of CNN are adjusted continuously until we find the best combination of parameters for which the overall accuracy is the highest from 1800 test \n",
      "samples. After parameter tuning, we achieve the best CNN model that will be used in the subsequent process of imag e dataset label prediction. Parameter \n",
      "optimization  CNN training  \n",
      "CNN model  High-resolution  \n",
      "remote sensing image  \n",
      "Image dataset  Training dataset  \n",
      "Test dataset  \n",
      "Image dataset  \n",
      " label prediction  \n",
      "Sample merging  \n",
      "Final detection \n",
      "results  \n",
      "Figure 2. The ﬂowchart of our proposed method.\n",
      "3.2. CNN Training and Parameter Optimization\n",
      "The LeNet convolutional neural network used in this study is constructed of two convolutional\n",
      "layers, two pooling layers and a fully connected layer, as shown in Figure 3. The input to the fully\n",
      "connected layer is the set of all features maps at the layer below. The fully connected layers correspond\n",
      "to a traditional multilayer perception constructed by a hidden layer and a logistic regression layer.\n",
      "We use the Rectiﬁed Linear Unit (ReLU) as the activation function of the CNN. In this research,\n",
      "we manually interpreted 5000 palm tree samples and 4000 background samples from two regions of\n",
      "our study area (denoted by the blue rectangles in Figure 1). Then, we randomly select 7200 of these\n",
      "samples as the training dataset of the convolutional neural network, and the other 1800 samples as its\n",
      "test dataset. Only a sample with a palm located at its center will be labeled as “palm tree”. Each sample\n",
      "corresponds to 17 \u000217 pixels with three bands (Red, Green and Blue) selected from the original four\n",
      "bands. The main parameters of CNN are adjusted continuously until we ﬁnd the best combination\n",
      "of parameters for which the overall accuracy is the highest from 1800 test samples. After parameter\n",
      "tuning, we achieve the best CNN model that will be used in the subsequent process of image dataset\n",
      "label prediction.Remote Sens. 2017 ,9, 22 5 of 13\n",
      "7200 training samples 1800 test samplesPalm Background Palm Background……………………\n",
      "Convolutional layer \u0001 Convolutional layer \u0001 Max-pooling layer \u0001 Max-pooling layer \u0001Fully connected layer \u0001\n",
      "Palm\n",
      " Backgroun d\n",
      " Palm\n",
      " Background\n",
      "7200 training sample s\n",
      " 1800 test sample sPalm tree Background Palm tree Background\n",
      "7200 training samples 1800 test samples\n",
      "Figure 3. The structure of the convolutional neural network (CNN).\n",
      "3.3. Label Prediction\n",
      "The image dataset for label prediction is collected through the sliding window technique, as\n",
      "shown in Figure 4. The size of the sliding window is 17 \u000217 pixels, which is consistent with the\n",
      "feature size of our training and test samples. In addition, the sliding step (the moving distance of the\n",
      "sliding window in each step) will have a great inﬂuence on the ﬁnal palm tree detection results. If the\n",
      "sliding step is too large, many palm samples will be missed and will not be detected. On the other\n",
      "hand, if the sliding step is too small, one palm sample might be detected repeatedly. Moreover, the\n",
      "process of label prediction will become slower due to the increasing number of samples in the image\n",
      "dataset, which is actually unnecessary and a waste of time. In this study, the sliding step is set as three\n",
      "pixels through experimental tests. After collecting all samples of the image dataset through the sliding\n",
      "window technique, we use the best CNN model obtained in Section 3.2 to predict the label for each\n",
      "sample in the image dataset.Remote Sens. 2017 ,9, 22 6 of 13\n",
      "… \n",
      "… \n",
      "Figure 4. The sliding window technique.\n",
      "3.4. Sample Merging\n",
      "After the labels of all samples in the image dataset are predicted, we collect the spatial coordinates\n",
      "of all the samples that are predicted as “palm tree” class. At this point, the number of predicted\n",
      "palm tree coordinates could be larger than the actual number of palm trees because one palm tree\n",
      "might correspond to several predicted palm tree coordinates. To avoid this problem, the coordinates\n",
      "corresponding to the same palm tree sample will be merged into one coordinate iteratively, as shown in\n",
      "Figure 5. Assuming that, in our study area, the spatial distance between two palm trees cannot be less\n",
      "than 8 pixels, the merging process will take six iterations. In each iteration, all groups of coordinates\n",
      "with the Euclidean distance less than a certain threshold (3, 4, 5, 6, 7, 8 pixels) will be merged into\n",
      "one coordinate. That is, the original group of coordinates will be replaced by their average coordinate.\n",
      "The remaining palm tree coordinates after the merging process represent the actual coordinates of\n",
      "detected palm trees.\n",
      "Figure 5. Sample merging.Remote Sens. 2017 ,9, 22 7 of 13\n",
      "4. Results\n",
      "4.1. Classiﬁcation Accuracy and Parameter Optimization\n",
      "In this study, the classiﬁcation accuracy of our CNN model was assessed by 1800 test samples\n",
      "collected independently from 7200 training samples. The classiﬁcation accuracy can be affected by\n",
      "many parameters, such as the size of the convolutional kernel and the max-pooling kernel, the number\n",
      "of kernels in each convolutional layer and hidden units in fully connected layers, etc. For our CNN\n",
      "model, the size of the convolutional kernel is ﬁve, the size of the max-pooling kernel is two, the size\n",
      "of mini-batch is 10 and the maximum number of iterations is 8000. We adjusted three important\n",
      "parameters to optimize the model: the number of kernels in the ﬁrst convolutional layer, the number\n",
      "of kernels in the second convolutional layer and the number of hidden units in the fully connected\n",
      "layer. Experimental results showed that we could obtain the highest overall accuracy of 95% after\n",
      "7500 iterations when the number of kernels in two convolutional layers are set as 30 and 55 and the\n",
      "number of hidden units in fully connected layers is set as 600.\n",
      "4.2. Detection Results Evaluation\n",
      "To evaluate the performance of our proposed oil palm tree detection method quantitatively,\n",
      "we calculate the precision, recall and overall accuracy of the palm tree detection results through\n",
      "comparison with the ground truth. The precision is the probability that a detected oil palm tree is\n",
      "valid, as described in Formula (1); the recall is the probability that an oil palm tree in ground truth is\n",
      "detected, as described in Formula (2); the overall accuracy is the average of precision and recall, as\n",
      "described in Formula (3). A palm is regarded as detected correctly only if the distance between the\n",
      "center of a detected palm and the center of a palm in ground truth is less than or equal to ﬁve pixels:\n",
      "Precision =The number of correctly detected palm trees\n",
      "The number of all detected objects, (1)\n",
      "Recall =The number of correctly detected palm trees\n",
      "The number of palm trees in ground truth, (2)\n",
      "Overall Accuracy =Precision +Recall\n",
      "2. (3)\n",
      "Table 1 shows that the overall accuracies of regions 1, 2 and 3 are 96.05%, 96.34% and 98.77%,\n",
      "respectively. In addition, for each of the three regions, the difference between the predicted number\n",
      "of palm trees (the number of all detected objects) and the true number of palm trees (the number of\n",
      "palm trees in ground truth) is less than 4%. These evaluation results show that our proposed method\n",
      "is effective for both palm tree detection and counting.\n",
      "Table 1. Detection results of convolutional neural network (CNN).\n",
      "Evaluation Index Region 1 Region 2 Region 3\n",
      "The number of correctly detected palm trees 1651 1607 1683\n",
      "The number of all detected objects 1729 1695 1706\n",
      "The number of palm trees in ground truth 1709 1642 1702\n",
      "Precision 95.49% 94.81% 98.65%\n",
      "Recall 96.61% 97.87% 98.88%\n",
      "Overall accuracy 96.05% 96.34% 98.77%\n",
      "5. Discussion\n",
      "To further evaluate our proposed palm tree detection method, we implemented three other\n",
      "representative existing palm trees or tree crown detection methods (i.e., Artiﬁcial Neural Network\n",
      "(ANN), template matching, and local maximum ﬁlter) and compared their detection results withRemote Sens. 2017 ,9, 22 8 of 13\n",
      "our proposed method. The procedure of the ANN based method is the same as our proposed\n",
      "method, including the ANN training, parameter optimization, image dataset label prediction, and\n",
      "sample merging.\n",
      "The local maximum ﬁlter based method [ 9] and the template matching based method [ 11] are\n",
      "two traditional tree crown detection methods. For the template matching based method, we used\n",
      "5000 manually labeled palm tree samples as the template dataset, and a 17 \u000217 window to slide\n",
      "through the whole image. We chose the CV_TM_SQDIFF_NORMED provided by OpenCV [ 30] as\n",
      "our matching method. A sliding window will be detected as a palm tree if it matches any sample in\n",
      "the template dataset (the difference between the sliding window and the template calculated by the\n",
      "CV_TM_SQDIFF_NORMED method is less than a threshold. In this study, the threshold is set as ﬁve\n",
      "through experimental tests).\n",
      "For the local maximum ﬁlter based method, we ﬁrst applied a non-overlapping 10 \u000210 local\n",
      "maximum ﬁlter to the absolute difference image of the NIR and red spectral bands. Then, we conducted\n",
      "transect sampling and a scaling scheme to obtain potential tree apexes, and adjusted the locations of\n",
      "tree apexes to the new local maximum positions.\n",
      "Finally, the outputs of the template matching based method and the local maximum ﬁlter based\n",
      "method are post-processed (described in Section 3.4) to obtain the ﬁnal palm tree detection results.\n",
      "Figures 6–8 show the detection images of each method for extracted areas of regions 1, 2 and 3,\n",
      "respectively. Each red circle denotes a detected palm tree. Each green square denotes a palm tree in\n",
      "ground truth that cannot be detected correctly. Each blue square denotes a background sample that is\n",
      "detected as a palm tree by mistake.\n",
      "Tables 2–4 show the detection results of ANN, template matching (TMPL), and local maximum\n",
      "ﬁlter (LMF), respectively. Table 5 summarizes the performance of all four methods in terms of the\n",
      "number of correctly detected palm trees. Table 6 summarizes the performance of all four methods in\n",
      "terms of precision, recall and overall accuracy (OA). The proposed method (CNN) performs better\n",
      "than any of the other three methods in the number of correctly detected palm trees and in OA.\n",
      "Generally, machine learning based approaches (i.e., CNN and ANN) perform better than traditional\n",
      "tree crown detection methods (i.e., TMPL and LMF) in our study area, especially in region 1 and region\n",
      "2. For example, the local maximum ﬁlter based method cannot detect palm trees correctly for regions\n",
      "where palm trees are very young and small (see Figure 7d), as the local maximum of each ﬁlter does\n",
      "not locate around the apex of young palm trees. The template matching method is not suitable for\n",
      "regions where the palm trees are very crowded and the canopies often overlap (see Figure 6c).\n",
      "Table 2. Detection results of artiﬁcial neural network (ANN).\n",
      "Evaluation Index Region 1 Region 2 Region 3\n",
      "The number of correctly detected palm trees 1648 1585 1679\n",
      "The number of all detected objects 1800 1725 1718\n",
      "The number of palm trees in ground truth 1709 1642 1702\n",
      "Precision 91.56% 91.88% 97.73%\n",
      "Recall 96.43% 96.53% 98.64%\n",
      "Overall accuracy 94.00% 94.21% 98.19%\n",
      "Table 3. Detection results of template matching (TMPL).\n",
      "Evaluation Index Region 1 Region 2 Region 3\n",
      "The number of correctly detected palm trees 1429 1392 1608\n",
      "The number of all detected objects 1532 1493 1684\n",
      "The number of palm trees in ground truth 1709 1642 1702\n",
      "Precision 93.28% 93.24% 95.49%\n",
      "Recall 83.62% 84.77% 94.48%\n",
      "Overall accuracy 88.45% 89.01% 94.99%Remote Sens. 2017 ,9, 22 9 of 13\n",
      "Table 4. Detection results of local maximum ﬁlter (LMF).\n",
      "Evaluation Index Region 1 Region 2 Region 3\n",
      "The number of correctly detected palm trees 1493 1397 1643\n",
      "The number of all detected objects 1719 1675 1761\n",
      "The number of palm trees in ground truth 1709 1642 1689\n",
      "Precision 86.85% 83.40% 93.30%\n",
      "Recall 87.36% 85.08% 97.28%\n",
      "Overall accuracy 87.11% 84.24% 95.29%\n",
      "Remote Sens. 2017 , 9, 22  9 of 13 \n",
      " Table 4.  Detection results of local maximum filter (LMF). \n",
      "Evaluation Index Region 1 Region 2 Region 3\n",
      "The number of correctly detected palm trees 1493 1397 1643 \n",
      "The number of all detected objects 1719 1675 1761 \n",
      "The number of palm trees in ground truth 1709 1642 1689 \n",
      "Precision 86.85% 83.40% 93.30% \n",
      "Recall 87.36% 85.08% 97.28% \n",
      "Overall accuracy 87.11% 84.24% 95.29% \n",
      " \n",
      "(a) Convolutional neural network (b) Artificial neural network  \n",
      "(c) Template matching  (d) Local maximum filter  \n",
      "Figure 6.  Detection image of each method for region 1 (extracted area). Each red circle denotes a \n",
      "detected palm tree. Each green  square denotes a palm tree in ground truth that cannot be detected \n",
      "correctly. Each blue  square denotes a background sample that is detected as a palm tree by mistake. \n",
      "  \n",
      "Figure 6. Detection image of each method for region 1 (extracted area). Each redcircle denotes\n",
      "a detected palm tree. Each green square denotes a palm tree in ground truth that cannot be detected\n",
      "correctly. Each blue square denotes a background sample that is detected as a palm tree by mistake.Remote Sens. 2017 ,9, 22 10 of 13\n",
      "Table 5. Summary of the number of correctly detected palm trees for all four methods.\n",
      "Methods Region 1 Region 2 Region 3\n",
      "CNN 1651 1607 1683\n",
      "ANN 1648 1585 1679\n",
      "TMPL 1429 1392 1608\n",
      "LMF 1493 1397 1643\n",
      "Table 6. Summary of the precision, recall and overall accuracy (OA) of all four methods.\n",
      "MethodsRegion 1 Region 2 Region 3\n",
      "Precision Recall OA Precision Recall OA Precision Recall OA\n",
      "CNN 95.49% 96.61% 96.05% 94.81% 97.87% 96.34% 98.65% 98.88% 98.77%\n",
      "ANN 91.56% 96.43% 94.00% 91.88% 96.53% 94.21% 97.73% 98.64% 98.19%\n",
      "TMPL 93.28% 83.62% 88.45% 93.24% 84.77% 89.01% 95.49% 94.48% 94.99%\n",
      "LMF 86.85% 87.36% 87.11% 83.40% 85.08% 84.24% 93.30% 97.28% 95.29%\n",
      "Remote Sens. 2017 , 9, 22  10 of 13 \n",
      " Table 5.  Summary of the number of correctly dete cted palm trees for all four methods. \n",
      "Methods Region 1 Region 2 Region 3\n",
      "CNN 1651 1607 1683 \n",
      "ANN 1648 1585 1679 \n",
      "TMPL 1429 1392 1608 \n",
      "LMF 1493 1397 1643 \n",
      "Table 6.  Summary of the precision, recall and overall accuracy (OA) of all four methods. \n",
      "Methods Region 1 Region 2 Region 3 \n",
      "Precision Recall OA Precision Recall OA Precision Recall OA \n",
      "CNN 95.49% 96.61% 96.05% 94.81% 97.87% 96.34% 98.65% 98.88% 98.77% \n",
      "ANN 91.56% 96.43% 94.00% 91.88% 96.53% 94.21% 97.73% 98.64% 98.19% \n",
      "TMPL 93.28% 83.62% 88.45% 93.24% 84.77% 89.01% 95.49% 94.48% 94.99% \n",
      "LMF 86.85% 87.36% 87.11% 83.40% 85.08% 84.24% 93.30% 97.28% 95.29% \n",
      " \n",
      " \n",
      "(a) Convolutional neural network (b) Artificial neural network  \n",
      " \n",
      "(c) Template matching  (d) Local maximum filter  \n",
      "Figure 7.  Detection image of each method for region 2 (extracted area). \n",
      "Figure 7. Detection image of each method for region 2 (extracted area).Remote Sens. 2017 ,9, 22 11 of 13\n",
      "Remote Sens. 2017 , 9, 22  11 of 13 \n",
      " (a) Convolutional neural network (b) Artificial neural network  \n",
      "(c) Template matching  (d) Local maximum filter  \n",
      "Figure 8.  Detection image of each method for region 3 (extracted area). \n",
      "6. Conclusions \n",
      "In this paper, we designed and implemented a deep  learning based framework for oil palm tree \n",
      "detection and counting from high-resolution remote sensing images. Three representative regions in \n",
      "our study area are selected for assessment of ou r proposed method. Experimental results show the \n",
      "effectiveness of our proposed method for palm tr ee detection and counting. First, the palm tree \n",
      "detection results are very similar to the manually labeled ground truth in general. Secondly, the overall accuracies of region 1, region 2 and region 3 are 96%, 96% and 99%, respectively, which are higher than the accuracies of the three other methods used in this research. Moreover, the difference between the predicted number of palm trees and the true number of palm trees is less than 4% for \n",
      "each region of the study area. In our future work , the palm tree detection results should be further \n",
      "improved through enlarging the number of manu ally interpreted samples and optimizing our \n",
      "proposed CNN based detection framework. We also wa nt to take the computation time of different \n",
      "detection methods into consideration, and explor e the deep learning based detection framework for \n",
      "larger scale palm tree detection studies. \n",
      "Figure 8. Detection image of each method for region 3 (extracted area).\n",
      "6. Conclusions\n",
      "In this paper, we designed and implemented a deep learning based framework for oil palm tree\n",
      "detection and counting from high-resolution remote sensing images. Three representative regions\n",
      "in our study area are selected for assessment of our proposed method. Experimental results show\n",
      "the effectiveness of our proposed method for palm tree detection and counting. First, the palm tree\n",
      "detection results are very similar to the manually labeled ground truth in general. Secondly, the overall\n",
      "accuracies of region 1, region 2 and region 3 are 96%, 96% and 99%, respectively, which are higher than\n",
      "the accuracies of the three other methods used in this research. Moreover, the difference between the\n",
      "predicted number of palm trees and the true number of palm trees is less than 4% for each region of\n",
      "the study area. In our future work, the palm tree detection results should be further improved through\n",
      "enlarging the number of manually interpreted samples and optimizing our proposed CNN based\n",
      "detection framework. We also want to take the computation time of different detection methods into\n",
      "consideration, and explore the deep learning based detection framework for larger scale palm tree\n",
      "detection studies.Remote Sens. 2017 ,9, 22 12 of 13\n",
      "Acknowledgments: This work was supported in part by the National Natural Science Foundation of China (Grant\n",
      "Nos. 61303003 and 41374113), the National High-Tech R&D (863) Program of China (Grant No. 2013AA01A208),\n",
      "the Tsinghua University Initiative Scientiﬁc Research Program (Grant No. 20131089356), and the National Key\n",
      "Research and Development Plan of China (Grant No. 2016YFA0602200).\n",
      "Author Contributions: Weijia Li, Haohuan Fu and Le Yu conceived of the study; Weijia Li wrote code, performed\n",
      "the analysis and wrote the article; Haohuan Fu performed the analysis and wrote the article; Le Yu and\n",
      "Arthur Cracknell performed the analysis and commented on the article.\n",
      "Conﬂicts of Interest: The authors declare no conﬂict of interest.\n",
      "References\n",
      "1. Suhaily, S.; Jawaid, M.; Khalil, H.A.; Mohamed, A.R.; Ibrahim, F. A review of oil palm biocomposites for\n",
      "furniture design and applications: Potential and challenges. BioResources 2012 ,7, 4400–4423.\n",
      "2. Malek, S.; Bazi, Y .; Alajlan, N.; AlHichri, H.; Melgani, F. Efficient framework for palm tree detection in UA V images .\n",
      "IEEE J. Sel. Top. Appl. Earth Obs. 2014 ,7, 4692–4703. [CrossRef]\n",
      "3. Cracknell, A.P .; Kanniah, K.D.; Tan, K.P .; Wang, L. Evaluation of MODIS gross primary productivity and land\n",
      "cover products for the humid tropics using oil palm trees in Peninsular Malaysia and Google Earth imagery.\n",
      "Int. J. Remote Sens. 2013 ,34, 7400–7423. [CrossRef]\n",
      "4. Tan, K.P .; Kanniah, K.D.; Cracknell, A.P . A review of remote sensing based productivity models and their\n",
      "suitability for studying oil palm productivity in tropical regions. Prog. Phys. Geogr. 2012 ,36, 655–679. [CrossRef]\n",
      "5. Tan, K.P .; Kanniah, K.D.; Cracknell, A.P . Use of UK-DMC2 and ALOS PALSAR for studying the age of oil\n",
      "palm trees in southern peninsular Malaysia. Int. J. Remote Sens. 2013 ,34, 7424–7446. [CrossRef]\n",
      "6. Kanniah, K.D.; Tan, K.P .; Cracknell, A.P . UK-DMC2 satellite data for deriving biophysical parameters of oil\n",
      "palm trees in Malaysia. In Proceedings of the IEEE International Geoscience and Remote Sensing Symposium,\n",
      "Munich, Germany, 22–27 July 2012; pp. 6569–6572.\n",
      "7. Cheng, Y.; Yu, L.; Cracknell, A.P .; Gong, P . Oil palm mapping using Landsat and PALSAR: A case study\n",
      "in Malaysia. Int. J. Remote Sens. 2016 ,37, 5431–5442. [CrossRef]\n",
      "8. Cracknell, A.P .; Kanniah, K.D.; Tan, K.P .; Wang, L. Towards the development of a regional version of MOD17\n",
      "for the determination of gross and net primary productivity of oil palm trees. Int. J. Remote Sens. 2015 ,36,\n",
      "262–289. [CrossRef]\n",
      "9. Pouliot, D.A.; King, D.J.; Bell, F.W.; Pitt, D.G. Automated tree crown detection and delineation in\n",
      "high-resolution digital camera imagery of coniferous forest regeneration. Remote Sens. Environ. 2002 ,\n",
      "82, 322–334. [CrossRef]\n",
      "10. Shafri, H.Z.; Hamdan, N.; Saripan, M.I. Semi-automatic detection and counting of oil palm trees from high\n",
      "spatial resolution airborne imagery. Int. J. Remote Sens. 2011 ,32, 2095–2115. [CrossRef]\n",
      "11. Ke, Y.; Quackenbush, L.J. A review of methods for automatic individual tree-crown detection and delineation\n",
      "from passive remote sensing. Int. J. Remote Sens. 2011 ,32, 4725–4747. [CrossRef]\n",
      "12. Srestasathiern, P .; Rakwatin, P . Oil palm tree detection with high resolution multi-spectral satellite imagery.\n",
      "Remote Sens. 2014 ,6, 9749–9774. [CrossRef]\n",
      "13. Manandhar, A.; Hoegner, L.; Stilla, U. Palm Tree Detection Using Circular Autocorrelation of Polar Shape\n",
      "Matrix. ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci. 2016 ,III-3, 465–472. [CrossRef]\n",
      "14. Krizhevsky, A.; Sutskever, I.; Hinton, G.E. ImageNet classiﬁcation with deep convolutional neural networks.\n",
      "In Proceedings of the Advances in Neural Information Processing Systems, Lake Tahoe, NV , USA,\n",
      "3–8 December 2012; pp. 1097–1105.\n",
      "15. Ciregan, D.; Meier, U.; Schmidhuber, J. Multi-column deep neural networks for image classiﬁcation.\n",
      "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Rhode Island,\n",
      "RI, USA, 16–21 June 2012; pp. 3642–3649.\n",
      "16. Sun, Y.; Wang, X.; Tang, X. Deep learning face representation from predicting 10,000 classes. In Proceedings\n",
      "of the IEEE Conference on Computer Vision and Pattern Recognition, Columbus, OH, USA, 24–27 June 2014;\n",
      "pp. 1891–1898.\n",
      "17. Li, H.; Lin, Z.; Shen, X.; Brandt, J.; Hua, G. A convolutional neural network cascade for face detection.\n",
      "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Boston, MA, USA,\n",
      "7–12 June 2015; pp. 5325–5334.Remote Sens. 2017 ,9, 22 13 of 13\n",
      "18. Ouyang, W.; Wang, X. Joint deep learning for pedestrian detection. In Proceedings of the IEEE International\n",
      "Conference on Computer Vision, Sydney, Australia, 1–8 December 2013; pp. 2056–2063.\n",
      "19. Zeng, X.; Ouyang, W.; Wang, X. Multi-stage contextual deep learning for pedestrian detection. In Proceedings of\n",
      "the IEEE International Conference on Computer Vision, Sydney, Australia, 1–8 December 2013; pp. 121–128.\n",
      "20. Chen, Y.; Lin, Z.; Zhao, X.; Wang, G.; Gu, Y. Deep learning-based classiﬁcation of hyperspectral data. IEEE J.\n",
      "Sel. Top. Appl. Earth Obs. 2014 ,7, 2094–2107. [CrossRef]\n",
      "21. Yue, J.; Zhao, W.; Mao, S.; Liu, H. Spectral–spatial classiﬁcation of hyperspectral images using deep\n",
      "convolutional neural networks. Remote Sens. Lett. 2015 ,6, 468–477. [CrossRef]\n",
      "22. Li, W.; Fu, H.; Yu, L.; Gong, P .; Feng, D.; Li, C.; Clinton, N. Stacked Autoencoder-based deep learning for\n",
      "remote-sensing image classiﬁcation: a case study of African land-cover mapping. Int. J. Remote Sens. 2016 ,\n",
      "37, 5632–5646. [CrossRef]\n",
      "23. Hu, F.; Xia, G.S.; Hu, J.; Zhang, L. Transferring deep convolutional neural networks for the scene classiﬁcation\n",
      "of high-resolution remote sensing imagery. Remote Sens. 2015 ,7, 14680–14707. [CrossRef]\n",
      "24. Zou, Q.; Ni, L.; Zhang, T.; Wang, Q. Deep learning based feature selection for remote sensing scene classification .\n",
      "IEEE Geosci. Remote Sens. Lett. 2015 ,12, 2321–2325. [CrossRef]\n",
      "25. Penatti, O.A.; Nogueira, K.; dos Santos, J.A. Do deep features generalize from everyday objects to remote\n",
      "sensing and aerial scenes domains? In Proceedings of the IEEE Conference on Computer Vision and Pattern\n",
      "Recognition Workshops, Boston, MA, USA, 7–12 June 2015; pp. 44–51.\n",
      "26. Chen, X.; Xiang, S.; Liu, C.L.; Pan, C.H. Vehicle detection in satellite images by hybrid deep convolutional\n",
      "neural networks. IEEE Geosci. Remote Sens. Lett. 2014 ,11, 1797–1801. [CrossRef]\n",
      "27. Vakalopoulou, M.; Karantzalos, K.; Komodakis, N.; Paragios, N. Building detection in very high resolution\n",
      "multispectral data with deep learning features. In Proceedings of the 2015 IEEE International Geoscience\n",
      "and Remote Sensing Symposium (IGARSS), Milan, Italy, 26–31 July 2015; pp. 1873–1876.\n",
      "28. Laben, C.A.; Brower, B.V . Process for Enhancing the Spatial Resolution of Multispectral Imagery Using\n",
      "Pan-Sharpening. U.S. Patent 6,011,875, 4 January 2000.\n",
      "29. Abadi, M.; Agarwal, A.; Barham, P .; Brevdo, E.; Chen, Z.; Citro, C.; Corrado, G.S.; Davis, A.; Dean, J.;\n",
      "Devin, M.; et al. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. 2015. Available\n",
      "online: https://arxiv.org/abs/1603.04467 (accessed on 31 August 2016).\n",
      "30. Bradski, G.; Kaehler, A. Learning OpenCV: Computer Vision with the OpenCV Library ; O’Reilly Media, Inc.:\n",
      "Sebastopol, CA, USA, 2008.\n",
      "©2016 by the authors; licensee MDPI, Basel, Switzerland. This article is an open access\n",
      "article distributed under the terms and conditions of the Creative Commons Attribution\n",
      "(CC-BY) license (http://creativecommons.org/licenses/by/4.0/).\n",
      "Extracted from Detection_and_extraction_of_road_networks_from_high_resolution_satellite_images.pdf: DETECTION AND EXTRACTION OF ROAD NETWORKS \n",
      "FROM HIGH RESOLUTION SATELLITE IMAGES \n",
      "Renatid Pdteri, Jtilien Celle and Thierry Ranchin \n",
      "Remote Sensing & Modeling Group, Ecole des Mines de Paris \n",
      "B.P. 207 - 06904 Sophia Antipolis cedex, France \n",
      "renaud.peteri@ensmp.fr \n",
      "ABSTRACT \n",
      "This article addresses the problem of road extraction from \n",
      "new high resolution satellite images. The proposed algo- \n",
      "rithm is divided in two' sequential modules : a topologi- \n",
      "cally corrcct graph of the road network is first extracted, and \n",
      "roads are then extracted as surface elements. The graph of \n",
      "the network is extracted by a following algorithm which mi- \n",
      "nimizes a cost function. The extraction algorithm makes use \n",
      "of specific active contours (snakes) combined with a multi- \n",
      "resolution analysis (MRA) for minimizing the problem of \n",
      "geometric noise. This reconstruction phase is composed of \n",
      "two steps : the extraction of road segments and the extrac- \n",
      "tion of road intersections. Results of the road network ex- \n",
      "traction are presented in order to illustrate the different steps \n",
      "of the method and future prospects are exposed. \n",
      "1. ROAD NETWORK EXTRACTION \n",
      "1.1. State of the art \n",
      "Road extraction from remotely sensed images has been \n",
      "the purpose of many works in the image processing field, \n",
      "and because of its complexity, is still a challenging topic. \n",
      "These methods arc based on generic tools of image proces- \n",
      "sing, such as linear filtering ([l]), mathematical morphology \n",
      "([2]), Markov fields ([3]), ncural networks ([4j), dynamic \n",
      "programming ([5]j, or multiresolution analysis (161 ; [7]). \n",
      "Road models are common for all authors, i.e. the radiome- \n",
      "try along one road is relatively homogeneous and contrasted \n",
      "compared to its background. Moreoverthe width of ihe road \n",
      "and its curvature are supposed to vary slowly, and the road \n",
      "network is supposed to be connex. Promising studies try to \n",
      "take thc context of the road into account in order to focus \n",
      "the extraction on the most promising regions (,[6] ; [SI). \n",
      "The recent possibility to have satellite images with a high \n",
      "spatial resolution (I meter or less) has re-boosted the in- \n",
      "terest for road extraction (especially for the applications in \n",
      "This work was suppoded hy a CNRSlffiA gran1 ofthe french Minis- \n",
      "Uy of Defence. 'The authors would like Io thank the firm G.I.M. (Geogro- \n",
      "pkic hfbrniotion Managemoil) forthe IKONOS image. urban areas). This increased resolution enables a more accu- \n",
      "rate localization of the road sides as well as its extraction as \n",
      "a surface element. In return, it generates a higher complexity \n",
      "of the image and an increase of geometric noise (vehicles, \n",
      "trees along the road, occlusions, . . .). \n",
      "2. THE PROPOSED APPROACH \n",
      "2.1. Description \n",
      "A method has been developed in order to extract and \n",
      "characterize the road network from high resolution images. \n",
      "Inputs of the algorithm, besides the high resolution satellite \n",
      "image, are models of roads (using roads properties defined \n",
      "by [7]) and propcrties of road network (such as connexity). \n",
      "Our algorithm is composed of two sequential modules (fig. I). \n",
      "Fig. 1. The methodology including topology management \n",
      "and road reconstruction \n",
      "Firstly, a topologically correct graph of the road network is \n",
      "extracted. This step aims at giving correct spatial connec- \n",
      "tions between roads as well as an approximation of their lo- \n",
      "cation. The next step is the actual road reconstruction. Due \n",
      "to the high resolution of the images, a surface reconstmc- \n",
      "tion has to be performed. This step uses the previous step \n",
      "0-7803-7750-8/03/$17.00 02003 IEEE 1 - 301 \n",
      "Authorized licensed use limited to: b-on: ISCTE. Downloaded on November 21,2023 at 13:35:08 UTC from IEEE Xplore.  Restrictions apply. of graph management as an initialization for the reconstruc- \n",
      "tion. \n",
      "In the next sections, the different modules of the process are \n",
      "more precisely described. \n",
      "2.2. Graph management \n",
      "This module intends to extract a topologically correct \n",
      "graph of the road network. It aims at giving correctly spa- \n",
      "tial connections between roads as well as an approximation \n",
      "of their location. The graph can come from a road database \n",
      "or he extracted automatically. In this second case, the graph \n",
      "extraction algorithm is based on the work of [9] : a follo- \n",
      "wing algorithm selects the hest path for the potential road \n",
      "by minimizing a cost function. The cost function evaluates \n",
      "the homogeneity of the local radiometly variance for seve- \n",
      "ral propagation directions. In order to overcome noise which \n",
      "may disturb the process, a decrease of the image resolution \n",
      "can be performed by applying a blurring filter. \n",
      "At this step, the extracted graph is topologically correct but \n",
      "the different polylines are not necessary well registered. \n",
      "From the extracted graph, polylines are then sampled and \n",
      "propagated along their normal direction in order to initia- \n",
      "lize the surface reconstruction module. \n",
      "2.3. Reconstruction module \n",
      "2.3.1. Description \n",
      "The goal of this module is to reconstruct roads as sur- \n",
      "face elements from the graph provided by the previous step. \n",
      "This module makes use of specific active contours (snaks) \n",
      "combined with a multiresolution analysis (MRA). The snake \n",
      "implementation is based on the pleedy algoritliiii described \n",
      "by[lO].Theuseofthe MRA withthe Waveletrransformen- \n",
      "ables to perform a multiresolution edge detection (see [I I]). \n",
      "It also increases the algorithm convergence by minimizing \n",
      "the problem of noise (vehicles, ground markings,. . .). \n",
      "Two sequential steps compose this reconstruction phase : \n",
      "the extraction of road segments with parallel sides and the \n",
      "extraction of road intersections. Indeed, these two objects \n",
      "present too many differences in both topology and shape to \n",
      "be processed in the same way. The frontier between the two \n",
      "kinds of process is defined by a circle including the whole \n",
      "intersection (fig. 2). \n",
      "2.3.2. Extraction ofparallel load sides \n",
      "For extracting portions of road with parallel sides, a new \n",
      "object has been defined ; the DoirbleSnake. It is composed \n",
      "of two branches which are two snakes evolving jointly. The \n",
      "DorrbleSnake energy functional has a new term E,, that \n",
      "constrains the DoribleSnake to maintain a local parallelism \n",
      "between its two branches. Moreover, their extremity points are forced to minimize their energy staying on the intersec- \n",
      "tion circle. \n",
      "‘The snake energy functional is : \n",
      "E = [aiE&,, i- $E&, + ~~q~~~~.~ +@E;/] \n",
      "i \n",
      "(1) \n",
      "where irepresents the point i of one of the branch. \n",
      "E&,,, and E:uru are internal energies that control the SM- \n",
      "ke’s shape. A special attention is paid on the image energy \n",
      "term E;j,nagc, as it the one that attracts the snake to the \n",
      "object of interest. The image energy is computed with the \n",
      "wavelet coefficients at different spatial resolutions : \n",
      "Ei ZJi”..,, = -Jlqjm2 + Iw;j.f(i)l? (2) \n",
      "where W,;’f(i) are the coordinates ofthe wavelet trans- \n",
      "form and j is the resolution in a dyadic analysis. \n",
      "A more detailed description of the different energy terms \n",
      "can be found in ([12]) \n",
      "2.3.3. Extraction of road intersections \n",
      "Once road segment extraction is finished, the intersec- \n",
      "tion extraction starts. They are extracted by simple snakes \n",
      "which are initialized by pairing extremity points of the Dorr- \n",
      "ble3rake.y (see fig. 2). This ZntersectionSnake has its extre- \n",
      "mities fixed and is constrained not to go out of the circle. . \n",
      "Fig. 2. Initialization of the intersection snakes \n",
      "2.3.4. Reconslniclion algorirhniflowchart \n",
      "Fig. 3 represents the different steps of the reconstruction \n",
      "algorithm. From the original image, a multiresolution am- \n",
      "lysis is performed, giving several approximation images at \n",
      "coarser resolutions and several wavelet coefficient images. \n",
      "DoubleSnakes are first applied on the coarsest resolution \n",
      "image, then on each intermediate resolution images until \n",
      "they run on the original resolution image. Coefficients of \n",
      "I - 302 \n",
      "Authorized licensed use limited to: b-on: ISCTE. Downloaded on November 21,2023 at 13:35:08 UTC from IEEE Xplore.  Restrictions apply. the different energy terms are adapted to the image resolu- \n",
      "tion. For instance, at the coarsest resolution, a high value \n",
      "of the image term allows the snake to be attracted from far. \n",
      "Refining the estimation on a finer resolution image is then \n",
      "done by releasing image constraints and increasing the im- \n",
      "portance of the internal energy. \n",
      "Once DoubleSnukes have all minimized their energy, the In- \n",
      "rersectio!rSnukes are initialized and the extraction starts in \n",
      "the same multiresolution process. \n",
      "Pig. 3. Reconstruction algorithm \n",
      "The algorithm stops when all snakes are in an equilibrium \n",
      "state. \n",
      "3. RESULTS AND PROSPECTS \n",
      "3.1. Real-case study \n",
      "As the graph module is currently under integration, only \n",
      "the reconstruction module will be illustrated in the follo- \n",
      "wing example. Image of figure 4 comes from the IKONOS \n",
      "satellite which has spatial resolution of 1 meter in the pan- \n",
      "chromatic mode. \n",
      "Fig. 4. Original image with the road graph This image includes a crossroad composed of two main roads \n",
      "on which the input graph (coming from a database or ma- \n",
      "nually given) has been superposed. \n",
      "Images 5, 6 and 7 represent the different steps of the algo- \n",
      "rithm. On image 5, polylines of the input graph have been \n",
      "sampled and propagated. The circle delimiting the two types \n",
      "of the reconstruction process is also drawn. \n",
      "Fig. 5. Afler propagation of the polylines \n",
      "At step of image 6, portions of roads with parallel sides have \n",
      "been extracted by Doub1eSmke.s after running on the dif- \n",
      "ferent resolutiou images. \n",
      "Fig. 6. After the parallel contour extraction \n",
      "The extraction final result afler the intersection process is \n",
      "represented on image 7. \n",
      "Contours have globally been extracted with a good preci- \n",
      "sion (except the sharp edge of the intersection, partly due \n",
      "to its poor image force). The use of the multiresolution ap- \n",
      "proach has prevented the snakes from being trapped by the \n",
      "geometric noise such as ground marking. \n",
      "I - 303 \n",
      "Authorized licensed use limited to: b-on: ISCTE. Downloaded on November 21,2023 at 13:35:08 UTC from IEEE Xplore.  Restrictions apply. Fig. 7. After the intersection extraction \n",
      "3.2. Conclusion and future prospects \n",
      "This article describes a new method for extracting the \n",
      "road network from high resolution satellite images. A topo- \n",
      "logically correct graph or the road network is first extrac- \n",
      "ted, and roads are then extracted as surface elements. ‘The \n",
      "graph of the network is extracted by a following algorithm \n",
      "[Y] which minimizes a cost function. The extraction algo- \n",
      "rithm makes use of specific snakes combined with a mul- \n",
      "tiresolution analysis. Current works aim at increasing the \n",
      "robustness of the algorithm, particularly in noisy environ- \n",
      "ments such as urban areas. The use the wavelet coefficients \n",
      "deriving from the MRA for extracting texture information \n",
      "proper to roads could constraint the snake evolution. Some \n",
      "contextual information (such as building or car alignment) \n",
      "can also he a clue for the extraction. \n",
      "Results on test images are encouraging. However, results \n",
      "should be validated and characterized using quantitative cri- \n",
      "teria described in [13]. \n",
      "4. REFERENCES \n",
      "[I] J.F. Wang and P.J. Howarth, “Automated road network \n",
      "extraction from landsat tm imagery,” in Pmceedings \n",
      "qf the annual ASPRS/ACSM Conventionl. Baltimore. \n",
      "MU, USA, 1987, vol I, pp 429-438 [2] 1. Destivdl, “Recherche automatique de rkseaux \n",
      "IinCaires sur des images spot,” Societe! Franpise de \n",
      "Photogranimehie er de TeWiktecrion, vol. 66, pp. 5- \n",
      "16, 1987. \n",
      "p] N. Merlet ,and J. ZBrubia, “New prospects in line de- \n",
      "tection by dynamic programming,” IEEE Transactions \n",
      "ow Pattern Analvsis and Machine Intelligence, vol. 18, \n",
      "no. 4, pp. 426430, 1996. \n",
      "141 U. Bhattacharya and S.K. Parui, “An improved back- \n",
      "propagation neural network for detection of road-like \n",
      "features in satellite imagery;’ International Jorirnal of \n",
      "RenroteSensiiig,vol. 18, no. 16,pp. 3379-3394, 1997. \n",
      "[5] A. Gruen and H. Li, “Road extraction from aerial \n",
      "and satellite images by dynamic programming:’ IS- \n",
      "PRS Journal ofPhotograsmretw and Reinole Sensing, \n",
      "vol. 50, no. 4, pp. 11-20, 1995. \n",
      "[6] A. Baumgartner, C. Steger, H. Mayer, W. Eckstein, \n",
      "and E. Heinrich, “Automatic road extraction based on \n",
      "multi-scale, grouping, and context;‘ Photogi’anrnietric \n",
      "Engineering and Remote Sensing, vol. 65, no. 7, pp. \n",
      "7777785,1999. \n",
      "[7] 1. Couloigner and T. Ranchin, “Mapping of urban \n",
      "areas : A multiresolution modeling approach for semi- \n",
      "automatic extraction of streets,“ Phologramsietric En- \n",
      "gineering and Reniofe Sensing, vol. 66, no. 7, pp. 867- \n",
      "874,2000. \n",
      "[8] R. Ruskone, Extraction untomatique du re!seau routier \n",
      "par inteqwetution locale du contate : application a la \n",
      "pmducfion de donnkes cartogvaphiques, Ph.D. thesis, \n",
      "Universite de Marne-la-Vallee, 1996. \n",
      "[9] S. Airault and 0. Jamet, “Detection et restitution au- \n",
      "tomatique du riseau routier sur des images &riennes,” \n",
      "Trailernent du Signal, vol. 12, no. 2, pp. 189-200, \n",
      "1995. \n",
      "[IO] D.J. Williams and M. Shah, “A fast algorithm for ac- \n",
      "tive contours and curvature estimation,“ in CVIP : \n",
      "Image Understanding, January 1992, vol. 55, pp. 14- \n",
      "26. \n",
      "[I I] S.G. Mallat, A Wavelet ToiirofSignal Pmcessing, AP \n",
      "Professional, London, 1997. \n",
      "[I21 R. Peteri and T. Ranchin, “Multiresolution snakes \n",
      "for urban road extraction from ikonos and quickhird \n",
      "images,’’ in 23nd EARSeL Annual Sviirposiurn ”Re- \n",
      "niote Sensing in Transition ”, Ghent, Belgium, 2-5 \n",
      "June 2003, to appear. \n",
      "[I31 R. Peteri, I. Couloigner, and T. Ranchin, “How to as- \n",
      "sess quantitatively road extracted from high resolution \n",
      "imagely 1,” Phofogramnetric Engrireering & Reiirofe \n",
      "Sensing, 2003, to appear. \n",
      "I - 304 \n",
      "Authorized licensed use limited to: b-on: ISCTE. Downloaded on November 21,2023 at 13:35:08 UTC from IEEE Xplore.  Restrictions apply. \n",
      "Extracted from Object-Based_Convolutional_Neural_Network_for_High-Resolution_Imagery_Classification.pdf: 3386 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV A TIONS AND REMOTE SENSING, VOL. 10, NO. 7, JULY 2017\n",
      "Object-Based Convolutional Neural Network for\n",
      "High-Resolution Imagery Classiﬁcation\n",
      "Wenzhi Zhao, Shihong Du, and William J. Emery , Fellow, IEEE\n",
      "Abstract —Timely and accurate classiﬁcation and interpretation\n",
      "of high-resolution images are very important for urban planning\n",
      "and disaster rescue. However, as spatial resolution gets ﬁner, it is in-\n",
      "creasingly difﬁcult to recognize complex patterns in high-resolution\n",
      "remote sensing images. Deep learning offers an efﬁcient strategy\n",
      "to ﬁll the gap between complex image patterns and their semantic\n",
      "labels. However, due to the hierarchical abstract nature of deep\n",
      "learning methods, it is difﬁcult to capture the precise outline of\n",
      "different objects at the pixel level. To further reduce this prob-\n",
      "lem, we propose an object-based deep learning method to accu-\n",
      "rately classify the high-resolution imagery without intensive hu-\n",
      "man involvement. In this study, high-resolution images were used\n",
      "to accurately classify three different urban scenes: Beijing (China),\n",
      "Pavia (Italy), and Vaihingen (Germany). The proposed method is\n",
      "built on a combination of a deep feature learning strategy and an\n",
      "object-based classiﬁcation for the interpretation of high-resolution\n",
      "images. Speciﬁcally, high-level feature representations extracted\n",
      "through the convolutional neural networks framework have been\n",
      "systematically investigated over ﬁve different layer conﬁgurations.\n",
      "Furthermore, to improve the classiﬁcation accuracy, an object-\n",
      "based classiﬁcation method also has been integrated with the deep\n",
      "learning strategy for more efﬁcient image classiﬁcation. Experi-\n",
      "mental results indicate that with the combination of deep learning\n",
      "and object-based classiﬁcation, it is possible to discriminate differ-\n",
      "ent building types in Beijing Scene, such as commercial buildings\n",
      "and residential buildings with classiﬁcation accuracies above 90%.\n",
      "Index T erms —Convolutional neural network (CNN), deep\n",
      "learning, high-resolution image, image classiﬁcation.\n",
      "I. I NTRODUCTION\n",
      "REMOTE sensing imagery has provided a real-time and\n",
      "low-cost means to map urban land cover during the last\n",
      "few decades. The recent availability of submeter resolution im-\n",
      "agery from advanced satellite sensors, such as WorldView-3\n",
      "and GaoFen, can provide new opportunities for detailed urban\n",
      "land cover mapping at the object level (such as commercial\n",
      "buildings and residential buildings). However, the complexity\n",
      "Manuscript received November 23, 2016; revised January 11, 2017 and Febru-\n",
      "ary 13, 2017; accepted March 5, 2017. Date of publication March 29, 2017;\n",
      "date of current version July 26, 2017. This work was supported by the National\n",
      "Natural Science Foundation of China under Grant 41471315. (Corresponding\n",
      "author: Shihong Du.)\n",
      "W. Zhao and S. Du are with the Beijing Key Laboratory of Spatial In-\n",
      "formation Integration and Its Applications, Institute of Remote Sensing and\n",
      "Geographic Information System, Peking University, Beijing 100871, China\n",
      "(e-mail: w.zhao@pku.edu.cn; dshgis@hotmail.com).\n",
      "W. J. Emery is with the Colorado Center for Astrodynamics Research, Uni-\n",
      "versity of Colorado, Boulder CO 80303 USA (e-mail: emery@colorado.edu).\n",
      "Color versions of one or more of the ﬁgures in this paper are available online\n",
      "at http://ieeexplore.ieee.org.\n",
      "Digital Object Identiﬁer 10.1109/JSTARS.2017.2680324of urban imagery increases sharply as the observation scale gets\n",
      "ﬁner. More efﬁcient high-resolution image classiﬁcation meth-\n",
      "ods need to be proposed in order to efﬁciently perform urban\n",
      "high-resolution imagery classiﬁcation.\n",
      "Remote sensing imagery acquired by advanced satellite sen-\n",
      "sors has the potential for detailed mapping of these urban\n",
      "images. However, rich spatial information presented in high-\n",
      "resolution images also hinders accurate interpretation [1], [2].\n",
      "In fact, objects in urban areas are commonly composed of differ-\n",
      "ent construction materials, which can produce confused feature\n",
      "representations in the spectral and spatial domains. For one\n",
      "thing, small objects are cojointly distributed around target ob-\n",
      "jects (such as antennas or chimneys on building roofs) which\n",
      "results in a strong intraclass variation as the spatial resolution\n",
      "gets ﬁner. For another, man-made objects share similar spectral\n",
      "properties (such as parking lots and roofs with similar construc-\n",
      "tion materials), which makes it even harder to accurately classify\n",
      "high-resolution images [3], [4]. As a consequence, the spectral\n",
      "and spatial responses from ground objects in urban scenes ex-\n",
      "hibit complex patterns, especially for high-resolution images.\n",
      "The main purpose of this study is to transform the inefﬁcient\n",
      "process of manually designed features into automatic feature\n",
      "learning by employing deep learning [5], for the efﬁcient clas-\n",
      "siﬁcation of high-resolution satellite imagery. However, due to\n",
      "the hierarchical structure of deep learning, deeply learned fea-\n",
      "tures suffer greatly from abstraction and often fail to capture\n",
      "the objects’ contours at the pixel level. Therefore, pixel-level\n",
      "segments are combined to further improve the performances of\n",
      "interpretation in complex urban scenes [6]–[8].\n",
      "The proposed method is based on the analysis of deep features\n",
      "extracted from high-resolution images with convolutional neural\n",
      "networks (CNN). To account for the boundary information of ur-\n",
      "ban scenes, an object-based classiﬁcation method has been used\n",
      "for imagery interpretation for the combination of deeply learned\n",
      "features. Speciﬁcally, deep features have been computed over\n",
      "the ﬁxed receptive window with a ﬁve-layer CNN framework. In\n",
      "addition, three different segment scales have been employed in\n",
      "evaluating the effectiveness of object-based classiﬁcation with\n",
      "extracted deep features.\n",
      "The remainder of this paper is organized as follows. Re-\n",
      "lated work on high-resolution image feature exploration and\n",
      "classiﬁcation is outlined in Section II, and the datasets are de-\n",
      "scribed in Section III. Detailed information about CNN-based\n",
      "deep learning feature exploration and the object-based classiﬁ-\n",
      "cation are introduced in Section IV. Experimental results and\n",
      "analysis of the deep features as well as segmentation scales of\n",
      "1939-1404 © 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\n",
      "See http://www.ieee.org/publications standards/publications/rights/index.html for more information.\n",
      "Authorized licensed use limited to: b-on: ISCTE. Downloaded on December 03,2023 at 19:35:37 UTC from IEEE Xplore.  Restrictions apply. ZHAO et al. : OBJECT-BASED CONVOLUTIONAL NEURAL NETWORK FOR HIGH-RESOLUTION IMAGERY CLASSIFICA TION 3387\n",
      "objects are discussed in Section V, followed by the conclusion in\n",
      "Section VI.\n",
      "II. R ELA TED WORK\n",
      "A. Feature Design V ersus Feature Learning\n",
      "Over the past few decades, intensive studies have focused on\n",
      "high-resolution image classiﬁcation with handcrafted features\n",
      "elaborated from both spectral and spatial domains. For spectral\n",
      "information, the brightness of each image band is usually taken\n",
      "as the primary feature for recognition of various objects in re-\n",
      "mote sensing imagery. Later, semantic spectral features which\n",
      "contain physical meanings have been proposed for efﬁcient clas-\n",
      "siﬁcation of certain objects, where it strengthened the reﬂectance\n",
      "discrepancy of different objects on speciﬁc wavelengths, such\n",
      "as the normalized difference vegetation index (NDVI). Further\n",
      "exploration of image texture demonstrated that texture-based\n",
      "descriptors characterize spectral variations that can provide sup-\n",
      "plementary information for efﬁcient image classiﬁcation, such\n",
      "as statistical descriptors based on the gray-level co-occurrence\n",
      "matrix (GLCM) [9], [10]. Compared with their spectral proper-\n",
      "ties, high-resolution imagery contains much richer information\n",
      "in the spatial domain. To characterize these spatial features,\n",
      "Benediktsson et al. proposed extended morphological proﬁles\n",
      "for high-resolution imagery classiﬁcation in urban areas [11]. It\n",
      "efﬁciently captures spatial information by implementing mor-\n",
      "phological operations (open and close) on high-resolution im-\n",
      "ages. Similarly, spatial ﬁlters (such as Gabor ﬁlters [12]) and\n",
      "wavelet analysis [13] were also proposed for the extraction of\n",
      "spatial features in the context of high-resolution images. How-\n",
      "ever, it usually requires lots of experience and expert knowledge\n",
      "for end-users to deﬁne such elaborate features. Moreover, even\n",
      "after the complicated design process of various features, it is still\n",
      "difﬁcult to ﬁnd the most effective features for the recognition of\n",
      "different objects. To illustrate the complexity of high-resolution\n",
      "imagery, a typical Worldview-2 image is presented here as Fig. 1.\n",
      "Due to the strong intraclass variation of buildings (caused by\n",
      "a sunlit wall and chimney), traditional feature representations\n",
      "have shown much more mixture and variation than the deep\n",
      "learning ones. For this reason, it is necessary to explore more\n",
      "efﬁcient and representative image features from high-resolution\n",
      "images in order to accurately recognize objects with complex\n",
      "patterns, such as complex buildings in an urban area.\n",
      "How to develop automatic classiﬁcation schemes for feature\n",
      "learning has become one of the most signiﬁcant topics in image\n",
      "classiﬁcation over the last few years. Instead of handcrafted fea-\n",
      "ture design, Cheriyadat [14] introduced a sparse coding scheme\n",
      "for learning of high-resolution image features with predeﬁned\n",
      "ﬁlter banks. It indicated that handcrafted features have a certain\n",
      "level of redundancy which leads to lower interpretation accura-\n",
      "cies. Also, Tuia et al. [15] proposed a feature learning model by\n",
      "using sparse-constrained support vector machine (SVM). Re-\n",
      "gardless of the compulsory step to select for predeﬁned features\n",
      "of conventional methods, this method can automatically dis-\n",
      "cover the relevant features in the potentially inﬁnite space of\n",
      "image features and choose them in a heuristic way. It concluded\n",
      "that the automatic learning scheme can keep image featuresmore compact, discriminative, and robust than human visual\n",
      "selection methods, thus resulting in better classiﬁcation accu-\n",
      "racies. In practical applications, it demonstrates that nonlinear\n",
      "features are more effective for class discrimination due to the\n",
      "existence of nonlinear class boundaries. Therefore, besides us-\n",
      "ing a sparse coding strategy, the exploration of image features\n",
      "through nonlinear transformations also has been proven to be\n",
      "effective [16]. Furthermore, Tuia et al. [17] proposed a sparse\n",
      "and hierarchical feature learning model to ﬁnd efﬁcient data rep-\n",
      "resentations for a good classiﬁcation. In this model, the selected\n",
      "features could be repeatedly used for ﬁltering at the next feature\n",
      "generation step, thus, producing more representative features\n",
      "with higher nonlinearity. However, the above methods still re-\n",
      "quire a certain level of experience for end-users, such as the\n",
      "deﬁnition of the possible feature space and a corresponding\n",
      "selection criteria.\n",
      "Deep learning [18], [19] is one of the signiﬁcant advances\n",
      "in artiﬁcial intelligence, and it has shown great potential for\n",
      "discriminative feature learning without human intervention. In-\n",
      "spired by the hierarchical cognition process of the human brain,\n",
      "deep learning can automatically generate robust and represen-\n",
      "tative features layer by layer in neural networks [20], [21]. Dis-\n",
      "tinctly different from low-level feature representations, deeply\n",
      "learned features are generally more general and robust, and\n",
      "it has demonstrated great effectiveness in image classiﬁcation,\n",
      "such as face recognition [22] and scene classiﬁcation [23]. In the\n",
      "remote sensing ﬁeld, several studies have focused on imagery\n",
      "classiﬁcation using deep learning models, such as stacked au-\n",
      "toencoder (SAE) and convolutional neural network (CNN). But,\n",
      "the original SAE focused on extracting one-dimensional spec-\n",
      "tral features which probably is not sufﬁcient for high-resolution\n",
      "image interpretation. Therefore, Chen et al. [24] improved the\n",
      "SAE model by introducing spatial features for the efﬁcient clas-\n",
      "siﬁcation of hyperspectral images. At the same time, the CNN\n",
      "algorithm became popular for high-resolution image classiﬁca-\n",
      "tion due to its effectiveness in spatial feature exploration [3],\n",
      "[25]–[27]. Furthermore, to ﬁt the character of remote sensing\n",
      "imagery, Zhao and Du [28] proposed a multiscale strategy based\n",
      "on the CNN model to retrieve the information in high-resolution\n",
      "imagery. Although the CNN-based method is efﬁcient for ro-\n",
      "bust spatial feature extraction, two major ﬂaws of the basic CNN\n",
      "framework for high-resolution image classiﬁcation also need to\n",
      "be clariﬁed. On one hand, the CNN framework feeds on la-\n",
      "beled image patches of ﬁxed sizes and outputs feature vectors\n",
      "using layer-wise activation and abstraction. Since the output\n",
      "features are highly abstract and usually without speciﬁc spatial\n",
      "arrangement, it is difﬁcult to predict the precise contour of target\n",
      "objects in image and producing blurred edge prediction results.\n",
      "On the other hand, deep features are learned from local images\n",
      "patches, which, regardless of the contextual image information,\n",
      "could produce misclassiﬁcation results, as shown here in Fig. 2.\n",
      "B. Per-Pixel V ersus Object-Based Classiﬁcation\n",
      "As the spatial resolution gets ﬁner, the detailed image data\n",
      "exhibit complicated urban patterns in both spectral and spatial\n",
      "domains. Using the per-pixel classiﬁcation of high-resolution\n",
      "Authorized licensed use limited to: b-on: ISCTE. Downloaded on December 03,2023 at 19:35:37 UTC from IEEE Xplore.  Restrictions apply. 3388 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV A TIONS AND REMOTE SENSING, VOL. 10, NO. 7, JULY 2017\n",
      "Fig. 1. Measurements of different feature representatives on a Worldview-2 study site. The high-dimensional feature representations were project ed into two-\n",
      "dimensional space for illustration. (a) Worldview-2 image; (b) GLCM feature representation; (c) EMAP feature representation; and (d) CNN feature r epresentation\n",
      "(generated by ﬁve-layer CNN).\n",
      "Fig. 2. Classiﬁcation results of WorldView-2 image (after pan sharpening)\n",
      "with different strategies.\n",
      "images may lead to poorer interpretation results due to the “salt\n",
      "and pepper effect” [29], as shown in Fig. 2(b). To reduce the\n",
      "pixel-level spectral heterogeneity and improve the classiﬁcation\n",
      "performance, an object-based approach can be used to efﬁciently\n",
      "delineate and classify high-resolution imagery [30]. Object-\n",
      "based classiﬁcation method takes image segments as building\n",
      "blocks for the overall image analysis. In contrast to pixel-based\n",
      "approaches, image objects mainly have two characteristics:\n",
      "1) They are relatively homogeneous and 2) they can provide rich\n",
      "image features. On one hand, objects integrated with contextual\n",
      "information can greatly reduce local spectral variation and over-\n",
      "come the so-called “pepper & salt” effect. On the other hand,\n",
      "objects in images opens a new gate to access the target of in-\n",
      "terest, thus more discriminant semantic features can be deﬁned,\n",
      "such as an objects’ shape and sizes. So far, object-based classi-\n",
      "ﬁcation method has shown great potential for efﬁcient mapping\n",
      "of high-resolution images, especially for complex urban scenes.\n",
      "However, the process of object-based classiﬁcation is usually\n",
      "very complicated and needs a certain level of expert knowledge\n",
      "to produce satisfactory results. Speciﬁcally, three factors weigh\n",
      "heavily in terms of object-based classiﬁcation accuracy: 1) the\n",
      "segmentation scale, 2) feature extraction and selection, and 3)\n",
      "classiﬁcation rules. A possible solution to these problems is to\n",
      "incorporate deep learning with the object-based method for fea-\n",
      "ture self-learning and automatic classiﬁcation of complex urban\n",
      "Fig. 3. Outline of the proposed method. High-resolution images and their\n",
      "reference maps are combined to train CNN frameworks. Segmentation results\n",
      "are used to improve the performance of pixel-level boundary prediction.\n",
      "high-resolution imagery without much supervision. The general\n",
      "outline of this process is presented in Fig. 3.\n",
      "III. M ETHODOLOGY\n",
      "A. Deep Feature Learning Through CNN\n",
      "The complexity of high-resolution images causes traditional\n",
      "human-dependent classiﬁcation methods to fail due to the lim-\n",
      "ited representation power of handcrafted features. CNN as the\n",
      "core of deep learning has shown great potential for robust au-\n",
      "tomatic feature extraction and complex object recognition in\n",
      "high-resolution images [31]–[33].\n",
      "To obtain deep feature representations, two parts of trainable\n",
      "parameters for a CNN framework should be determined, i.e.,\n",
      "the ﬁlters Wand the biases b, which are collectively denoted θ.\n",
      "During the training stage, a CNN framework fwithLlay-\n",
      "ers feeds with training samples Xi,i∈{ 1,...,N }and is\n",
      "formulated\n",
      "f(X;θ)= WLhL−1+bl (1)\n",
      "where hl,l∈{ 1,...,L −1}denotes the vector of hidden\n",
      "units at the lth layer. Particularly, h0represents the original\n",
      "input data, as shown in Fig. 4.\n",
      "More speciﬁcally, when training a CNN, the input image is\n",
      "ﬁrst connected with convolution layers by a set of convolution\n",
      "kernel banks. For the convolutional layers, the kernels were\n",
      "denoted as Wland are combined with the bias terms blto\n",
      "convolute the input image. After that, a point-wise nonlinear\n",
      "activation function g(·)(typically the tan hfunction) is deployed\n",
      "before the ﬁnal output of this layer. Then, the spatial pooling\n",
      "layer usually follows to generate the dominant features over\n",
      "Authorized licensed use limited to: b-on: ISCTE. Downloaded on December 03,2023 at 19:35:37 UTC from IEEE Xplore.  Restrictions apply. ZHAO et al. : OBJECT-BASED CONVOLUTIONAL NEURAL NETWORK FOR HIGH-RESOLUTION IMAGERY CLASSIFICA TION 3389\n",
      "Fig. 4. Framework of a traditional CNN. Convolution layer interspersed with\n",
      "pooling layer in this hierarchical structure. Full connection is found at the last\n",
      "layer and linked with the classiﬁer.\n",
      "nonoverlapping windows for each feature map. To formulate\n",
      "the feed-forward process, we have\n",
      "hl=pool (g(hl−1∗Wl+bl)). (2)\n",
      "Once the parameters θare trained, the unlabeled datasets Yj,\n",
      "j∈{ 1,2,...,N }can be encoded by\n",
      "Fj=f(Yj,θ). (3)\n",
      "As mentioned above, deep features extracted by the CNN\n",
      "framework are generally robust and effective for complex image\n",
      "pattern descriptions, especially for the case of high-resolution\n",
      "urban scenes. Unlike the traditional CNN-based image classi-\n",
      "ﬁcation methods, we focus on exploring deep features as local\n",
      "patch descriptors. However, deep features with high-level ab-\n",
      "stractions naturally fail to detect the edges of complex objects\n",
      "on the pixel level. Object-based classiﬁcation methods interpret\n",
      "high-resolution images with segmented objects which can pre-\n",
      "serve objects’ edges and reduce spectral variation effects. There-\n",
      "fore, it is widely suggested that deep features should combine\n",
      "with object-based image analysis methods for better classiﬁca-\n",
      "tion performances.\n",
      "B. Object-Based Classiﬁcation With Deep Features\n",
      "Although, the CNN-based methods are efﬁcient for complex\n",
      "pattern description, the deep features generated from multilayer\n",
      "activations are usually highly abstracted, and thus, failed to\n",
      "predict objects’ contours [34], [35]. In contrast to the patch-\n",
      "based image classiﬁcation methods (e.g., CNN-based method),\n",
      "the object-based methods can effectively classify image objects\n",
      "using image segments with precise boundaries. Segments are\n",
      "homogeneous regions which are generated by one or more ho-\n",
      "mogeneity measurements in feature space (typically, spectral\n",
      "space). Therefore, image segments can transform the whole im-\n",
      "age into meaningful regions while preserving the precise edges\n",
      "of targets of interest. Furthermore, image segments have ad-\n",
      "ditional information compared to pixels or image patches in\n",
      "terms of spatial and contextual extensions (objects’ shape and\n",
      "topologies). Thus, it is natural to combine the deep features\n",
      "with object-based classiﬁcation for the accurate interpretation\n",
      "of high-resolution images.\n",
      "In this paper, highly abstracted deep features are combined\n",
      "with image segments for precise mapping of complex remote\n",
      "sensing images. Speciﬁcally, the multiresolution segmentation\n",
      "algorithm is used to generate image objects with precise edges.\n",
      "Fig. 5. Flowchart of object-based high-resolution image classiﬁcation com-\n",
      "bining with deep features.\n",
      "Fig. 6. Illustration of the CNN-based deep learning framework.\n",
      "Then, with the shape constraint of the image objects, deep fea-\n",
      "tures are combined with objects’ features on the pixel level and\n",
      "used for image classiﬁcation. Finally, for each image object,\n",
      "the optimal statistical method was applied to determine the land\n",
      "cover class for mapping.\n",
      "Suppose an image Icontains Nimage objects Oi,i∈\n",
      "{1,2,...,N }, and, there are Mpixels Ij,j∈{ 1,2,...,M }\n",
      "inside object Oi. For each pixel Ij, the deep feature representa-\n",
      "tion is denoted as Fj. Similarly, the object-based features (such\n",
      "as, NDVI) are represented as Rj. For feature combination pur-\n",
      "pose, the deep features are stacked with object-based properties\n",
      "Uj=[Fj,Rj]for joint feature classiﬁcation of the original im-\n",
      "agery at the pixel level. For this method, the label of an object\n",
      "Oiis predicted from the majority statistics of feature vectors Uj\n",
      "using a two-layer neural network:\n",
      "pj=W 2tanh(W 1Uj+b1) (4)\n",
      "di,a=1\n",
      "t(Oi)/summationdisplay\n",
      "j∈[1,M ]count (pj==a). (5)\n",
      "Matrices W 1and W 2are the trainable parameters of the\n",
      "two-layer neural network classiﬁer. pjis the predicted label at\n",
      "location j, andt(Oi)is the pixel number of the object. The ﬁnal\n",
      "label for each image object Oiis given by\n",
      "li= arg max\n",
      "a∈classesdi,a. (6)\n",
      "The ﬂowchart of object-based deep learning classiﬁcation is\n",
      "depicted in Fig. 5.\n",
      "IV . D A TASETS\n",
      "The datasets analyzed include three very different cities with\n",
      "complex urban conditions: Beijing, Pavia, and V aihingen. The\n",
      "ﬁrst Beijing scene was acquired by Worldview-2 in 2010. The\n",
      "second scene was acquired by the ROSIS sensor during a ﬂight\n",
      "campaign over Pavia, northern Italy, and was distributed in 2008\n",
      "as part of a test of the airborne Intergraph/ZI Digital Mapping\n",
      "Authorized licensed use limited to: b-on: ISCTE. Downloaded on December 03,2023 at 19:35:37 UTC from IEEE Xplore.  Restrictions apply. 3390 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV A TIONS AND REMOTE SENSING, VOL. 10, NO. 7, JULY 2017\n",
      "Fig. 7. Study datasets and ground truth reference maps.\n",
      "Camera Instrument. The V aihingen dataset was provided by the\n",
      "German Association of Photogrammetry and Remote Sensing.\n",
      "A. Description of Scenes\n",
      "The Beijing scene, as shown in Fig. 7(a), contains 1036 ×\n",
      "1146 pixels, and it has eight multispectral bands with a spatial\n",
      "resolution of 0.5 m (after pan sharpening). It contains typical\n",
      "residential buildings and examples of commercial structures\n",
      "with similar heights (about ﬁve or six ﬂoors) but of differ-\n",
      "ent sizes. The high-resolution Beijing image was chosen for\n",
      "two reasons. First, it embodies the main difﬁculties of high-\n",
      "resolution classiﬁcation, especially for the detailed mapping\n",
      "of complex buildings. Second, the Beijing scene represents a\n",
      "common Chinese urban landscape, including ﬂat building and\n",
      "narrow streets, which are obviously different from western style\n",
      "of modern cities. The other two study areas, Pavia and V aihin-\n",
      "gen are shown in Fig. 7(c) and (e). The Pavia scene, contains\n",
      "1096 ×715 pixels and 102 hyperspectral bands, with a spatial\n",
      "resolution of 1.3 m. It is composed of an elaborate urban lattice\n",
      "with complex structures showing a great variety in dimension,\n",
      "shapes, and heights. The image size of V aihingen dataset is 892\n",
      "×1498 (reduced half resolution for efﬁcient computation), with\n",
      "very high spatial resolution of 0.09 m per pixel. For better dis-\n",
      "crimination, the band combination of V aihingen dataset was set\n",
      "to infrared (IR), red (R), and green (G). The urban scene of V ai-\n",
      "hingen contains elements that differ from the other two, sinceit is mainly composed of small residential houses (two or three\n",
      "ﬂoors), roads, some sparse trees, and vegetated areas.\n",
      "B. Classes, Training, and V alidation Set Deﬁnition\n",
      "To efﬁciently map different scenes, several land cover types\n",
      "of interest have been identiﬁed. For the Beijing case, the pri-\n",
      "mary goal was to discriminate between residential buildings and\n",
      "commercial buildings based on the differences in construction\n",
      "style and size. These two kinds of buildings appear with com-\n",
      "plex elements, such as a small chimney on the top of a building\n",
      "and sunlit wall with great spectral variation. Moreover, roofs of\n",
      "commercial buildings are similar to roads in terms of spectral\n",
      "reﬂectance and shapes, which further increased the difﬁculty of\n",
      "performing a successful image classiﬁcation. A further explo-\n",
      "ration was made to distinguish the different uses of the asphalt\n",
      "surfaces, which included parking lots and roads. Other classes\n",
      "such as shadow, impervious, trees, and bare soil were also added\n",
      "for a total of eight classes. Since vegetation occupies a large por-\n",
      "tion of urban scenes, three bands with the combining IR, R, and\n",
      "G were chosen to discriminate vegetation from structures.\n",
      "Similar to the previous dataset, the Pavia center also has\n",
      "crowded residential buildings. Generally, the buildings change\n",
      "rapidly in terms of sizes and spectral reﬂectance where the left\n",
      "part of Pavia dataset contains with old constructions, while the\n",
      "eastern part of the city new buildings are mainly made of con-\n",
      "crete. It is possible to discriminate between asphalt (roads) and\n",
      "bitumen (newly developed roofs). Other classes of interest were\n",
      "also identiﬁed, including self-blocking bricks, water, trees, shad-\n",
      "ows, meadows, and bare soil for a total of nine classes. In order\n",
      "to reduce the computational cost and preserve the substantial\n",
      "information across spectral bands, we reduced the dimensional-\n",
      "ity of Pavia dataset by using principle component (PC) analysis.\n",
      "Following the previous studies [28], the ﬁrst three PCs hold\n",
      "more than 95% of the original spectral information. Thus, sim-\n",
      "ilar to the IR, R, and G bands, we selected the ﬁrst three PCs to\n",
      "feed the CNN model.\n",
      "The V aihingen scene has features different from the previous\n",
      "two. It has a ﬁner spatial resolution that can resolve more com-\n",
      "plex and challenging urban patterns, such as road markings and\n",
      "even car windows. Buildings with large varieties in shapes and\n",
      "sizes are interspersed with roads. It is also possible to distinguish\n",
      "short vegetation and trees at this level of resolution. Other com-\n",
      "mon classes like water, roads, and buildings were also included\n",
      "for classiﬁcation.\n",
      "The reference map for each scene were reported in Fig. 7(b),\n",
      "(d), and (f), respectively. Ground reference labels have been\n",
      "acquired by careful visual interpretation. Both the V aihingen\n",
      "scene and its reference map were distributed by the International\n",
      "Society for Photogrammetry and Remote Sensing (ISPRS) and\n",
      "the two-dimensional semantic labeling contest [36].1Instead of\n",
      "using the selected pixels, we have extended them into image\n",
      "patches with the labeled pixels in the center before feeding\n",
      "them to the CNN with its deep features. In the experiments,\n",
      "1The V aihingen dataset was provided by the German Society for Photogram-\n",
      "metry, Remote Sensing and Geoinformation (DGPF) [36], http://www.ifp.uni-\n",
      "stuttgart.de/dgpf/DKEP-Allg.html.\n",
      "Authorized licensed use limited to: b-on: ISCTE. Downloaded on December 03,2023 at 19:35:37 UTC from IEEE Xplore.  Restrictions apply. ZHAO et al. : OBJECT-BASED CONVOLUTIONAL NEURAL NETWORK FOR HIGH-RESOLUTION IMAGERY CLASSIFICA TION 3391\n",
      "TABLE I\n",
      "BEIJING SCENE :CLASSES ,COLOR LEGEND ,TRAINING ,AND VALIDA TION\n",
      "SAMPLES\n",
      "TABLE II\n",
      "PAV I A SCENE :CLASSES ,COLOR LEGEND ,TRAINING ,AND VALIDA TION\n",
      "SAMPLES\n",
      "TABLE III\n",
      "VAIHINGEN SCENE :CLASSES ,COLOR LEGEND ,TRAINING ,AND VALIDA TION\n",
      "SAMPLES\n",
      "we have subsampled the original image in order to reduce the\n",
      "computation complexity.\n",
      "To demonstrate the robustness of the proposed method, for\n",
      "each dataset, we randomly select 10% nonoverlapping sam-\n",
      "ples (NOS) for training and another 5% for the test. In or-\n",
      "der to avoid the overlap phenomenon between samples, the\n",
      "overlap threshold is set to 80% (rejected if the overlap area is\n",
      "more than 80% between two samples). Detailed information\n",
      "about the training samples (TR), test samples (TE), and to-\n",
      "tal NOS are reported in Tables I, II, and III for the Beijing,\n",
      "Pavia, and V aihingen cases, respectively. The overall accuracy\n",
      "and Kappa coefﬁcient were used to evaluate the classiﬁcation\n",
      "performance.V. E XPERIMENTS AND ANALYSIS\n",
      "In this section, we evaluate the capability of the object-based\n",
      "deep learning method to classify the complex urban scenes de-\n",
      "scribed previously. As mentioned above, we combine the pow-\n",
      "erful representations of deep features with shape-preserving\n",
      "object-based classiﬁcation method for accurate high-resolution\n",
      "image interpretation. More speciﬁcally, the CNN-based deep\n",
      "feature learning algorithm has been adopted to describe com-\n",
      "plex urban patterns. The depth of the CNN can directly affect\n",
      "the abstraction level of our deep features. A detailed analysis\n",
      "of this depth effect has been carried out by assigning differ-\n",
      "ent layer conﬁgurations on the CNN framework, as described\n",
      "in Section V-A. Then, the effect of segmentation scales and\n",
      "classiﬁcation accuracies is illustrated in Section V-B. In addi-\n",
      "tion, deep features are combined with image objects for efﬁ-\n",
      "cient urban mapping, the classiﬁcation results are reported in\n",
      "Section V-C.\n",
      "A. Depth Effect on Deep Feature Learning\n",
      "The CNN-based deep learning framework has been used to\n",
      "extract complex urban patterns in this work. Instead of hand-\n",
      "crafted features, the CNN network automatically learn effective\n",
      "feature representations from the hierarchical activation struc-\n",
      "ture. Two parameters, the input size of the training samples and\n",
      "the depth of the CNN play important roles in terms of classiﬁ-\n",
      "cation accuracies.\n",
      "For the input training samples (also known as the receptive\n",
      "ﬁeld), it should have an appropriate spatial coverage for the\n",
      "objects of interest. To achieve this goal, we set the window\n",
      "size of training sample extraction to 18 ×18 pixels (about\n",
      "9, 23.4, and 3.24 m, for Beijing, Pavia, and V aihingen scene,\n",
      "respectively). The majority of urban geographical objects have\n",
      "the dominant scales between about 3.0 and 24.0 m [37].\n",
      "The depth conﬁguration of the CNN framework controls the\n",
      "robustness of deep features in terms of their abstraction level.\n",
      "The representation power increases as the CNN involves more\n",
      "layers. However, due to the constraint of input training sam-\n",
      "ple sizes, the deepest conﬁguration of the CNN should be no\n",
      "more than ﬁve layers with the 3 ×3 kernels in our network.\n",
      "Following to the previous studies [3], [28], we set the number\n",
      "of features to 20 for each convolutional layer (except the last\n",
      "layer). Fig. 6 presents the detailed conﬁguration of our CNN-\n",
      "based deep learning framework. The learning rate was set to\n",
      "0.001.\n",
      "In order to illustrate the importance of CNN depth when\n",
      "classifying high-resolution images, we separately trained CNN\n",
      "models with different depth that varies from 1 to 5. That is, one-\n",
      "layer CNN only has a single convolution layer, teo-layer CNN is\n",
      "combined with two convolution layers, three-layer CNN has the\n",
      "combination of two convolution layers and one pooling layer,\n",
      "four-layer CNN constitutes by three convolution layers and one\n",
      "pooling layer, and ﬁve-layer CNN is with three convolution\n",
      "layers and two pooling layers. The classiﬁcation results using\n",
      "different CNN models are reported in Tables IV–VI. The overall\n",
      "classiﬁcation accuracy is calculated to measure the classiﬁcation\n",
      "performance of the CNN networks.\n",
      "Authorized licensed use limited to: b-on: ISCTE. Downloaded on December 03,2023 at 19:35:37 UTC from IEEE Xplore.  Restrictions apply. 3392 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV A TIONS AND REMOTE SENSING, VOL. 10, NO. 7, JULY 2017\n",
      "TABLE IV\n",
      "CLASSIFICA TION ACCURACIES (INPERCENTAGE )OFWORLDVIEW DATA S E T WITHDIFFERENT CONFIGURA TION CNN S\n",
      "CNNs CB RB RD VG PL SD IM BS OA\n",
      "One-layer 60.64 95.27 35.40 81.19 13.68 4.43 0 0 68.45\n",
      "Two-layers 97.38 91.02 76.46 99.68 64.90 17.65 76.18 46.26 89.30\n",
      "Three-layers 99.09 97.48 87.65 100 76.86 97.44 78.97 78.91 95.89\n",
      "Four-layers 99.04 98.79 88.73 99.98 77.94 97.58 66.58 59.31 96.06\n",
      "Five-layers 99.40 98.10 88.80 99.98 79.47 96.08 78.32 69.53 96.29\n",
      "CB: Commercial building, RB: Residential building, RD: Road, VG: V egetation, PL: Parking lot, SD:\n",
      "Shadow, IM: Impervious, BS: Bare soil.\n",
      "TABLE V\n",
      "CLASSIFICA TION ACCURACIES (INPERCENTAGE )OFPAV I A DATA S E T WITHDIFFERENT CONFIGURA TION CNN S\n",
      "CNNs W A TR ME BB BS AS BI TI SD OA\n",
      "One-layer 100 98.66 85.67 44.77 72.61 93.83 87.39 97.59 99.22 95.64\n",
      "Two-layers 99.96 95.60 92.28 53.45 80.17 95.16 84.96 96.82 93.58 95.72\n",
      "Three-layers 99.98 96.86 93.88 82.72 89.92 95.64 89.06 99.14 95.77 97.77\n",
      "Four-layers 99.96 96.74 92.32 85.81 90.18 92.29 84.71 98.31 94.46 97.10\n",
      "Five-layers 99.95 97.50 90.18 90.69 92.90 93.91 89.41 98.42 94.74 97.69\n",
      "W A: Water, TR: Tree, ME: Meadow, BB: Bricks, BS: Bare soil, AS: Asphalt, BI: Bitumen, TI: Tiles, SD: Shadow.\n",
      "TABLE VI\n",
      "CLASSIFICA TION ACCURACIES (INPERCENTAGE )OFPAV I A DATA S E T WITH\n",
      "DIFFERENT CONFIGURA TION CNN S\n",
      "CNNs BD W A TR GR CA RD OA\n",
      "One-layer 67.29 0.01 88.29 43.66 1.49 69.23 63.27\n",
      "Two-layers 75.50 90.40 90.39 42.36 3.64 75.98 75.64\n",
      "Three-layers 86.52 94.84 91.92 66.75 38.46 78.47 83.14\n",
      "Four-layers 82.68 75.15 90.13 54.10 26.48 89.53 82.13\n",
      "Five-layers 86.60 95.99 94.29 62.58 40.49 90.38 87.14\n",
      "BD: Buildings, W A: Water, TR: Trees, GR: Grass, CA: Cars, RD: Road.\n",
      "Fig. 8. In (A), (B), and (C) is shown the classiﬁcation results by using ﬁve-\n",
      "layer CNN network over Worldview-2, Pavia, and V aihingen complex urban\n",
      "scene, respectively.\n",
      "The depth parameter of the CNN framework signiﬁcantly\n",
      "impacts the classiﬁcation performance in terms of accuracy. As\n",
      "stated above, classiﬁcation accuracies become greater if feature\n",
      "representations get deeper and deeper. Therefore, the deepest\n",
      "CNN conﬁguration will obtain the best classiﬁcation results. The\n",
      "classiﬁcation results using a ﬁve-layer conﬁguration of CNN are\n",
      "presented in Fig. 8. To quantitatively evaluate the accuracy of\n",
      "the classiﬁcation results for these three datasets, we reported\n",
      "Fig. 9. Classiﬁcation results with 10, 20, 30 segmentation scales for\n",
      "Worldview-2 (a)–(c), Pavia center (d)–(f), and V aihigen (g)–(i).\n",
      "the classiﬁcation accuracies and their variations over different\n",
      "depth, as shown in Tables IV–VI. In general, these tables indicate\n",
      "that the overall classiﬁcation accuracy increase as the CNN gets\n",
      "deeper. However, for the cars in V aihingen dataset and bare soil\n",
      "in the scene of Worldview-2, there is a signiﬁcant drop with\n",
      "the deeper conﬁguration in terms of classiﬁcation accuracies.\n",
      "The reason for this phenomenon is probably that the bare soil\n",
      "does not have a ﬁxed spatial pattern but mainly depends on\n",
      "spectral reﬂectance for discrimination. The other classes with\n",
      "ﬁxed spatial patterns become well classiﬁed as the network gets\n",
      "Authorized licensed use limited to: b-on: ISCTE. Downloaded on December 03,2023 at 19:35:37 UTC from IEEE Xplore.  Restrictions apply. ZHAO et al. : OBJECT-BASED CONVOLUTIONAL NEURAL NETWORK FOR HIGH-RESOLUTION IMAGERY CLASSIFICA TION 3393\n",
      "deeper. We also noticed that the classiﬁcation results of the\n",
      "V aihingen dataset suffer from the “pepper & salt” affect, which\n",
      "also reduced the classiﬁcation accuracy. Therefore, to further\n",
      "increase the classiﬁcation accuracy, it is advisable to combine\n",
      "image segments with deeply abstracted features by introducing\n",
      "accurate boundary information.\n",
      "B. Segmentation Scale Effects\n",
      "Although deep features are efﬁcient in terms of complex pat-\n",
      "tern description, they fail to predict image objects edges due\n",
      "to the highly abstracted feature representations. To improve the\n",
      "classiﬁcation accuracy through deep learning, an object-based\n",
      "complementary strategy is necessary. The object-based method\n",
      "overcomes local spectral variation and provides accurate edge\n",
      "realizations with the image segments. Furthermore, the image\n",
      "objects have access to geographic entities, thus object-based im-\n",
      "age features with more semantic meanings could be utilized for\n",
      "better discrimination of complex urban scenes.\n",
      "The multiresolution segmentation algorithm [38] is applied to\n",
      "generate image objects through experiments. Three key param-\n",
      "eters need to be determined before the segmentation algorithm\n",
      "is deployed, namely they are scale ( Ssc), shape ( Ssh), and com-\n",
      "pactness ( Scm). The scale parameter directly controls the size of\n",
      "segmented objects by matching the required level of detail. In\n",
      "the experiments, we set the shape parameter Sshand compact-\n",
      "ness parameter Scmto 0.1 and 0.5, respectively. To evaluate the\n",
      "segmentation scale effects on classiﬁcation results, we visually\n",
      "checked segmented objects by considering scale parameter Ssc\n",
      "varieties from 10 to 30. As reported in [39], the lowest scale\n",
      "level presented the best classiﬁcation performance. Therefore,\n",
      "to provide accurate edges for deep learning-based classiﬁcation,\n",
      "we set oversegmentation scales 10, 20, and 30 for each image\n",
      "dataset. For each object, the mean brightness of each band and\n",
      "the NDVI are selected as object-based features.\n",
      "With three different segmentation scales, the classiﬁcation\n",
      "results of three datasets are reported in Fig. 9. To quantitatively\n",
      "evaluate the performance of the object-based classiﬁcation re-\n",
      "sults, we present the detailed information about segmented im-\n",
      "age objects and the variation of classiﬁcation accuracies at dif-\n",
      "ferent segmentation scales. The number of segmented objects\n",
      "for each dataset is presented in Fig. 10(a). Classiﬁcation results\n",
      "of each dataset are reported in Fig. 10(b)–(d). These results\n",
      "indicate that the classiﬁcation accuracy signiﬁcantly drops as\n",
      "the segmentation scale gets larger. Therefore, we can conclude\n",
      "that oversegmentation is more suitable for deep feature-based\n",
      "image interpretation, especially for high-resolution complex ur-\n",
      "ban scenes.\n",
      "C. Method Comparison\n",
      "To further illustrate the effectiveness of the object-based\n",
      "CNN, we compared this proposed classiﬁcation method with\n",
      "several traditional classiﬁcation methods by classifying all\n",
      "three datasets. More speciﬁcally, we compared the object-CNN\n",
      "(OCNN) with SVM (spectral features), extended morphologi-\n",
      "cal attribute proﬁles method (EMAP), spectral+EMAP , pixel-\n",
      "based CNN (PCNN), spectral and spatial feature classiﬁcation\n",
      "(SSFC)[3], and the multiscale CNN (MCNN)[28]. During thecomparison, the EMAP features are built using the area (related\n",
      "to the size of the regions) and standard deviation (which mea-\n",
      "sures the homogeneity of the pixels enclosed by the regions)\n",
      "attributes. The threshold values of area are chosen in the range\n",
      "of{50,500 }and standard deviation ranging from 2.5% to 20%.\n",
      "The classiﬁcation maps are shown in Fig. 11, and the classiﬁca-\n",
      "tion accuracies are reported in Tables VII–IX. For the traditional\n",
      "methods, we investigated the pure spectral information, EMAP\n",
      "features and spectral-EMAP features for image classiﬁcation.\n",
      "The default classiﬁer for traditional methods is linear SVM,\n",
      "the parameter of SVM classiﬁer selected by ﬁve-cross valida-\n",
      "tion. However, due to the low-level feature similarities between\n",
      "different classes (such as roads and roofs), the predicted maps\n",
      "with traditional methods suffer a lot from misclassiﬁcation and\n",
      "noises. Different from using the traditional feature descriptors,\n",
      "the PCNN explored high-level feature representations with hier-\n",
      "archical framework. Therefore, the PCNN-based classiﬁcation\n",
      "results are more robust and accurate. However, it fails to cap-\n",
      "ture the spectral information over different image bands. The\n",
      "SSFC method was proposed to integrate the rich spectral infor-\n",
      "mation and high-level spatial features. Thus, the SSFC method\n",
      "achieved better classiﬁcation results, in terms of classiﬁcation\n",
      "accuracies. The geographical objects in remote sensing images\n",
      "are commonly displayed in different scales (e.g., roofs with\n",
      "different sizes). But, both PCNN and SSFC are not able to\n",
      "capture the useful information over scales. The MCNN was\n",
      "designed to model image objects over different scales. As a\n",
      "result, the classiﬁcation results of MCNN are better than the\n",
      "previous two models. The PCNN, SSFC, and MCNN explored\n",
      "deep features from image patches, which have overlooked the\n",
      "boundary information of image objects. To overcome this prob-\n",
      "lem, the object-based CNN is presented. It combined the robust\n",
      "deep features and accurate boundaries from image segments\n",
      "for better classiﬁcation of high-resolution images. In this work,\n",
      "we chose a ﬁve-layer CNN for deep feature extraction and the\n",
      "scale parameter for image objects generation was set to 10. The\n",
      "classiﬁcation results of OCNN demonstrated that the strategy of\n",
      "combining deep features and image objects is effective, in terms\n",
      "of classiﬁcation accuracies.\n",
      "As shown in Fig. 11, the classiﬁcation maps directly classiﬁed\n",
      "by using spectral information or EMAP features suffer greatly\n",
      "from the “pepper and salt” effect. The CNN-based methods\n",
      "produce smoothing and accurate results, especially for the class\n",
      "building. Although the previous CNN methods are effective\n",
      "for complex spatial feature extraction, it shows less strength in\n",
      "capturing boundary information. For the complex classes (such\n",
      "as buildings and roads), the classiﬁcation results of using PCNN,\n",
      "SSFC, and MCNN are less promising. However, by integrating\n",
      "image objects and deep features, the classiﬁcation accuracies\n",
      "for all the datasets are increased sharply.\n",
      "VI. D ISCUSSION\n",
      "In this study, we propose an object-based CNN for the clas-\n",
      "siﬁcation of high-resolution images. Feature extraction is one\n",
      "of the biggest challenges for the analysis of remote sensing\n",
      "images. Traditional image classiﬁcation methods require nu-\n",
      "merous image features to be empirically designed and linked to\n",
      "Authorized licensed use limited to: b-on: ISCTE. Downloaded on December 03,2023 at 19:35:37 UTC from IEEE Xplore.  Restrictions apply. 3394 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV A TIONS AND REMOTE SENSING, VOL. 10, NO. 7, JULY 2017\n",
      "Fig. 10. Quantitative evaluation of segmentation scale on classiﬁcation accuracies. (a) Number of objects. (b) Worldview-2 classiﬁcation results . (c) Pavia center\n",
      "classiﬁcation results. (d) V aihingen classiﬁcation results.\n",
      "Fig. 11. Classiﬁcation results for the Worldview-2, Pavia center, and V aihingen datasets, respectively. (a) SVM classiﬁcation, (b) EMAP-based cla ssiﬁcation,\n",
      "(c) spectral and the EMAP combined classiﬁcation, (d) pixel-based CNN classiﬁcation, (e) SSFC classiﬁcation, (f) MCNN classiﬁcation, and (g) objec t-based\n",
      "classiﬁcation.\n",
      "Authorized licensed use limited to: b-on: ISCTE. Downloaded on December 03,2023 at 19:35:37 UTC from IEEE Xplore.  Restrictions apply. ZHAO et al. : OBJECT-BASED CONVOLUTIONAL NEURAL NETWORK FOR HIGH-RESOLUTION IMAGERY CLASSIFICA TION 3395\n",
      "TABLE VII\n",
      "CLASSIFICA TION ACCURACIES OF THE WORLDVIEW -2 D ATA S E T\n",
      "SVM EMAP SEMAP PCNN SSFC MCNN OCNN\n",
      "CB 91.74 86.70 91.84 98.15 98.36 99.56 99.34\n",
      "RB 67.68 91.23 94.17 95.66 96.37 97.92 98.71\n",
      "RD 29.61 7.12 47.43 86.52 90.73 93.69 94.56\n",
      "VG 33.28 83.81 94.45 99.36 98.13 99.97 99.67\n",
      "PL 7.37 8.37 14.75 80.95 91.67 84.66 88.67\n",
      "SD 62.68 45.57 4.83 76.83 89.20 98.81 99.03\n",
      "IM 19.91 28.43 32.80 62.28 74.14 80.27 86.62\n",
      "BS 5.27 18.48 23.85 46.03 45.52 74.90 66.58\n",
      "AA 39.69 46.21 50.51 80.72 85.52 91.22 91.65\n",
      "OA 66.47 74.97 82.16 93.78 95.29 97.11 97.45\n",
      "KP 48.72 61.52 73.16 91.03 93.26 95.86 96.49\n",
      "Both spectral and EMAP feature representations were classiﬁed by the SVM\n",
      "classiﬁer. OCNN refers to the object-based CNN method.\n",
      "TABLE VIII\n",
      "CLASSIFICA TION ACCURACIES OF THE PAV I A CENTER DATA S E T\n",
      "SVM EMAP SEMAP PCNN SSFC MCNN OCNN\n",
      "W A 99.79 100 99.98 100 100 100 100\n",
      "TR 92.95 98.99 93.18 97.50 95.56 99.10 99.53\n",
      "ME 79.06 24.40 82.78 97.09 99.58 98.70 99.81\n",
      "BB 53.59 88.34 70.24 98.51 99.18 99.78 99.93\n",
      "BS 41.18 85.27 65.42 97.94 99.94 99.98 99.97\n",
      "AS 91.71 95.49 96.40 99.72 99.94 99.98 100\n",
      "BI 81.47 90.30 81.62 96.25 99.20 99.89 99.82\n",
      "TI 97.74 99.47 98.76 99.53 99.66 99.93 99.98\n",
      "SD 69.58 92.21 96.23 99.37 99.93 100 100\n",
      "AA 78.56 86.05 87.18 98.32 99.13 99.67 99.88\n",
      "OA 92.98 96.44 95.65 99.34 99.61 99.90 99.95\n",
      "KP 89.94 94.94 93.80 99.07 99.44 99.86 99.94\n",
      "Both spectral and EMAP feature representations were classiﬁed by the SVM clas-\n",
      "siﬁer. OCNN refers to the object-based CNN method.\n",
      "TABLE IX\n",
      "CLASSIFICA TION ACCURACIES OF THE VAIHINGEN DATA S E T\n",
      "SVM EMAP SEMAP PCNN SSFC MCNN OCNN\n",
      "BD 58.41 78.36 94.50 96.00 96.37 96.20 96.40\n",
      "W A 43.61 68.64 79.80 98.15 98.85 94.69 97.73\n",
      "TR 90.01 87.83 86.86 93.30 95.48 94.68 95.70\n",
      "GR 11.99 35.51 12.94 83.25 86.55 88.02 81.87\n",
      "CA 2.84 11.81 13.27 62.06 70.81 68.78 66.05\n",
      "RD 85.70 80.30 74.37 92.05 91.16 93.23 94.27\n",
      "AA 48.33 58.64 52.82 87.54 89.87 89.27 88.67\n",
      "OA 66.60 74.64 75.51 92.77 92.41 92.60 93.84\n",
      "KP 54.60 65.95 62.81 90.47 91.35 91.57 91.87\n",
      "Both spectral and EMAP feature representations were classiﬁed by the SVM clas-\n",
      "siﬁer. OCNN refers to the object-based CNN method.\n",
      "the characteristics of different images, which is time-consuming\n",
      "and often fails to achieve accurate interpretation. Unlike these\n",
      "traditional methods, in this paper, the CNN as the character-\n",
      "istic deep learning method was chosen for automatic feature\n",
      "learning. With the hierarchical structure of the CNN, image fea-\n",
      "tures at higher levels can be automatically extracted and the\n",
      "method has shown robustness and good accuracy in the pres-\n",
      "ence of complex targets. As the abstraction level increased,\n",
      "the extracted deep features demonstrated strong invariance in\n",
      "terms of semantic content. However, the method often fails tocapture boundary information of the target and suffers from the\n",
      "well-known “pepper & salt” effect.\n",
      "To further improve the quality of our CNN-based classiﬁca-\n",
      "tion results, we suggested to combine the CNN-based features\n",
      "with the low-level image segments for better descriptions of\n",
      "targets’ boundary and to reduce the “pepper & salt” effect. As\n",
      "demonstrated in our experiments, the combination of image\n",
      "objects and deep features is quite effective. For one thing, it\n",
      "alleviates people from the time-consuming process of feature\n",
      "selection. For another, the combination of object-based image\n",
      "interpretation and deep learning method provides accurate tar-\n",
      "gets’ boundary information as well as semantic labels.\n",
      "VII. C ONCLUSION\n",
      "In this paper, we propose an effective way to classify high-\n",
      "resolution images by combining deep features and image ob-\n",
      "jects. Compared to the traditional classiﬁcation methods, the\n",
      "proposed procedure utilizes deep CNN framework to automat-\n",
      "ically extract robust and discriminative features for complex\n",
      "urban objects classiﬁcation (such as building roofs and cars).\n",
      "To evaluate the effectiveness of deep features, we tested the\n",
      "CNN framework with ﬁve different layer conﬁgurations for the\n",
      "classiﬁcation of high-resolution imagery. However, as the CNN\n",
      "framework gets deeper, the generated features become more\n",
      "and more robust but often too abstract (overlooked the shape\n",
      "of the target objects) to describe boundary information. Com-\n",
      "plementary, the object-based classiﬁcation method can preserve\n",
      "edge information in complex urban scenes which can be inte-\n",
      "grated with the highly abstracted deep features. As a solution, an\n",
      "object-based classiﬁcation method is combined with deep fea-\n",
      "tures to promote the interpretation accuracy of high-resolution\n",
      "images. Experimental results indicate that the combination of\n",
      "deep learning and object-based classiﬁcation method is effective\n",
      "for mapping complex urban image datasets.\n",
      "However, the proposed method has no access to the contextual\n",
      "information at the global level. The relationships between image\n",
      "objects are also important to promote the classiﬁcation results.\n",
      "Therefore, we focus on modeling the contextual information\n",
      "with deep feature-based image objects and for the improved\n",
      "image classiﬁcation.\n",
      "REFERENCES\n",
      "[1] D. Tuia, F. Ratle, F. Paciﬁci, M. F. Kanevski, and W. J. Emery, “Active\n",
      "learning methods for remote sensing image classiﬁcation,” IEEE Trans.\n",
      "Geoscience Remote Sensing , vol. 47, no. 7, pp. 2218–2232, 2009.\n",
      "[2] L. Bruzzone and L. Carlin, “A multilevel context-based system for clas-\n",
      "siﬁcation of very high spatial resolution images,” IEEE Trans. Geosci.\n",
      "Remote Sens. , vol. 44, no. 9, pp. 2587–2600, Sep. 2006.\n",
      "[3] W. Zhao and S. Du, “Spectral-spatial feature extraction for hyperspectral\n",
      "image classiﬁcation: A dimension reduction and deep learning approach,”\n",
      "IEEE Trans. Geosci. Remote Sens. , vol. 54, no. 8, pp. 4544–4554, Aug.\n",
      "2016.\n",
      "[4] F. Zhang, B. Du, L. Zhang, and M. Xu, “Weakly supervised learning\n",
      "based on coupled convolutional neural networks for aircraft detection,”\n",
      "IEEE Trans. Geosci. Remote Sens. , vol. 54, no. 9, pp. 5553–5563, Sep.\n",
      "2016.\n",
      "[5] Y . Bengio, “Learning deep architectures for ai,” F ound. Trends Mach.\n",
      "Learn. , vol. 2, no. 1, pp. 1–127, 2009.\n",
      "[6] U. C. Benz, P . Hofmann, G. Willhauck, I. Lingenfelder, and M. Heynen,\n",
      "“Multi-resolution, object-oriented fuzzy analysis of remote sensing data\n",
      "for gis-ready information,” ISPRS J. Photogrammetry Remote Sens. ,\n",
      "vol. 58, no. 3, pp. 239–258, 2004.\n",
      "Authorized licensed use limited to: b-on: ISCTE. Downloaded on December 03,2023 at 19:35:37 UTC from IEEE Xplore.  Restrictions apply. 3396 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV A TIONS AND REMOTE SENSING, VOL. 10, NO. 7, JULY 2017\n",
      "[7] F. Zhang, B. Du, and L. Zhang, “Saliency-guided unsupervised feature\n",
      "learning for scene classiﬁcation,” IEEE Trans. Geosci. Remote Sens. ,\n",
      "vol. 53, no. 4, pp. 2175–2184, Apr. 2015.\n",
      "[8] W. Zhao and S. Du, “Scene classiﬁcation using multi-scale deeply\n",
      "described visual words,” Int. J. Remote Sens. , vol. 37, no. 17,\n",
      "pp. 4119–4131, 2016. [Online]. Available: http://dx.doi.org/10.1080/\n",
      "01431161.2016.1207266\n",
      "[9] A. Puissant , J. Hirsch, and C. Weber, “The utility of texture analysis\n",
      "to improve perpixel classiﬁcation for high to very high spatial resolution\n",
      "imagery,” Int. J. Remote Sens. , vol. 26, no. 4, pp. 733–745, 2005. [Online].\n",
      "Available: http://dx.doi.org/10.1080/01431160512331316838\n",
      "[10] W. Zhao, L. Luo, Z. Guo, J. Y ue, X. Y u, H. Liu, and J. Wei, “Road\n",
      "extraction in remote sensing images based on spectral and edge analysis,”\n",
      "Spectrosc. Spectral Anal. , vol. 35, no. 10, pp. 2814–2819, 2015.\n",
      "[11] J. A. Benediktsson, J. A. Palmason, and J. R. Sveinsson, “Classiﬁcation\n",
      "of hyperspectral data from urban areas based on extended morphological\n",
      "proﬁles,” IEEE Trans. Geosci. Remote Sens. , vol. 43, no. 3, pp. 480–491,\n",
      "Mar. 2005.\n",
      "[12] T. C. Bau, S. Sarkar, and G. Healey, “Hyperspectral region classiﬁcation\n",
      "using a three-dimensional gabor ﬁlterbank,” IEEE Trans. Geosci. Remote\n",
      "Sens. , vol. 48, no. 9, pp. 3457–3464, Sep. 2010.\n",
      "[13] X. Huang, L. Zhang, and P . Li, “A multiscale feature fusion approach for\n",
      "classiﬁcation of very high resolution satellite imagery based on wavelet\n",
      "transform,” Int. J. Remote Sens. , vol. 29, no. 20, pp. 5923–5941, 2008.\n",
      "[Online]. Available: http://dx.doi.org/10.1080/01431160802139922\n",
      "[14] A. M. Cheriyadat, “Unsupervised feature learning for aerial scene classi-\n",
      "ﬁcation,” IEEE Trans. Geosci. Remote Sens. , vol. 52, no. 1, pp. 439–451,\n",
      "Jan. 2014.\n",
      "[15] D. Tuia, M. V olpi, M. D. Mura, A. Rakotomamonjy, and R. Flamary,\n",
      "“Automatic feature learning for spatio-spectral image classiﬁcation with\n",
      "sparse svm,” IEEE Trans. Geosci.Remote Sens. , vol. 52, no. 10, pp. 6062–\n",
      "6074, Oct. 2014.\n",
      "[16] J. Li et al. , “Multiple feature learning for hyperspectral image classiﬁca-\n",
      "tion,” IEEE Trans. Geosci. Remote Sens. , vol. 53, no. 3, pp. 1592–1606,\n",
      "Mar. 2015.\n",
      "[17] D. Tuia, R. Flamary, and N. Courty, “Multiclass feature learning for\n",
      "hyperspectral image classiﬁcation: Sparse and hierarchical solutions,”\n",
      "J. Photogrammetry Remote Sens. , vol. 105, pp. 272–285, 2015.\n",
      "[18] G. E. Hinton, S. Osindero, and Y .-W. Teh, “A fast learning algorithm\n",
      "for deep belief nets,” Neural Comput. , vol. 18, no. 7, pp. 1527–1554,\n",
      "2006.\n",
      "[19] Y . LeCun, Y . Bengio, and G. Hinton, “Deep learning,” Nature , vol. 521,\n",
      "no. 7553, pp. 436–444, 2015.\n",
      "[20] Q. V . Le, “Building high-level features using large scale unsupervised\n",
      "learning,” in Proc. IEEE Int. Conf. Acoust. Speech Signal Process. , 2013,\n",
      "pp. 8595–8598.\n",
      "[21] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\n",
      "with deep convolutional neural networks,” in Proc. Adv. Neural Inf. Pro-\n",
      "cess. Syst. Conf. , 2012, pp. 1097–1105.\n",
      "[22] Y . Sun, Y . Chen, X. Wang, and X. Tang, “Deep learning face representation\n",
      "by joint identiﬁcation-veriﬁcation,” in Proc. Adv. Neural Inf. Process. Syst.\n",
      "Conf. , 2014, pp. 1988–1996.\n",
      "[23] L.-J. Li, H. Su, L. Fei-Fei, and E. P . Xing, “Object bank: A high-level image\n",
      "representation for scene classiﬁcation & semantic feature sparsiﬁcation,”\n",
      "inProc. Adv. Neural Inf. Process. Syst. Conf. , 2010, pp. 1378–1386.\n",
      "[24] Y . Chen, Z. Lin, X. Zhao, G. Wang, and Y . Gu, “Deep learning-based\n",
      "classiﬁcation of hyperspectral data,” IEEE J. Sel. Top. Appl. Earth Observ.\n",
      "Remote Sens. , vol. 7, no. 6, pp. 2094–2107, Jun. 2014.\n",
      "[25] W. Zhao, Z. Guo, J. Y ue, X. Zhang, and L. Luo, “On combining multiscale\n",
      "deep learning features for the classiﬁcation of hyperspectral remote sens-\n",
      "ing imagery,” Int. J. Remote Sens. , vol. 36, no. 13, pp. 3368–3379, 2015.\n",
      "[Online]. Available: http://dx.doi.org/10.1080/2150704X.2015.1062157\n",
      "[26] X. Chen, S. Xiang, C.-L. Liu, and C.-H. Pan, “V ehicle detection in satellite\n",
      "images by hybrid deep convolutional neural networks,” IEEE Geosci.\n",
      "Remote Sens. Lett. , vol. 11, no. 10, pp. 1797–1801, Oct. 2014.\n",
      "[27] J. Y ue, W. Zhao, S. Mao, and H. Liu, “Spectral–spatial classiﬁcation of\n",
      "hyperspectral images using deep convolutional neural networks,” Remote\n",
      "Sens. Lett. , vol. 6, no. 6, pp. 468–477, 2015.\n",
      "[28] W. Zhao and S. Du, “Learning multiscale and deep representations for\n",
      "classifying remotely sensed imagery,” ISPRS J. Photogrammetry Remote\n",
      "Sens. , vol. 113, pp. 155–165, 2016.\n",
      "[29] T. Blaschke, S. Lang, E. Lorup, J. Strobl, and P . Zeil, “Object-oriented\n",
      "image processing in an integrated gis/remote sensing environment and per-\n",
      "spectives for environmental applications,” Environ. Inf. Planning Politics\n",
      "Public , vol. 2, pp. 555–570, 2000.[30] G. Duveiller, P . Defourny, B. Descl ´ee, and P . Mayaux, “Deforestation in\n",
      "central africa: Estimates at regional, national and landscape levels by ad-\n",
      "vanced processing of systematically-distributed landsat extracts,” Remote\n",
      "Sens. Environ. , vol. 112, no. 5, pp. 1969–1981, 2008.\n",
      "[31] Y . Jia et al. , “Caffe: Convolutional architecture for fast feature embedding,”\n",
      "inProc. ACM Int. Conf. Multimedia , 2014, pp. 675–678.\n",
      "[32] F. Zhang, B. Du, and L. Zhang, “Scene classiﬁcation via a gradient boost-\n",
      "ing random convolutional network framework,” IEEE Trans. Geosci. Re-\n",
      "mote Sens. , vol. 54, no. 3, pp. 1793–1802, Mar. 2016.\n",
      "[33] M. A. Ranzato, F. J. Huang, Y .-L. Boureau, and Y . LeCun, “Unsupervised\n",
      "learning of invariant feature hierarchies with applications to object recog-\n",
      "nition,” in Proc. IEEE Conf. Comput. Vis. Pattern Recog. , 2007, pp. 1–8.\n",
      "[34] C. Farabet, C. Couprie, L. Najman, and Y . LeCun, “Learning hierarchical\n",
      "features for scene labeling,” IEEE Trans. Pattern Anal. Mach. Intell. ,\n",
      "vol. 35, no. 8, pp. 1915–1929, Aug. 2013.\n",
      "[35] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hierarchies\n",
      "for accurate object detection and semantic segmentation,” in Proc. IEEE\n",
      "Conf. Comput. Vis. Pattern Recognit. , 2014, pp. 580–587.\n",
      "[36] M. Cramer, “The dgpf-test on digital airborne camera evaluation–overview\n",
      "and test design,” Photogrammetrie Fernerkundung Geoinf. , vol. 2010,\n",
      "no. 2, pp. 73–82, 2010.\n",
      "[37] C. Small, “High spatial resolution spectral mixture analysis of urban re-\n",
      "ﬂectance,” Remote Sens. Environ. , vol. 88, nos. 1/2, pp. 170–186, 2003.\n",
      "[38] A. Darwish, K. Leukert, and W. Reinhardt, “Image segmentation for the\n",
      "purpose of object-based classiﬁcation,” in Proc. IEEE Int. Geosci. Remote\n",
      "Sens. Symp. , 2003, vol. 3, pp. 2039–2041.\n",
      "[39] S. W. Myint, P . Gober, A. Brazel, S. Grossman-Clarke, and Q. Weng,\n",
      "“Per-pixel vs. object-based classiﬁcation of urban land cover extraction\n",
      "using high spatial resolution imagery,” Remote Sens. Environ. , vol. 115,\n",
      "no. 5, pp. 1145–1161, 2011.\n",
      "Wenzhi Zhao was born in Shandong, China, in 1990.\n",
      "He is currently working toward the Ph.D. degree in\n",
      "the Institution of Remote Sensing and Geographic In-\n",
      "formation System, Peking University, Beijing, China.\n",
      "His research interests include hyperspectral data\n",
      "analysis, high-resolution image processing, deep\n",
      "learning techniques, and computational intelligence\n",
      "in remote sensing images.\n",
      "Shihong Du received the B.S. and M.S. degrees in\n",
      "cartography and geographic information system from\n",
      "the Wuhan University, Hubei, China, and the Ph.D.\n",
      "degree in cartography and geographic information\n",
      "system from the Institute of Remote Sensing Ap-\n",
      "plications, Chinese Academy of Sciences, Beijing,\n",
      "China, in 1998, 2001, and 2004, respectively.\n",
      "He is currently an Associate Professor with the\n",
      "Peking University, Beijing. His research interests in-\n",
      "clude qualitative knowledge representation, reason-\n",
      "ing and its applications, and semantic understanding\n",
      "of spatial data including GIS and remote sensing data.\n",
      "William J. Emery (F’02) received the Ph.D. de-\n",
      "gree in physical oceanography from the University\n",
      "of Hawaii, Honolulu, HI, USA, in 1975.\n",
      "After working at Texas A&M University, he\n",
      "moved to the University of British Columbia, in\n",
      "1978, where he created a Satellite Oceanography\n",
      "facility/education/research program. He was an Ap-\n",
      "pointed Professor in Aerospace Engineering Sciences\n",
      "at the University of Colorado in 1987. He is an Ad-\n",
      "junct Professor of informatics at Tor V ergata Univer-\n",
      "sity, Rome, Italy. He has authored more than 182-\n",
      "refereed publications and 2 textbooks in addition to having given 91 conference\n",
      "papers. Dr. Emery is the VP for publications of the IEEE Geoscience and Re-\n",
      "mote Sensing Society (GRSS). He received the 2004 GRSS Educational Award\n",
      "and the 2009 GRSS Outstanding Service Award. He is a Fellow of the Ameri-\n",
      "can Meteorological Society (2010), the American Astronautical Society (2011),\n",
      "and the American Geophysical Union (2012). He is the new Chair of the IEEE\n",
      "Periodicals Committee of the IEEE Technical Advisory Board.\n",
      "Authorized licensed use limited to: b-on: ISCTE. Downloaded on December 03,2023 at 19:35:37 UTC from IEEE Xplore.  Restrictions apply. \n",
      "Extracted from Road Detection Using Satellite Imagery.pdf: \n",
      "Extracted from road detection with deep learning in satellite images .pdf: Majlesi Journal of Telecommunication Devices                                                                 Vol. 12, No. 1, March 2023  \n",
      " \n",
      "43 \n",
      "Paper type: Research paper  \n",
      "DOI: 10.30486/mjtd.2023.1979006.1024  \n",
      "How to cite this paper: Z . Dorrani, “ Road Detection with Deep Learning in Satellite Images ”, Majlesi  Journal of \n",
      "Telecommunication Devices , Vol. 12, No. 1, pp. 43-47, 2023.  \n",
      "  \n",
      "Road Detection  with Deep Learning  in Satellite Images  \n",
      " \n",
      "Zohreh  Dorrani  \n",
      "Department of Electrical Engineering, Payame Noor University, Tehran, Iran.  \n",
      "Email: dorrani.z @pnu.ac.ir   (Corresponding author)  \n",
      " \n",
      "Received : 29 December 2022   Revised : 27 January 2023   Accepted : 18 February 2023  \n",
      " \n",
      " \n",
      "ABSTRACT:  \n",
      "Road detection from high -resolution satellite images using deep learning is proposed in this article. The VGG19 \n",
      "architecture, which is one of the deep convolutional neural network architectur es, is used in the proposed method. To \n",
      "detect the road, two steps are implemented. To achieve high accuracy, image segmentation is done in the first step. At \n",
      "this stage, based on the semantic division, the objects whose area is small are removed. In the se cond stage, edge \n",
      "detection of images combines two techniques of segmentation and edge detection to improve road detection. Considering \n",
      "the good accuracy of the VGG19 architecture and the need for few parameters, the obtained results are favorable. To \n",
      "check  the performance of the proposed method, the IoU criterion was used. The values obtained for this criterion show \n",
      "an improvement of more than 80%. While this criterion is less than 80% for the compared methods. The obtained results \n",
      "can be used for the purpo ses of digital mapping, transportation management and many other applications.  \n",
      " \n",
      "KEYWORDS:  Convolutional Neural Networks, Deep Learning, Edge Detection, Road Segmentation, Satellite Images , \n",
      "VGG19.  \n",
      "  \n",
      "1.  INTRODUCTION  \n",
      "Nowadays, due to the expansion of satellites, the \n",
      "analysis and analysis of satellite images  [1, 2]  is one of \n",
      "the most widely used fields in image processing  [3,4]. \n",
      "Due to the accuracy and details of the information \n",
      "required in this field, ther e are many limitations in the \n",
      "data collection stage, which include the cost of the \n",
      "process and the need to spend a long time  [5]. \n",
      "Road detection  [6] in aerial images is one of the most \n",
      "critical applications of satellite image analysis. It plays \n",
      "an essentia l role in transportation applications, because \n",
      "it creates, maintains, and updates the road network \n",
      "database. This information is used in order to perform \n",
      "activities such as traffic management, automatic \n",
      "navigation of vehicles, maintaining security and \n",
      "assessing risks and natural disasters, exchanging and \n",
      "sharing location data in the form of location data \n",
      "infrastructure, and preventing pa rallel work and \n",
      "repetition p laced  [7]. The main challenge in road \n",
      "extraction is the complex structure of images, which \n",
      "includes different objects such as roads, houses, trees, \n",
      "etc., with differences in shape and texture. Meanwhile, \n",
      "the ever -increasing increase in deep learning  [8] \n",
      "capabilities, which has brought many applications to \n",
      "various sciences and techniques, has caused  engineers to \n",
      "pay attention to it and use it in various fields to carry out \n",
      "their projects. Since the introduction of convolutional \n",
      "neural networks, various architectures have been presented that has led to deeper and thus better network \n",
      "and increased accu racy [9]. In this field, VGG \n",
      "architecture has a good structure, and simple and \n",
      "appropriate precision, which has been used in many \n",
      "applications and has had favorable results  [10]. \n",
      "Therefore, in this article, this architecture is used to \n",
      "extract roads. In the next section, some research in this \n",
      "field was examined. In the following, the proposed \n",
      "method is analyzed. At the end comes the results and \n",
      "conclusions section.  \n",
      " \n",
      "2.  RELATED WORK S \n",
      "     Satellite images contain valuable data that need to be \n",
      "processed and extract ed. For this purpose, various \n",
      "techniques have been presented that can extract useful \n",
      "information. The extraction of roads and their \n",
      "segmentation is important information that can be \n",
      "extracted and used for digital mapping. But the presence \n",
      "of factors such a s complexity, noise, lack of proper \n",
      "clarity, presence of shadows, obstructions, and \n",
      "environmental factors requires the use of deep learning \n",
      "methods to process these types of images  [11]. A method \n",
      "based on deep learning for road detection based on \n",
      "semantic segmentation and edge detection is presented, \n",
      "in which the two techniques of segmentation and edge \n",
      "detection are combined to improve road detection. In \n",
      "this model, a two -part hybrid encoder includes full -\n",
      "resolution feature extraction and high -resolution fe ature Majlesi  Journal of Telecommunication Devices                                                                 Vol. 12, No. 1, March 20 23 \n",
      " \n",
      "44 \n",
      " encoding. The second part is used to increase the overall \n",
      "receiving field, which provides backgr ound information \n",
      "to the network  [5]. \n",
      "A deep learning approach using the DenseUNet model \n",
      "is also proposed. This method uses dense connection \n",
      "units and jump  connections to solve the problem of \n",
      "decreasing accuracy as the network deepens. On the \n",
      "other hand, the integration of different scales is done in \n",
      "this model, which leads to the strengthening of the \n",
      "network by connections in different layers [ 12]. \n",
      "  SegNet  architecture [ 13] was presented for semantic \n",
      "pixel segmentation. This architecture consists of an \n",
      "encoder network that provides low -resolution encoder \n",
      "feature maps to a decoder network with a pixel \n",
      "classification layer corresponding to the feature maps to  \n",
      "generate full input resolution for pixel classification. \n",
      "Therefore, in this model, the input feature map is \n",
      "sampled with a lower resolution. Also, the calculated \n",
      "integration indices are used to perform non -linear \n",
      "sampling to eliminate the need for samplin g learning.  \n",
      "For segmentation and separation, fully complex \n",
      "networks (FCNs) [1 4] can also be used. These networks also use an encoder and a decoder. The first part extracts \n",
      "feature at different levels to segment objects of different \n",
      "sizes, and then a decode r combines the coded features.  \n",
      " \n",
      "3.  PROPOSED  METHOD  \n",
      "The VGG network has a good and simple structure \n",
      "with a small number of layers and is used in many \n",
      "computer vision fields. This network was introduced \n",
      "with two different architectures VGG16, and VGG19  \n",
      "[15 ,16]. First, the VGG16 network was proposed, and \n",
      "later, with minor changes in the VGG16 network, the \n",
      "VGG19 network was proposed. VGG16 network \n",
      "includes 16 convolutional layers or 16 parametric layers \n",
      "and VGG19 consists of 19 layers. The VGG19 network \n",
      "is an inn ovative object recognition model and is actually \n",
      "a deep CNN that performs very well in many tasks and \n",
      "datasets other than ImageNet. This network is one of the \n",
      "most widely used image recognition architectures and its \n",
      "architecture is shown in Figure 1.  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Fig. 1. VGG19.  \n",
      " \n",
      " \n",
      "The unique aspect of VGG19 is that instead of \n",
      "having a large number of meta -parameters, it is a 3x3 \n",
      "filter with a stride of 1 and the max pooling layer is a \n",
      "2x2 filter with a stride of 2. This architecture includes \n",
      "two convolutional layers with 64 3x3 filt ers that are \n",
      "placed one behind the other. Then, a 2x2 max pooling \n",
      "layer with a stride of 2 is set. In addition to sampling, \n",
      "this max -pooling layer also has the task of reducing the \n",
      "dimension of features by half. Next, two more \n",
      "convolutional layers with 128  filters of 3x3 and a 2x2 \n",
      "max pooling layer, and 2 jumps are placed. Similarly, \n",
      "three convolutional layers with 256 filters of 3x3and, \n",
      "one 2x2 max pooling layer with 2 hops are included. 3 \n",
      "convolutional layers with 512 filters of 3x3 and a max pooling laye r are the continuation of this network, which \n",
      "of course is repeated twice. Finally, the features are \n",
      "converted into a feature vector for fully connected neural \n",
      "layers. Two layers of neurons with dimensions of 4096 \n",
      "are placed one behind the other. Finally, a neural layer \n",
      "with dimensions of 1000, which corresponds to the \n",
      "number of application classes, is considered. The \n",
      "activation function called RELU has been used in all \n",
      "convolution and neuron layers.  \n",
      "Edge detection using VGG19  [17] is used for road \n",
      "detection. Edge detection from raw satellite images is \n",
      "very difficult and does not lead to desirable results. \n",
      "Hence, segmentation is done first and its output is given \n",
      "to edge detection.  Convolution+ReLU                           max pooling                fully connected+ReLU  MAX  \n",
      "POOling  CONV1  \n",
      "CONV 2 CONV 3 CONV 4 CONV 5 FC6           FC7           FC8  \n",
      "Majlesi Journal of Telecommunication Devices                                                                 Vol. 12, No. 1, March 2023  \n",
      " \n",
      "45 \n",
      " Table 1.  Size of layer in VGG19 . \n",
      "Layer  size \n",
      "CONV1  224×224×64 \n",
      "CONV2  112×112×128 \n",
      "CONV3  56×56×256 \n",
      "CONV4  28×28×512 \n",
      "CONV5  14×14×512 \n",
      "MAX  POOling  7×7×512 \n",
      "FC6 1×1×4096  \n",
      "FC7 1×1×4096  \n",
      "FC8 1×1×1000  4.  RESULT  \n",
      "Reference images [ 6] have been used to check the \n",
      "proposed method. In this reference, there are 3 satellite \n",
      "images shown in Figure 2. In order to extract roads from \n",
      "these input images, image segmentation has been done \n",
      "first, and using its output, edge detection has been done. \n",
      "For a better comparison, the ground truth is shown in \n",
      "Figure 3b. And in part c of this figure, the results of the \n",
      "method are presented.  \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      "  \n",
      " \n",
      "(A) \n",
      "   \n",
      "(B) \n",
      "   \n",
      "(C) \n",
      "Fig. 2. A: Input Image, B: Ground T ruth C: Edge Detection with proposed method.  \n",
      "Majlesi  Journal of Telecommunication Devices                                                                 Vol. 12, No. 1, March 20 23 \n",
      " \n",
      "46 \n",
      " Comparing the results obtained with ground truth \n",
      "shows that road extraction is done with proper accuracy \n",
      "using deep learning.  \n",
      "The use of evaluation criteria is a suitable method to \n",
      "compare the performance of the proposed method with \n",
      "some existing methods, therefore, the IoU criterion is \n",
      "used, which is obtained by the following relationship  \n",
      "[5]: \n",
      " \n",
      "𝐼𝑜𝑈=1\n",
      "𝑐∑𝑇𝑃(𝑘)\n",
      "𝑇𝑃(𝑘)+𝐹𝑃(𝑘)+𝐹𝑁(𝑘)𝑐\n",
      "𝑘 (1) \n",
      " \n",
      "C: the number of categories,  \n",
      "TP(k): True Positive,  \n",
      " FP(k):  False Positive,  \n",
      "FN(k): False Negative.  \n",
      " \n",
      " \n",
      "Fig. 3. IoU criterion  for proposed method compared \n",
      "to other methods  . \n",
      " \n",
      "The criterion values show that this criterion is above \n",
      "80% for the proposed method, which is lower than 80% \n",
      "for the compared methods. In the proposed method, an \n",
      "architecture is used that does not require many \n",
      "parameters and therefore can be unique in this aspect.  \n",
      " \n",
      "5.  CON CLUSION  \n",
      "Satellite imaging and satellite image proce ssing are \n",
      "used in the mapping. These maps are used in various \n",
      "fields such as agriculture, urban, regional, and forestry \n",
      "planning, and transportation. Today's digital aerial \n",
      "cameras have good storage capabilities. But to do this, \n",
      "they also need a suitable s ystem to store a complete set \n",
      "of images, and the ability to work with very large data \n",
      "and analyze them. In some cases, the ability to prepare a \n",
      "large amount of image data and transfer it to the desired \n",
      "ground reference is required. Computer vision \n",
      "techniqu es used to process these images can also help to \n",
      "prepare these images. Therefore, in this article, a method \n",
      "based on deep learning was proposed to analyze these images. The VGG19 method was used to extract the \n",
      "roads, which is implemented in two steps. The first stage \n",
      "is segmentation and the next stage is edge detection so \n",
      "that roads can be extracted. The proposed method has \n",
      "led to the improvem ent of the Io U criterion and has \n",
      "increased its value to over 80%.  \n",
      " \n",
      "REFERENCES  \n",
      "[1] M. Alkhelaiwi, W. Boulila, J.  Ahmad, A. Koubaa, \n",
      "M. Driss, “An Efficient Approach Based on \n",
      "Privacy -Preserving Deep Learning for Satellite \n",
      "Image Classification,”  Remote Sens. Vol.13, pp. \n",
      "2221 -2232, 2021.  \n",
      "[2] E McAllister, A Payo , A Novellino , T Dolphin , and \n",
      "E. Medina -Lopez. “Multispectral satellite imagery \n",
      "and machine learning for the extra ction of \n",
      "shoreline indicators,”  Coastal Eng ineering , vol. 10, \n",
      "pp.104102 -104113, 2022 . \n",
      "[3] M. Zarei, and M. Esmaeilpour,  “Enhancing the \n",
      "Quality of Satellite Images Enhancing through \n",
      "Combination of Feature and Pixel Level Image \n",
      "Fusion,” Majlesi Journal of Telecommunication \n",
      "Devices vol. 8, no. 4, pp. 141-147, 2019 . \n",
      "[4] A.A. Shahraki, and M. Emadi, “Improving Image \n",
      "Quality based on Feature Extraction and \n",
      "Gaussian Model,” Majlesi Journal of \n",
      "Telecommunication Devices , vol.  8, no. 2 , pp. 35 -\n",
      "41, 2019 . \n",
      "[5] H. Ghandorh, W . Boulila, S . Masood, and A. \n",
      "Koubaa. “Semantic segmentation and edge \n",
      "detection —Approach to road detection in very \n",
      "high resolution satellite images,”  Remote Sensing , \n",
      "vol. 14, no. 3, 1 -22, 2022.  \n",
      "[6] J. Wan, Z. Xie, Y. Xu, S. Chen, and Q.  Qiu, \n",
      "“RoadNet: A Dual -Attention Network for Road \n",
      "Extraction from High Resolution  Satellite \n",
      "Imagery ,” IEEE J. Sel. Top. Appl. Earth Obs. \n",
      "Remote Sens. Vol. 14, pp. 6302 –6315 , 2021.  \n",
      "[7] A. Abdollahi, B. Pradhan, N. Shukla, S. \n",
      "Chakraborty, A. Alamri, “Deep Learning \n",
      "Approaches Applied to Remote Sensing Datasets \n",
      "for Road Extraction: A State -Of-The-Art \n",
      "Review,” Remote Sens. Vol. 12, pp. 1444 -1452, \n",
      "2020.  \n",
      "[8] M. Ghasemi, and M, Bayati , “Improving the \n",
      "Accuracy of Detecting Cancerous Tumors Based \n",
      "on Deep Learning on MRI Images,”  Majlesi \n",
      "Journal of Telecommunication Devices , vol. 11, no. \n",
      "2 ,2022 . \n",
      "[9] Z. Dorrani, H. Farsi, and S. Moh amadzadeh, “Deep \n",
      "Learning in Vehicle Detection Using ResUNet -a \n",
      "Architecture,”  Jordan Journal of Electrical \n",
      "Engineering , Volume 8, no. 2, pp.  166-178, 2022.  \n",
      "[10] Z. Dorrani, H. Farsi, and S. Moh amadzadeh , “Edge \n",
      "Detection and Identification using Deep  Learning \n",
      "to Identify Vehicles,”  Journal of Information \n",
      "Systems and Telec ommunication, vol. 3, no. 39, pp. \n",
      "201-213, 2022.  \n",
      "[11] S. Chen, Z. Zhang, R.  Zhong, L. Zhang, H. Ma, L.  \n",
      "Liu, “A dense feature pyramid network -based \n",
      "deep learning model for road marking instance \n",
      "segmentation using MLS point clouds,”  IEEE \n",
      "Majlesi Journal of Telecommunication Devices                                                                 Vol. 12, No. 1, March 2023  \n",
      " \n",
      "47 \n",
      " Trans. Geosci. Remote Sens , vol. 59, pp. 784–800, \n",
      "2020 . \n",
      "[12] K. Heidler, L. Mou, C. Baumhoer, A. Dietz, X.X. \n",
      "Zhu, “HED -UNet: Combined S egmentation and \n",
      "Edge Detection for Monitoring the Antarctic \n",
      "Coastline,”  IEEE Trans. Geosci. Remot e Sens, vol.  \n",
      "60, pp. 1 –14, 2021.  \n",
      "[13] V. Badrinarayanan , A. Kendall , and R. Cipolla, \n",
      "“SegNet: A Deep Convolutional Encoder -\n",
      "Decoder Architecture for Image Segmentation,”  \n",
      "IEEE Trans. Pattern Anal. Mach. Intell, vol. 39, pp. \n",
      "2481 –2495, 2017.  \n",
      "[14] K. Chaiyasarn , A. Buatik , H. Mohamad , M. Zhou , S. \n",
      "Kongsilp, and N. Poovarodom,  “Integrated pixel -\n",
      "level CNN -FCN crack detection via \n",
      "photogrammetric 3D texture mapping of concrete structures,”  Automation in Construction, \n",
      "vol. 140, pp. 104388 -104398, 2022 . \n",
      "[15] A. Karacı, “VGGCOV19 -NET: automatic \n",
      "detection of COVID -19 cases from X -ray images \n",
      "using modified VGG19 CNN architecture  and \n",
      "YOLO algorithm,”  Neural Computing and \n",
      "Applications, vol. 34, no. 10, pp. 8253 -8274 , 2022 . \n",
      "[16] M. Ghasemzade , “Extracting Image Features \n",
      "Through Deep Learning,” Majlesi Journal of \n",
      "Telecommunication Devices , vol.  9, no. 3 , pp. 109-\n",
      "114, 2020.  \n",
      "[17] MJ. Ahmed, and P. Nayak, “Detection of \n",
      "Lymphoblastic Leukemia Using VGG19 Model,”  \n",
      "In 2021 Fifth International Conference on I -SMAC \n",
      "(IoT in Social, Mobile, Analytics and Cloud)  (I-\n",
      "SMAC), pp. 716 -723, 2021.\n",
      " \n",
      " \n",
      " \n",
      "Extracted from Robust_Road_Extraction_for_High_Resolution_Satellite_Images.pdf: ROBUST ROAD EXTRACTION FOR HIGH RESOLUTION SATELLITE IMAGES\n",
      "Emmanuel Christophe and Jordi Inglada\n",
      "CNES, BPI 1219, 18 avenue Edouard Belin, 31401 Toulouse Cedex 9, France\n",
      "emmanuel.christophe@cnes.fr, jordi.inglada@cnes.fr\n",
      "ABSTRACT\n",
      "Automatic road extraction is a cr itical feature for an efﬁcient\n",
      "use of remote sensing imagery in most contexts. This paperproposes a robust geometric method to provide a ﬁrst step ex-\n",
      "traction level. These results can be used as an initialization\n",
      "for other algorithms or as a starting point for manual road ex-traction. Results of the extraction are vectorized for GIS in-\n",
      "tegration and for a better interaction with human experts that\n",
      "can reﬁne the results. The algorithm is fast, has very few pa-\n",
      "rameters and is only slightly affected by the image properties\n",
      "(resolution, noise). The algorithm is available in the open-source Orfeo Toolbox.\n",
      "Keywords: Remote sensing, Road extraction\n",
      "1. INTRODUCTION\n",
      "Road extraction is a critical feature for an efﬁcient use of high\n",
      "resolution satellite images. There are many applications ofroad extraction: update of GIS database, reference for im-\n",
      "age registration, help for identiﬁcation algorithms and rapid\n",
      "mapping for example. Road network can be used to regis-\n",
      "ter an optical image with a map or an optical image with a\n",
      "radar image for example. Road network extraction can helpfor other algorithms: isolated building detection, bridge de-\n",
      "tection. In these cases, a rough extraction can be sufﬁcient.\n",
      "In the context of response to crisis, a fast mapping is neces-sary: within 6 hours, infrastructures for the designated area\n",
      "are required [1]. Within this timeframe, a manual extraction\n",
      "is inconceivable and an automatic help is necessary.\n",
      "A high resolution satellite image typically has a resolution\n",
      "of 0.5 to 1 m and four spectral bands. Using the result of a\n",
      "fusion, a product combining four spectral bands at the higher\n",
      "resolution can be obtained. This is usually the case for theQuickbird satellite from DigitalGlobe and will be the standard\n",
      "product for the future Pleiades constellation from CNES.\n",
      "One of the most important key features of satellites is\n",
      "their ability to cover an important area, thus gathering a huge\n",
      "amount of data. One Pleiades scene will cover an area of20×20 km. A manual exploitation of these data, even just\n",
      "for road extraction is not possible. An operator would need\n",
      "to visualize more than 1000 screens to process these data at\n",
      "full resolution. An efﬁcient automatic algorithm leading to agood (even if not perfect) solution would facilitate the work\n",
      "of remote sensing experts.\n",
      "Since 1990, different methods have been proposed to pro-\n",
      "vide automatic or semi-automatic road extraction in remote\n",
      "sensing images. In [2], an exhaustive bibliography on auto-\n",
      "mated road extraction is provided. These methods includesnakes [3], higher order active contours [4], dynamic pro-\n",
      "gramming [5] or probabilistic approaches [6]. However, to\n",
      "our best knowledge, none of these algorithms satisﬁes the op-erators, mainly because of com putation time. In most situa-\n",
      "tions, road extraction is still manual.\n",
      "The main problem of these methods is the difﬁculty to\n",
      "provide the best parameters for a given image. Given a newimage, the user often has to try different combinations be-\n",
      "fore obtaining a satisfactory result. The computation time is\n",
      "also often a problem. As the resolution of optical sensors in-creases, size of images increases as well, thus increasing the\n",
      "computation time necessary to pr ocess a scene. Information\n",
      "provided by the color is also often misused, most algorithms\n",
      "focusing on the panchromatic data.\n",
      "The aim of this paper is to provide a simple, fast, robust\n",
      "and efﬁcient algorithm to extr act roads. The algorithm pro-\n",
      "vides a vectorized result. The only entry parameter is thecolor of the road or the spectrum if more spectral bands are\n",
      "available. Results are not expected to be perfect but rather to\n",
      "be a good initialization for more complex algorithms or for\n",
      "human reﬁnement.\n",
      "2. ROAD DETECTION\n",
      "The color of the road provides a rich information which of-\n",
      "ten enables to distinguish between roads and vegetation. Thiscolor information, usually in four or more spectral bands can\n",
      "be interpreted as a vector. However, most algorithms prefer\n",
      "to work with scalar data. One efﬁcient way to convert this\n",
      "spectral information to a scalar data is to use the spectral an-\n",
      "gle with respect to a reference pixel. The spectral angle isdeﬁned as:\n",
      "SA =cos\n",
      "−1/parenleftBiggnb\n",
      "∑\n",
      "b=1r(b).p(b)/slashBig/radicalBigg\n",
      "nb\n",
      "∑\n",
      "b=1r(b)2nb\n",
      "∑\n",
      "b=1p(b)2/parenrightBigg\n",
      ",(1)V - 437 1-4244-1437-7/07/$20.00 ©2007 IEEE ICIP 2007\n",
      "Authorized licensed use limited to: b-on: ISCTE. Downloaded on November 21,2023 at 13:39:21 UTC from IEEE Xplore.  Restrictions apply. (a)\n",
      " (b)\n",
      " (c)\n",
      " (d)\n",
      " (e)\n",
      "Fig. 1 .Original image (a), Distance according to spectral angle (b), Gra dient directions (c), line detector (d) and after vectorization (e).\n",
      "bbeing the spectral band, ris the reference pixel and p\n",
      "the current pixel.\n",
      "There are two main advantages to using this representa-\n",
      "tion. The ﬁrst one being that the following algorithm is work-\n",
      "ing on the spectral angle image and does not depend on thenumber of bands. Thus the road extraction process can be\n",
      "applied to multispectral images with any number of bands.\n",
      "The second advantage is that choosing the reference pixel, wewill be able to extract either tarred roads or dirt tracks. The\n",
      "spectral angle is taken between the reference vector pixel and\n",
      "the current vector pixel. The resulting image contains all the\n",
      "roads in the darker color. Figure 1 (b) shows the distance be-\n",
      "tween each pixel and the reference pixel.\n",
      "On this spectral angle image we apply a line detection\n",
      "method based on a constrained gradient. This method was\n",
      "described in [7]. A gradient ﬁlter is ﬁrst applied to the image.Each pixel contains gradient direction and gradient intensity\n",
      "values. An example of gradient directions is illustrated on ﬁg-\n",
      "ure 1 (c). Then, knowing that the road is darker than pixels\n",
      "around, we know that the gradient direction will be opposite\n",
      "on each side of the road, as illustrated on ﬁgure 2 (a). To usethis property, we compute the scalar product between oppo-\n",
      "site gradient vectors around a pixel. The higher (in absolute\n",
      "value) scalar product which is also negative gives us the di-rection of the road. On ﬁgure 2 (b), the highest scalar value is\n",
      "obtained between pixels a\n",
      "1and b1, leading to the dotted red\n",
      "direction vector with the computed length. The spectral angle\n",
      "provides an important property that the original algorithm did\n",
      "not have: as we know that the road is darker, we can keep onlythe pixel where the neighbor gradient vectors point toward the\n",
      "pixel.\n",
      "To improve the line following, pixels which do not have\n",
      "the maximum scalar value across d irection are removed (Fig. 3).\n",
      "3. VECTORIZATION\n",
      "Vectorization of the extracted ro ads is decisive for an efﬁcient\n",
      "use in GIS systems. From the previous step, we had obtained\n",
      "an image with pieces of roads, often irregular as illustrated on\n",
      "ﬁgure 1 (d).\n",
      "After the previous part, paths can be extracted from the(a)a0 a1\n",
      "a2\n",
      "a3 b0\n",
      "b1b2b3\n",
      "(b)\n",
      "Fig. 2 .Gradient representation and the roads: (a) gradient direction\n",
      "towards the roads; (b) computation of the direction and scalar values\n",
      "for each pixel.\n",
      "Fig. 3 .Removal of non maximum scalar values across direction:\n",
      "one of the neighbor pixel across the road direction has a greater value\n",
      "than the current pixel which is removed for the path construction.\n",
      "image. To get a better localization, path vertices can be lo-\n",
      "cated at non integer positions (Fig. 4). Their position is ob-\n",
      "tained with the weighted barycenter of few pixels on the given\n",
      "direction. However, paths obtained with this process containtoo many vertices: about as many as the length of the path.\n",
      "They are also usually too short due to the noise on the gra-\n",
      "dient image. Few steps are necessary to reﬁne these pathsaccording to general road properties.\n",
      "Many vertices can be removed as they are usually aligned.\n",
      "Figure 5 illustrates this step. dis the distance between one\n",
      "vertex and the proposed new path. If the distance between\n",
      "all old vertices and the new proposed path is lower than one\n",
      "pixel, the new path is accepted.\n",
      "One result of the vectorization process is that paths can\n",
      "present a sharp angle at their extremity due to the noise on the\n",
      "gradient image. This sharp angle will be damageable for theV - 438\n",
      "Authorized licensed use limited to: b-on: ISCTE. Downloaded on November 21,2023 at 13:39:21 UTC from IEEE Xplore.  Restrictions apply. Fig. 4 .Vector coordinates are non integer, allowing a better accu-\n",
      "racy.\n",
      "Fig. 5 .Simplifying paths.\n",
      "next step. In general roads contain smooth curves and do not\n",
      "have sharp turns. To comply with this property we split paths\n",
      "which contain a sharp angle between two segments. An angle\n",
      "is considered as sharp if it is above π/8. The exact value does\n",
      "not inﬂuence much results of the algorithm.\n",
      "To allow the detected paths to go over tree shadows or\n",
      "vehicles on the road which imp act the gradient, a relation be-\n",
      "tween paths must be considered. The relation is described on\n",
      "ﬁgure 6. Within a given search distance d, which depends on\n",
      "the resolution, a link can be found between two paths. To be\n",
      "accepted, these paths have to comply with some conditions:\n",
      "angles between the ﬁrst path and the link, α1, the second path\n",
      "and the link, α2, and the last segment of both paths, α1−α2\n",
      "have to be within a certain range. For a 0.7 m resolution im-\n",
      "age, the search distance considered is 40 pixels and the angle\n",
      "was chosen to be within the [−π/8;π/8]interval.\n",
      "Fig. 6 .Linking paths.\n",
      "After this step, some very short paths are still present on\n",
      "the image. They usually correspond to noise and do not pro-\n",
      "vide much information so they are removed.\n",
      "The last step associates a conﬁdence value to each path\n",
      "according to the spectral angle v alue along the path. One path\n",
      "can have a low spectral angle value all along, thus the algo-\n",
      "rithm will be conﬁdent that this path is a road. Some otherpaths can have higher spectral angle value leading the algo-\n",
      "rithm to be less conﬁdent. The conﬁdence value is presented\n",
      "by the color of the path: darker color for low conﬁdence and\n",
      "brighter color for high conﬁdence.4. RESULTS\n",
      "All the results are obtained within the Orfeo Toolbox (OTB)\n",
      "framework [8]. The OTB is an open source set of tools for\n",
      "remote sensing data exploitation. This algorithm is included\n",
      "in the latest release of OTB and follows the principles of re-producible research.\n",
      "The algorithm is applied on two extracts from a 1000 ×\n",
      "1000 Quickbird image. We compare the results with thoseobtained only with the line detector from [7]. The algorithm\n",
      "is fast. On a standard PIV 2.8 GHz, processing a 1000 ×1000\n",
      "image takes less than 3 s. This short time enables interactive\n",
      "processing during visualization at full resolution on a screen\n",
      "size.\n",
      "On ﬁgure 8 (c), we can see that the new algorithm pro-\n",
      "vides improved results. Roads are better deﬁned and without\n",
      "staircase effects. Segments have an information about howconﬁdent the algorithm is: the dark segments are those for\n",
      "which the algorithm is not very conﬁdent (the segment on\n",
      "trees for example). The railroad track in the lower right cor-ner is not mistaken for a road. In two places, the algorithm\n",
      "successfully ﬁnds the road hidden by tree occultations.\n",
      "On ﬁgure 8 (f), focus was on extracting the main roads\n",
      "with the reference pixel taken from one of these roads. The\n",
      "algorithm manages to make the difference with the secondary\n",
      "roads from the top right of the image. Most main roads, or at\n",
      "least portions of them are extracted. Few false alarms appear\n",
      "on some buildings.\n",
      "The same algorithm can be app lied to radar images. As we\n",
      "have seen before, the computation of the spectral angle leads\n",
      "to a representation of the road in darker color. In radar images,as roads usually present specu lar reﬂections, they also appear\n",
      "in darker color (Fig. 7 (a)). We applied the same algorithm\n",
      "directly on the radar image instead on the spectral angle im-\n",
      "age to obtain the results presented in Fig. 7 (b). These results\n",
      "are good despite of the noise. Radar images contain a multi-plicative noise so the gradient ﬁlter is not the most adapted.\n",
      "A ratio of means, as in [9], will probably give better results.\n",
      "(a)\n",
      " (b)\n",
      "Fig. 7 .Road extraction result on a radar image: the same algorithm\n",
      "is applied without the spectral angle computation.V - 439\n",
      "Authorized licensed use limited to: b-on: ISCTE. Downloaded on November 21,2023 at 13:39:21 UTC from IEEE Xplore.  Restrictions apply. Fig. 8 (a) Original image\n",
      " Fig. 8 (b) Line detector\n",
      " Fig. 8 (c) Final algorithm\n",
      "Fig. 8 (d) Original image\n",
      " Fig. 8 (e) Line detector\n",
      " Fig. 8 (f) Final algorithm\n",
      "5. CONCLUSION\n",
      "The presented algorithm is automatic with very little interac-\n",
      "tion from the users. The main one is the color (or spectrum)\n",
      "of the road in which the user is interested in. The only param-\n",
      "eter which has a signiﬁcant imp act on the results is the search\n",
      "area to link paths. This parameter can be easily related to the\n",
      "image resolution. We have seen that the algorithm, originally\n",
      "designed for optical images, can successfully extract roads on\n",
      "a radar image. Moreover, this algorithm is fast, allowing in-\n",
      "teractive computation. And ﬁnally, this algorithm uses most\n",
      "of available properties of roads in high resolution satellite im-\n",
      "ages: spectral information (with the computation of the spec-\n",
      "tral angle) and the shape (ﬁrst with the gradient and then with\n",
      "the vectors).\n",
      "Of course, results are not perfect but could be reﬁned by\n",
      "more complex methods. These methods, usually more costly\n",
      "in terms of computation time, could be applied only in the\n",
      "selected area.\n",
      "6. ACKNOWLEDGMENTS\n",
      "The authors wish to thank Vinciane Lacroix from the Royal Military\n",
      "Academy for the fruitful discussions and for providing results with\n",
      "the original algorithm.7. REFERENCES\n",
      "[1] J. B´ equignon et al., “Report of the working group on space in-\n",
      "frastructure for the GMES emergency response service,” Tech.\n",
      "Rep. 3.2, DDSC, DLR, CNES, ASI and JRC, Dec. 2006.\n",
      "[2] J. Mena, “State of the art on automatic road extraction for\n",
      "GIS update: a novel classiﬁcation,” Pattern Recognition Letters ,\n",
      "vol. 24, pp. 3037–3058, 2003.\n",
      "[3] I. Laptev, H. Mayer, T. Lindeberg, W. Eckstein, C. Steger, and\n",
      "A. Baumgartner, “Automatic extraction of roads from aerial im-\n",
      "ages based on scale space and snakes,” Machine Vision and Ap-\n",
      "plications , vol. 12, pp. 23–31, 2000.\n",
      "[4] M. Rochery, I. Jermyn, and J. Zerubia, “Higher order active\n",
      "contours,” International Journal of Computer Vision , vol. 69,\n",
      "pp. 27–42, Aug. 2006.\n",
      "[5] M. Barzohar and D. B. Cooper, “Automatic ﬁnding of main\n",
      "roads in aerial images by using geometric-stochastic models and\n",
      "estimation,” IEEE Transactions on Pattern Analysis and Ma-\n",
      "chine Intelligence , vol. 18, pp. 707–721, July 1996.\n",
      "[6] M. Bicego, S. Dalﬁni, G. Vernazza, and V . Murino, “Auto-\n",
      "matic road extraction from aerial images by probabilistic con-\n",
      "tour tracking,” in IEEE International Conference on Image Pro-\n",
      "cessing, ICIP’03 , vol. 3, pp. 585–588, 2003.\n",
      "[7] V . Lacroix and M. Acheroy, “Feature extraction using the con-\n",
      "strained gradient,” ISPRS Journal of Photogrammetry & Remote\n",
      "Sensing , vol. 53, pp. 85–94, 1998.\n",
      "[8] “The ORFEO toolbox software guide.” http://otb.cnes.fr, 2007.\n",
      "[9] F. Tupin, H. Maˆ ıtre, J.-F. Mangin, J.-M. Nicolas, and E. Pech-\n",
      "ersky, “Detection of linear features in SAR images: applicationto road network extraction,” IEEE Transactions on Geoscience\n",
      "and Remote Sensing , vol. 36, pp. 434–453, Mar. 1998.V - 440\n",
      "Authorized licensed use limited to: b-on: ISCTE. Downloaded on November 21,2023 at 13:39:21 UTC from IEEE Xplore.  Restrictions apply. \n",
      "Extracted from Semi-Automatic Method of Extracting Road Networks fromHigh-Resolution Remote-Sensing Images.pdf: Citation: Yang, K.; Cui, W.; Shi, S.;\n",
      "Liu, Y.; Li, Y.; Ge, M. Semi-Automatic\n",
      "Method of Extracting Road Networks\n",
      "from High-Resolution\n",
      "Remote-Sensing Images. Appl. Sci.\n",
      "2022 ,12, 4705. https://doi.org/\n",
      "10.3390/app12094705\n",
      "Academic Editor: Feng Guo\n",
      "Received: 22 February 2022\n",
      "Accepted: 5 May 2022\n",
      "Published: 7 May 2022\n",
      "Publisher’s Note: MDPI stays neutral\n",
      "with regard to jurisdictional claims in\n",
      "published maps and institutional afﬁl-\n",
      "iations.\n",
      "Copyright: © 2022 by the authors.\n",
      "Licensee MDPI, Basel, Switzerland.\n",
      "This article is an open access article\n",
      "distributed under the terms and\n",
      "conditions of the Creative Commons\n",
      "Attribution (CC BY) license (https://\n",
      "creativecommons.org/licenses/by/\n",
      "4.0/).\n",
      "applied  \n",
      "sciences \n",
      "Article\n",
      "Semi-Automatic Method of Extracting Road Networks from\n",
      "High-Resolution Remote-Sensing Images\n",
      "Kaili Yang1,2\n",
      ", Weihong Cui1,2,*, Shu Shi3, Yu Liu2,4, Yuanjin Li1and Mengyu Ge1\n",
      "1School of Remote Sensing and Information Engineering, Wuhan University, Wuhan 430079, China;\n",
      "kailiyang@whu.edu.cn (K.Y.); liyuanjine@whu.edu.cn (Y.L.); mengyu_ge@whu.edu.cn (M.G.)\n",
      "2Key Laboratory of Aerospace Information Application of CETC, Shijiazhuang 050002, China;\n",
      "liuyu0213@stu.xidian.edu.cn\n",
      "3College of Geomatics, Xi’an University of Science and Technology, Xi’an 710054, China; shiishu@163.com\n",
      "4School of Artiﬁcial Intelligence, Xidian University, Xi’an 710071, China\n",
      "*Correspondence: whcui@whu.edu.cn\n",
      "Abstract: Road network extraction plays a critical role in data updating, urban development, and\n",
      "decision support. To improve the efﬁciency of labeling road datasets and addressing the problems\n",
      "of traditional methods of manually extracting road networks from high-resolution images, such as\n",
      "their slow speed and heavy workload, this paper proposes a semi-automatic method of road network\n",
      "extraction from high-resolution remote-sensing images. The proposed method needs only a few\n",
      "points to extract a single road in the image. After the roads are extracted one by one, the road network\n",
      "is generated according to the width of each road and the spatial relationships among the roads. For\n",
      "this purpose, we use regional growth, morphology, vector tracking, vector simpliﬁcation, endpoint\n",
      "modiﬁcation, road connections, and intersection connections to generate road networks. Experiments\n",
      "on four images with different terrains and different resolutions show that this method has high\n",
      "extraction accuracy under different image conditions. The comparisons with the semi-automatic\n",
      "GVF-snake method based on regional growth also showed its advantages and potentiality. The\n",
      "proposed method is a novel form of semi-automatic road network extraction, and it signiﬁcantly\n",
      "increases the efﬁciency of road network extraction.\n",
      "Keywords: high-resolution image; road extraction; semi-automatic; morphology; vector processing\n",
      "1. Introduction\n",
      "With the acceleration of urban and rural construction, quickly identifying and extract-\n",
      "ing roads and updating road networks has become a crucial issue [ 1–4]. As a signiﬁcant\n",
      "component of urban transportation, roads play an essential role in political [ 5–7], eco-\n",
      "nomic [ 5,8], and military ﬁelds [ 9,10], among others. At present, with the development\n",
      "of high-spatial-resolution remote sensors, the spatial resolution of remote-sensing images\n",
      "can be as ﬁne as the submeter level [ 11]. Unlike the thin line shape that roads present\n",
      "in low-resolution images, roads in high-resolution images are continuous homogeneous\n",
      "regions, which means that roads can be extracted from these images more accurately [ 10,12].\n",
      "However, due to the inﬂuence of ‘different objects with similar spectra’, different image\n",
      "resolutions, different road types, road occlusions, etc., the difﬁculty of designing road\n",
      "extraction algorithms is also increasing [13,14].\n",
      "Automatic road extraction methods represented by deep learning have been widely\n",
      "reported in previous studies [ 1,15–17]. For example, Gao et al. extracted the roads from\n",
      "optical satellite images using a reﬁned deep residual convolutional neural network with a\n",
      "post-processing stage [ 2]. Yang et al. proposed using recurrent convolution neural network\n",
      "U-Net to extract roads and predict center lines [ 3]. Zhang et al. applied a fully convolu-\n",
      "tional network (FCN) that introduced a weighted loss function to extract roads from aerial\n",
      "images [ 18]. However, they all need a sufﬁcient number of representative training data and\n",
      "Appl. Sci. 2022 ,12, 4705. https://doi.org/10.3390/app12094705 https://www.mdpi.com/journal/applsciAppl. Sci. 2022 ,12, 4705 2 of 21\n",
      "the prediction ability is highly related to the training samples fed into the model [3,19,20] .\n",
      "Owing to the complexity of roads themselves, automatic road extraction models cannot\n",
      "achieve good results through the direct application on another dataset [21–24] . Therefore,\n",
      "the limited remote-sensing datasets with labels are an obstacle to developing and eval-\n",
      "uating new deep learning methods [ 19,25]. In actual road labeling, road networks are\n",
      "still extracted by traditional manual sketching [ 26]. Although manual operation ensures\n",
      "that the topological relationship is established accurately, the workload is large and the\n",
      "efﬁciency is low [ 27]. It is necessary to propose a semi-automatic method that can improve\n",
      "the efﬁciency of road sample labeling [ 3]. Considering the need for deep learning methods\n",
      "and to improve the labeling efﬁciency to build new road datasets, semi-automatic road\n",
      "extraction methods combined with computer visual interpretation are still the focus of\n",
      "current research [9,13,22].\n",
      "In accordance with the process and focus of the extraction algorithm, existing semi-\n",
      "automatic road extraction methods are mainly based on regional growth [ 28–30]; dynamic\n",
      "programming [ 31–33]; edge detection [ 34], including contour identiﬁcation by ﬁnding\n",
      "the gradient and potential of the image [ 35] followed by edge thinning and division [ 36];\n",
      "image segmentation [ 9,10,23]; template matching [ 37,38]; active contour models such as\n",
      "Snake [39–41]; and machine learning and neural networks [ 2,21,37,42]. However, low\n",
      "efﬁciency and poor robustness remain as problems. The approaches of dynamic program-\n",
      "ming and Snake have complicated manual settings and are subject to the time-consuming\n",
      "process of iterative optimization [ 31,39]. Algorithms based on template matching and\n",
      "neural networks are greatly inﬂuenced by the template and the sample of the label. Hence,\n",
      "the adaptability of these models to different roads is not reliable [ 37]. Numerous road\n",
      "extraction methods based on regional growth, edges, and image segmentation have been\n",
      "developed, but they may confuse roads with surrounding objects due to the complexity of\n",
      "road edges [ 43]. At the same time, most of these semi-automatic methods focus on image\n",
      "raster processing, and there is little consideration of centerline extraction, vectorization,\n",
      "and road network generation. However, road vectors play a vital role in the construction\n",
      "of geographic information systems and the comprehensive analysis of information, and\n",
      "they need to be taken seriously [ 35,44]. Moreover, the methods mentioned above cannot\n",
      "completely overcome road extraction difﬁculties, such as occlusions, shadows, and varying\n",
      "resolutions and widths.\n",
      "Considering the problems described above, to improve the labeling efﬁciency of the\n",
      "road dataset, we propose a new method of interactive road extraction and automatic road\n",
      "network generation. The main contributions of this paper follow:\n",
      "1. A complete road network extraction framework with high accuracy and availability\n",
      "is proposed. With only a few seed points, the whole road can be obtained quickly.\n",
      "First, the width and seed points of a road are set interactively, and the skeleton of the\n",
      "road is extracted by using regional growth and morphological algorithms. Then, a\n",
      "single-road vector is obtained after vector tracking, vector simpliﬁcation, endpoint\n",
      "modiﬁcation, and road connection. Finally, the road network is generated by using\n",
      "intersection connection and buffer algorithms.\n",
      "2. To further improve the effectiveness of the proposed method, we adopt the road\n",
      "segment modiﬁcation and road network construction strategy using the combination\n",
      "of grid image level and vector level. At the raster level, morphological algorithms are\n",
      "used to acquire the initial road segment, and at the vector level, further corrections\n",
      "and connections are completed based on road geometric features. For example,\n",
      "considering the ‘T’, ‘Y’, and ‘+’ shape of the intersection, an intersection connection\n",
      "algorithm is proposed.\n",
      "3. The strategy proposed in this paper can be successfully applied to the extraction of\n",
      "rural areas, suburbs, and urban areas. At the same time, it also has a certain degree of\n",
      "correction effect on occlusion and shadow problems. The algorithm for extracting a\n",
      "single road can extract roads with a length of more than 4000 pixels at a time, whichAppl. Sci. 2022 ,12, 4705 3 of 21\n",
      "is fast and convenient, and has great potential for the application of labeling images\n",
      "for deep learning.\n",
      "2. Materials and Methods\n",
      "2.1. Experimental Data\n",
      "Four different types of remote-sensing images were selected for road extraction ex-\n",
      "periments. One type is aerial images with a spatial resolution of 0.2 m of a certain rural\n",
      "area in Huizhou, Guangdong Province, as shown in Figure 1a, an image with a size of\n",
      "5000 pixels\u00025000 pixels is selected and recorded as Data 1. The second type is GF-2\n",
      "satellite images with a spatial resolution of 1 m of a certain suburban district in Wuhan,\n",
      "Hubei Province, as shown in Figure 1b, an image with the same size as Data 1 is selected\n",
      "and recorded as Data 2. The third type is a partial image of the Massachusetts Road Data\n",
      "Set [ 45], with a size of 630 pixels \u0002600 pixels and spatial resolution of 1 m, which is\n",
      "recorded as Data 3 and shown in Figure 1c. For this image, a part of the image containing\n",
      "the road was left, and more details of the road can be seen by zooming in. The last image\n",
      "is one image in DeepGlobe Datasets [ 46], with a size of 1024 pixels \u00021024 pixels and a\n",
      "spatial resolution of 0.5 m, which is recorded as Data 4 and shown in Figure 1d. Data 1 has\n",
      "only 6 long roads and 4 simple intersections but at least 8 obvious occlusions on the roads.\n",
      "Data 2 includes 21 roads and 26 different intersections, together with 14 areas of buildings.\n",
      "Additionally, there are more than 15 roads and 20 intersections in Data 3 and Data 4, with\n",
      "many buildings on the roadside.\n",
      "Appl. Sci. 2022, 12, x FOR PEER REVIEW   3 of 22 \n",
      " \n",
      "of correction  effect on occlusion  and shadow problems.  The algorithm  for extracting  \n",
      "a single road can extract roads with a length of more than 4000 pixels at a time, which \n",
      "is fast and convenient,  and has great potential  for the application  of labeling images \n",
      "for deep learning.  \n",
      "2. Materials  and Methods  \n",
      "2.1. Experimental  Data \n",
      "Four different  types of remote‐sensing images were selected for road extraction  ex‐\n",
      "periments.  One type is aerial images with a spatial resolution  of 0.2 m of a certain rural \n",
      "area in Huizhou,  Guangdong  Province,  as shown in Figure 1a, an image with a size of \n",
      "5000 pixels × 5000 pixels is selected and recorded  as Data 1. The second type is GF‐2 sat‐\n",
      "ellite images with a spatial resolution  of 1 m of a certain suburban  district in Wuhan, Hu‐\n",
      "bei Province,  as shown in Figure 1b, an image with the same size as Data 1 is selected and \n",
      "recorded  as Data 2. The third type is a partial image of the Massachusetts  Road Data Set \n",
      "[45], with a size of 630 pixels × 600 pixels and spatial resolution  of 1 m, which is recorded  \n",
      "as Data 3 and shown in Figure 1c. For this image, a part of the image containing  the road \n",
      "was left, and more details of the road can be seen by zooming  in. The last image is one \n",
      "image in DeepGlobe  Datasets [46], with a size of 1024 pixels × 1024 pixels and a spatial \n",
      "resolution  of 0.5 m, which is recorded  as Data 4 and shown in Figure 1d. Data 1 has only \n",
      "6 long roads and 4 simple intersections  but at least 8 obvious occlusions  on the roads. Data \n",
      "2 includes 21 roads and 26 different  intersections,  together with 14 areas of buildings.  Ad‐\n",
      "ditionally,  there are more than 15 roads and 20 intersections  in Data 3 and Data 4, with \n",
      "many buildings  on the roadside.  \n",
      "  \n",
      "(a)  (b) \n",
      "   \n",
      "(c)  (d) \n",
      "Figure 1. Experimental  data: (a) Data 1 in a rural area, with a size of 5000 pixels × 5000 pixels and a \n",
      "spatial resolution  of 0.2 m; (b) Data 2 in a suburban  area, with a size of 5000 pixels × 5000 pixels and \n",
      "a spatial resolution  of 1.0 m; (c) Data 3 in an urban area with a size of 630 pixels × 600 pixels and \n",
      "Figure 1. Experimental data: ( a) Data 1 in a rural area, with a size of 5000 pixels \u00025000 pixels and a\n",
      "spatial resolution of 0.2 m; ( b) Data 2 in a suburban area, with a size of 5000 pixels \u00025000 pixels and a\n",
      "spatial resolution of 1.0 m; ( c) Data 3 in an urban area with a size of 630 pixels \u0002600 pixels and also a\n",
      "spatial resolution of 1 m; ( d) Data 4 in a rural residential area with a size of 1024 pixels\u00021024 pixels\n",
      "and a spatial resolution of 0.5 m.Appl. Sci. 2022 ,12, 4705 4 of 21\n",
      "2.2. Methodology\n",
      "In remote-sensing images, roads have obvious shape characteristics, including high\n",
      "aspect ratio, small width changes, small curvatures, and ‘T’-, ‘Y’-, and ‘+’-shaped intersec-\n",
      "tions, etc. The radiation characteristics of the roads are also obvious, such as consistent\n",
      "gray values, obvious edges, and uniform textures, etc. As shown in Figure 2, based on the\n",
      "characteristics of the road, the road network extraction method is composed of two parts:\n",
      "interactive single-road extraction and road network generation.\n",
      "Appl. Sci. 2022, 12, x FOR PEER REVIEW   4 of 22 \n",
      " \n",
      "also a spatial resolution  of 1 m; (d) Data 4 in a rural residential  area with a size of 1024 pixels × 1024 \n",
      "pixels and a spatial resolution  of 0.5 m. \n",
      "2.2. Methodology  \n",
      "In remote‐sensing images, roads have obvious shape characteristics,  including  high \n",
      "aspect ratio, small width changes,  small curvatures,  and ‘T’‐, ‘Y’‐, and ‘+’‐shaped intersec‐\n",
      "tions, etc. The radiation  characteristics  of the roads are also obvious,  such as consistent  \n",
      "gray values, obvious edges, and uniform textures,  etc. As shown in Figure 2, based on the \n",
      "characteristics  of the road, the road network extraction  method is composed  of two parts: \n",
      "interactive  single‐road extraction  and road network generation.  \n",
      " \n",
      "Figure 2. Road network extraction  flow chart. \n",
      "Single‐road extraction  is performed  in four steps. First, a single growing  point on the \n",
      "road is selected manually  to acquire the road width automatically.  Several seed points are \n",
      "further selected and a working  area of the algorithm  is generated  at the same time accord‐\n",
      "ing to the road width. Second, the skeleton of the road is obtained  by region growth, mor‐\n",
      "phological  closing operations,  and image thinning.  Third, a tracking algorithm  is used to \n",
      "vectorize  the skeleton.  Then, the optimized  road segments  are obtained  through simplifi‐\n",
      "cation and deletion of overly short segments.  Finally, the endpoint  modifications  and road \n",
      "connections  are processed  to optimize  the road segments  further, and the broken road \n",
      "segments  are connected  into a complete  road vector. \n",
      "After obtaining  a series of road vectors one by one, the road network generation  al‐\n",
      "gorithm is performed.  First, complete  vectorized  centerlines  are generated  by intersection  \n",
      "connection.  After that, the road network is formed through the buffer algorithm.  \n",
      "2.2.1. Interactive  Single‐Road Extraction  \n",
      "Image Road Skeleton  Extraction  \n",
      "To obtain a complete  road skeleton from high‐resolution  images, considering  the \n",
      "glitches, holes, and interruptions  that may be encountered  during the extraction  process, \n",
      "Figure 2. Road network extraction ﬂow chart.\n",
      "Single-road extraction is performed in four steps. First, a single growing point on the\n",
      "road is selected manually to acquire the road width automatically. Several seed points\n",
      "are further selected and a working area of the algorithm is generated at the same time\n",
      "according to the road width. Second, the skeleton of the road is obtained by region growth,\n",
      "morphological closing operations, and image thinning. Third, a tracking algorithm is\n",
      "used to vectorize the skeleton. Then, the optimized road segments are obtained through\n",
      "simpliﬁcation and deletion of overly short segments. Finally, the endpoint modiﬁcations\n",
      "and road connections are processed to optimize the road segments further, and the broken\n",
      "road segments are connected into a complete road vector.\n",
      "After obtaining a series of road vectors one by one, the road network generation\n",
      "algorithm is performed. First, complete vectorized centerlines are generated by intersection\n",
      "connection. After that, the road network is formed through the buffer algorithm.\n",
      "2.2.1. Interactive Single-Road Extraction\n",
      "Image Road Skeleton Extraction\n",
      "To obtain a complete road skeleton from high-resolution images, considering the\n",
      "glitches, holes, and interruptions that may be encountered during the extraction process, we\n",
      "use regional growth, morphological closing operations, and thinning algorithms to extract\n",
      "roads. To improve the accuracy and efﬁciency of road extraction, the spectral and geometric\n",
      "characteristics of roads are considered. We determine the implementation area of the\n",
      "algorithm according to the road width, thereby avoiding the problem of excessive growth.Appl. Sci. 2022 ,12, 4705 5 of 21\n",
      "For the purpose of obtaining the road width, we ﬁrst perform regional growth in a\n",
      "local area to obtain one segment with parallel edges. The operation of regional growth\n",
      "can be described as follows: ﬁrst, select a certain pixel on the road as the growing point.\n",
      "Especially, for road width determination, the growing point should be located on a road\n",
      "segment without glitches, holes, or interruptions. Next, compare the grayscale of the\n",
      "growing point with its eight neighborhoods. Finally, combine the neighbor pixels meeting\n",
      "the merge threshold requirements with growing point and set it as a new growing point\n",
      "until no new pixels are combined. Regional growth has a simple rule, high calculating\n",
      "speed and interactivity, which is suitable for road extraction requirements. The road width\n",
      "L is deﬁned in meters by calculating the minimum distance between the edges of the local\n",
      "road segment.\n",
      "Then, several seed points are selected manually on the road from one end to the other,\n",
      "and a working area of the regional growth algorithm is limited by a buffer algorithm with\n",
      "the line of seed points as the central axis. When selecting seed points, we only need to\n",
      "ensure that the working area can cover the whole road. As shown in Figure 3a, the seed\n",
      "points are in red and the green border delineates the working area. Then, the operations of\n",
      "regional growth and morphological processing are performed in the working area.\n",
      "Appl. Sci. 2022, 12, x FOR PEER REVIEW   5 of 22 \n",
      " \n",
      "we use regional growth, morphological  closing operations,  and thinning  algorithms  to \n",
      "extract roads. To improve  the accuracy  and efficiency  of road extraction,  the spectral and \n",
      "geometric  characteristics  of roads are considered.  We determine  the implementation  area \n",
      "of the algorithm  according  to the road width, thereby avoiding  the problem of excessive  \n",
      "growth. \n",
      "For the purpose of obtaining  the road width, we first perform regional growth in a \n",
      "local area to obtain one segment with parallel edges. The operation  of regional growth can \n",
      "be described  as follows: first, select a certain pixel on the road as the growing  point. Espe‐\n",
      "cially, for road width determination,  the growing  point should be located on a road seg‐\n",
      "ment without glitches, holes, or interruptions.  Next, compare  the grayscale  of the growing \n",
      "point with its eight neighborhoods.  Finally, combine  the neighbor  pixels meeting the \n",
      "merge threshold  requirements  with growing  point and set it as a new growing  point until \n",
      "no new pixels are combined.  Regional  growth has a simple rule, high calculating  speed \n",
      "and interactivity,  which is suitable for road extraction  requirements.  The road width L is \n",
      "defined in meters by calculating  the minimum  distance between the edges of the local \n",
      "road segment.  \n",
      "Then, several seed points are selected manually  on the road from one end to the \n",
      "other, and a working  area of the regional growth algorithm  is limited by a buffer algo‐\n",
      "rithm with the line of seed points as the central axis. When selecting  seed points, we only \n",
      "need to ensure that the working  area can cover the whole road. As shown in Figure 3a, \n",
      "the seed points are in red and the green border delineates  the working  area. Then, the \n",
      "operations  of regional growth and morphological  processing  are performed  in the work‐\n",
      "ing area. \n",
      "    \n",
      "(a)  (b)  (c)  (d) \n",
      "Figure 3. Road skeleton extraction:  (a) test image, where seed points are shown in red; (b) regional \n",
      "growth result; (c) closing operation  result; (d) thinning  result. \n",
      "To extract one long road at a time, we use the strategy of growing  multiple  seed \n",
      "points simultaneously.  This method has the advantages  of simple rules and a high calcu‐\n",
      "lation speed. The regional growth results are shown in Figure 3b, and a relatively  com‐\n",
      "plete road can be extracted.  However,  there are burrs and rough areas on the edges of the \n",
      "generated  roads due to the pixel‐by‐pixel processing  of the regional growth algorithm;  \n",
      "there are also many holes inside the road, which are caused by incomplete  growth due to \n",
      "shadows,  occlusions,  etc. Therefore,  further optimization  of the regional growth results is \n",
      "needed. To minimize  the errors of holes and fragmented  growth, this method adopts the \n",
      "morphological  closing operation,  which is most commonly  used for trimming  edges and \n",
      "filling gaps, to optimize  the initial extraction  results. The morphological  closing operation  \n",
      "[12] is defined as follows: \n",
      "A∙Bൌ ሺA⊕B ሻ⊖B,  (1) \n",
      "Figure 3. Road skeleton extraction: ( a) test image, where seed points are shown in red; ( b) regional\n",
      "growth result; ( c) closing operation result; ( d) thinning result.\n",
      "To extract one long road at a time, we use the strategy of growing multiple seed points\n",
      "simultaneously. This method has the advantages of simple rules and a high calculation\n",
      "speed. The regional growth results are shown in Figure 3b, and a relatively complete road\n",
      "can be extracted. However, there are burrs and rough areas on the edges of the generated\n",
      "roads due to the pixel-by-pixel processing of the regional growth algorithm; there are\n",
      "also many holes inside the road, which are caused by incomplete growth due to shadows,\n",
      "occlusions, etc. Therefore, further optimization of the regional growth results is needed. To\n",
      "minimize the errors of holes and fragmented growth, this method adopts the morphological\n",
      "closing operation, which is most commonly used for trimming edges and ﬁlling gaps, to\n",
      "optimize the initial extraction results. The morphological closing operation [ 12] is deﬁned\n",
      "as follows:\n",
      "A\u0001B=(B)\tB, (1)\n",
      "In the formula, the operator \u0001denotes the closing operation, A represents the binary\n",
      "image, and B represents the structural elements. The symbolsand\tdenote the expansion\n",
      "and corrosion operators, respectively. Figure 3c shows the result of the morphological\n",
      "closing operation.\n",
      "The road skeleton is the basis for generating the road network. Therefore, we use\n",
      "a morphological thinning algorithm to skeletonize the previously extracted roads. By\n",
      "progressively stripping the road pixels, only a single-pixel-width skeleton is left to maintain\n",
      "connectivity. The Zhang–Suen parallel algorithm [ 47] is used for reﬁnement. Because of its\n",
      "low number of iterations and high speed, it has superior thinning ability at intersectionsAppl. Sci. 2022 ,12, 4705 6 of 21\n",
      "and is widely used in the ﬁeld of image thinning [ 3,12,48]. By searching the points of roads\n",
      "one by one in the binary image obtained by the morphological closing operation, where\n",
      "a road point has a gray value of 255 and a background point has a value of 0, boundary\n",
      "points that do not affect road connectivity are deleted. The operation has two steps:\n",
      "Step 1: Find the boundary points that meet the following requirements:\n",
      "2\u0014N1\u00146, (2)\n",
      "T(p1)=1, (3)\n",
      "In the formulas, N1means the number of road points among eight neighborhoods, and\n",
      "T(p1)is the number of transitions of gray values from 0 to 255 among eight neighborhoods.\n",
      "Step 2: if a boundary point found meets requirements of either group A or group B, its\n",
      "gray value will be changed to 0, which means it is deleted. In the formulas, pimeans the\n",
      "gray value of the i-th pixel. The arrangement of pixels is shown in Figure 4.\n",
      "Appl. Sci. 2022, 12, x FOR PEER REVIEW   6 of 22 \n",
      " \n",
      "In the formula,  the operator  ∙ denotes the closing operation,  A represents  the binary \n",
      "image, and B represents  the structural  elements.  The symbols ⊕ and ⊖ denote the ex‐\n",
      "pansion and corrosion  operators,  respectively.  Figure 3c shows the result of the morpho‐\n",
      "logical closing operation.  \n",
      "The road skeleton is the basis for generating  the road network.  Therefore,  we use a \n",
      "morphological  thinning  algorithm  to skeletonize  the previously  extracted  roads. By pro‐\n",
      "gressively  stripping  the road pixels, only a single‐pixel‐width skeleton is left to maintain  \n",
      "connectivity.  The Zhang–Suen  parallel algorithm  [47] is used for refinement.  Because of \n",
      "its low number of iterations  and high speed, it has superior thinning  ability at intersec‐\n",
      "tions and is widely used in the field of image thinning  [3,12,48].  By searching  the points \n",
      "of roads one by one in the binary image obtained  by the morphological  closing operation,  \n",
      "where a road point has a gray value of 255 and a background  point has a value of 0, \n",
      "boundary  points that do not affect road connectivity  are deleted. The operation  has two \n",
      "steps: \n",
      "Step 1: Find the boundary  points that meet the following  requirements:  \n",
      "2൑N ଵ൑6,  (2) \n",
      "Tሺpଵሻൌ1,  (3) \n",
      "In the formulas,  Nଵmeans the number of road points among eight neighborhoods,  \n",
      "and Tሺpଵሻ is the number of transitions  of gray values from 0 to 255 among eight neigh‐\n",
      "borhoods.  \n",
      "Step 2: if a boundary  point found meets requirements  of either group A or group B, \n",
      "its gray value will be changed to 0, which means it is deleted. In the formulas,  p୧ means \n",
      "the gray value of the i‐th pixel. The arrangement  of pixels is shown in Figure 4. \n",
      "Group A: \n",
      "pଶ∙pସ∙p଺ൌ0,  (4) \n",
      "pସ∙p଺∙p଼ൌ0,  (5) \n",
      "Group B: \n",
      "pଶ∙pସ∙p଼ൌ0,  (6) \n",
      "pଶ∙p଺∙p଼ൌ0,  (7) \n",
      " \n",
      "Figure 4. Arrangement  of pixels in the operation  of morphological  thinning.  \n",
      "Figure 4. Arrangement of pixels in the operation of morphological thinning.\n",
      "Group A:\n",
      "p2\u0001p4\u0001p6=0, (4)\n",
      "p4\u0001p6\u0001p8=0, (5)\n",
      "Group B:\n",
      "p2\u0001p4\u0001p8=0, (6)\n",
      "p2\u0001p6\u0001p8=0, (7)\n",
      "Repeat the two steps until no more points should be deleted. The result is shown in\n",
      "Figure 3d.\n",
      "Road Skeleton Vectorization and Optimization\n",
      "After the processing described above, a road skeleton with a single-pixel width is\n",
      "acquired. Compared with the raster map, the vector map is more convenient to save and\n",
      "modify and easier to update and use in data analysis. Therefore, it is necessary to perform\n",
      "vectorization processing on the road skeleton obtained. We use a tracking algorithm to\n",
      "vectorize the road skeleton. To obtain a relatively accurate centerline vector of the road and\n",
      "address the problems of overly short segments and incomplete roads, various operations\n",
      "are adopted, which include simplifying road segments, deleting overly short segments,\n",
      "modifying endpoints, and connecting roads. The strategies and algorithms used in each\n",
      "part are introduced below.\n",
      "(1) Road vectorization to generate road segmentsAppl. Sci. 2022 ,12, 4705 7 of 21\n",
      "This method uses a road tracking algorithm to vectorize the raster road; the results\n",
      "of this algorithm are accurate, and its calculation speed is high. This operation can be\n",
      "described as the following steps:\n",
      "Step 1: Search for a starting road point (with a gray value of 255, pixel p1in Figure 4)\n",
      "in the whole image from top left to bottom right and record its coordinate, then record this\n",
      "point as the ﬁrst point of a road and change its gray value to 0, that is, delete it.\n",
      "Step 2: search for another road point in a certain order (see Figure 5) in the eight\n",
      "neighborhoods of the ﬁrst point and set it as a new starting point. Then record this point as\n",
      "the latter point of the road and also change its gray value to 0. Repeat this step until no\n",
      "more road points are to be recorded.\n",
      "Appl. Sci. 2022, 12, x FOR PEER REVIEW   7 of 22 \n",
      " \n",
      "Repeat the two steps until no more points should be deleted. The result is shown in \n",
      "Figure 3d. \n",
      "Road Skeleton  Vectorization  and Optimization  \n",
      "After the processing  described  above, a road skeleton with a single‐pixel width is \n",
      "acquired.  Compared  with the raster map, the vector map is more convenient  to save and \n",
      "modify and easier to update and use in data analysis.  Therefore,  it is necessary  to perform \n",
      "vectorization  processing  on the road skeleton obtained.  We use a tracking algorithm  to \n",
      "vectorize  the road skeleton.  To obtain a relatively  accurate centerline  vector of the road \n",
      "and address the problems  of overly short segments  and incomplete  roads, various opera‐\n",
      "tions are adopted,  which include simplifying  road segments,  deleting overly short seg‐\n",
      "ments, modifying  endpoints,  and connecting  roads. The strategies  and algorithms  used in \n",
      "each part are introduced  below. \n",
      "(1) Road vectorization  to generate  road segments  \n",
      "This method uses a road tracking algorithm  to vectorize  the raster road; the results \n",
      "of this algorithm  are accurate,  and its calculation  speed is high. This operation  can be de‐\n",
      "scribed as the following  steps: \n",
      "Step 1: Search for a starting road point (with a gray value of 255, pixel pଵ in Figure \n",
      "4) in the whole image from top left to bottom right and record its coordinate,  then record \n",
      "this point as the first point of a road and change its gray value to 0, that is, delete it. \n",
      "Step 2: search for another road point in a certain order (see Figure 5) in the eight \n",
      "neighborhoods  of the first point and set it as a new starting point. Then record this point \n",
      "as the latter point of the road and also change its gray value to 0. Repeat this step until no \n",
      "more road points are to be recorded.  \n",
      " \n",
      "Figure 5. Arrangement  order of pixels in the road tracking algorithm.  \n",
      "Step 3: As the first point may not be the actual starting point of a road, set the first \n",
      "point as the starting point again and continue  searching  in the same order as step 2, but \n",
      "record the new point found as a previous  point instead, indicating  that the road being \n",
      "tracked has a certain direction.  \n",
      "Step 4: When no new points needs to be recorded  in step 3, an entire road has been \n",
      "recorded  and all the points of it have been deleted. Start again from step 1 to track a new \n",
      "road and stop tracking when no road points can be found in the entire image. \n",
      "The simplifying  algorithm  and the operation  of deleting overly short segments  are \n",
      "applied after the tracking algorithm.  We use the Douglas–Peucker  algorithm  [49] for sim‐\n",
      "plification;  it has translation  and rotation invariance  and removes  unnecessary  points by \n",
      "setting a distance threshold  to perform data compression.  This algorithm  mainly includes \n",
      "these following  steps: \n",
      "Figure 5. Arrangement order of pixels in the road tracking algorithm.\n",
      "Step 3: As the ﬁrst point may not be the actual starting point of a road, set the ﬁrst\n",
      "point as the starting point again and continue searching in the same order as step 2, but\n",
      "record the new point found as a previous point instead, indicating that the road being\n",
      "tracked has a certain direction.\n",
      "Step 4: When no new points needs to be recorded in step 3, an entire road has been\n",
      "recorded and all the points of it have been deleted. Start again from step 1 to track a new\n",
      "road and stop tracking when no road points can be found in the entire image.\n",
      "The simplifying algorithm and the operation of deleting overly short segments are\n",
      "applied after the tracking algorithm. We use the Douglas–Peucker algorithm [ 49] for\n",
      "simpliﬁcation; it has translation and rotation invariance and removes unnecessary points\n",
      "by setting a distance threshold to perform data compression. This algorithm mainly\n",
      "includes these following steps:\n",
      "Step 1: Connect the two endpoints A and B and record them as reserved points.\n",
      "Calculate the Euclidean distance from each point between the two ends to line AB, and\n",
      "ﬁnd the point P with the greatest distance. If there is no point between the two ends, all\n",
      "points have been simpliﬁed, and the simplifying operation exits.\n",
      "Step 2: Compare the distance from point P to line ABand the simplifying threshold T,\n",
      "delete point P when the distance is less than T, otherwise, keep it.\n",
      "Step 3: Repeat the steps above with points A and P as endpoints and with B and P as\n",
      "endpoints, respectively.\n",
      "The simplifying threshold T is deﬁned as:\n",
      "T=k\u0002resolution, (8)\n",
      "where resolution is the image resolution in meters; k is a simplifying parameter, which is\n",
      "set to 3.0.\n",
      "Figure 6 shows the principle of the Douglas–Peucker algorithm: the endpoints are\n",
      "determined and deleted in turn, and ﬁnally a ﬁve-point polyline is used to describe theAppl. Sci. 2022 ,12, 4705 8 of 21\n",
      "initial seven-point polyline. Blue points are the ones being judged and then kept, and the\n",
      "red points are the ones judged and then deleted. Red dashed lines show the distances\n",
      "between the judged point P and the connecting line of endpoints A and B (blue line) and\n",
      "black dashed lines are the segments to be deleted in each step.\n",
      "Appl. Sci. 2022, 12, x FOR PEER REVIEW   8 of 22 \n",
      " \n",
      "Step 1: Connect the two endpoints  A and B and record them as reserved  points. Cal‐\n",
      "culate the Euclidean  distance from each point between the two ends to line ABതതതത, and find \n",
      "the point P with the greatest distance.  If there is no point between the two ends, all points \n",
      "have been simplified,  and the simplifying  operation  exits. \n",
      "Step 2: Compare  the distance from point P to line ABതതതത and the simplifying  threshold  \n",
      "T, delete point P when the distance is less than T, otherwise,  keep it. \n",
      "Step 3: Repeat the steps above with points A and P as endpoints  and with B and P as \n",
      "endpoints,  respectively.  \n",
      "The simplifying  threshold  T is defined as: \n",
      "Tൌ kൈr e s o l u t i o n ,  (8) \n",
      "where resolution  is the image resolution  in meters; k is a simplifying  parameter,  which is \n",
      "set to 3.0. \n",
      "Figure 6 shows the principle  of the Douglas–Peucker  algorithm:  the endpoints  are \n",
      "determined  and deleted in turn, and finally a five‐point polyline is used to describe the \n",
      "initial seven‐point polyline.  Blue points are the ones being judged and then kept, and the \n",
      "red points are the ones judged and then deleted. Red dashed lines show the distances  \n",
      "between the judged point P and the connecting  line of endpoints  A and B (blue line) and \n",
      "black dashed lines are the segments  to be deleted in each step. \n",
      "   \n",
      "(a)  (b)  (c) \n",
      "   \n",
      "(d)  (e)  \n",
      "Figure 6. Principle  of the Douglas–Peucker  algorithm:  (a) initial polyline;  (b–d) judgment  and pro‐\n",
      "cessing step by step; (e) simplifed  result. \n",
      "During the road tracking process, due to the influence  of intersections,  shadows,  etc., \n",
      "on the road, some overly short segments  may be generated,  as shown in Figure 7a,b. These \n",
      "overly short segments  are not part of the road and are likely to cause interference  in the \n",
      "subsequent  connection  algorithms.  Therefore,  we delete them by setting a length thresh‐\n",
      "old. The length threshold  d୲ is defined as: \n",
      "d୲ൌ1 0ൈL ,  (9) \n",
      "This definition  is based on the characteristics  of long and thin roads; according  to \n",
      "experience,  the aspect ratio of roads is generally  greater than 10. \n",
      "The road vectorization  algorithm  is shown in Algorithm  1: \n",
      "Algorithm  1: Road Vectorization  \n",
      "Input:  image Input Image  \n",
      "neighbor  Neighborhood  Search Order  \n",
      "dt Length Threshold  \n",
      "Output: Lines Segments  \n",
      "Figure 6. Principle of the Douglas–Peucker algorithm: ( a) initial polyline; ( b–d) judgment and\n",
      "processing step by step; ( e) simplifed result.\n",
      "During the road tracking process, due to the inﬂuence of intersections, shadows, etc.,\n",
      "on the road, some overly short segments may be generated, as shown in Figure 7a,b. These\n",
      "overly short segments are not part of the road and are likely to cause interference in the\n",
      "subsequent connection algorithms. Therefore, we delete them by setting a length threshold.\n",
      "The length threshold d tis deﬁned as:\n",
      "dt=10\u0002L, (9)\n",
      "Appl. Sci. 2022, 12, x FOR PEER REVIEW   9 of 22 \n",
      " \n",
      "1:  function  FindLines(image,  Lines, neighbor , dt) \n",
      "2:  BEGIN \n",
      "3:           while (findFirstPoint(image,  firstPt))  \n",
      "4:           BEGIN \n",
      "5:                  line.push_back(firstPt);   \n",
      "6:                  currPt = firstPt;  \n",
      "7:                  while (findNextPoint(neighbor,  image, currPt, nextPt))  \n",
      "8:                  BEGIN \n",
      "9:                         line.push_back(nextPt);   \n",
      "10:                          currPt = nextPt; \n",
      "11:                   END \n",
      "12:                   currPt = firstPt;  \n",
      "13:                   while (findNextPoint(neighbor,  image, currPt, nextPt)) \n",
      "14:                   BEGIN \n",
      "15:                          line.push_front(nextPt);   \n",
      "16:                          currPt = nextPt;  \n",
      "17:                   END \n",
      "18:                   if (line.length()  > dt) \n",
      "19:                   BEGIN \n",
      "20:                        line.simplify(T);  \n",
      "21:                          Lines.push_back(line);  \n",
      "22:                   END \n",
      "23:            END \n",
      "24:  END  \n",
      "Among the functions  above, findFirstPoint  (image, firstPt) is used to globally search \n",
      "for the first point firstPt, findNextPoint  (neighbor,  image, currPt, nextPt) is used to search \n",
      "for the next point nextPt, and line.simplify  (T) is used to simplify the line segment accord‐\n",
      "ing to the threshold  T. \n",
      "Figure 7 shows the result of vectorization  processing:  (a) is a local road thinning  re‐\n",
      "sult, (b) shows the road segments  obtained  by the tracking algorithm,  (c) is the result of \n",
      "deleting overly short segments,  and (d) is the result of the simplifying  algorithm.  \n",
      "    \n",
      "(a)  (b)  (c)  (d) \n",
      "Figure 7. Vectorization  to obtain segments:  (a) result after thinning;  (b) result after applying  the \n",
      "tracking algorithm;  (c) result after deleting overly short segments;  (d) result after simplification.  \n",
      "(2) Road segment optimization  and connection  \n",
      "The vectorization  operation  mentioned  above generates  a series of separate road seg‐\n",
      "ments, and the offset problem occurring  at the head and tail segments  during the thinning  \n",
      "process cannot be solved, as shown in Figure 8a,c. Therefore,  the road segments  generated  \n",
      "need to be further optimized  and connected  to form the entire road. \n",
      "Figure 7. Vectorization to obtain segments: ( a) result after thinning; ( b) result after applying the\n",
      "tracking algorithm; ( c) result after deleting overly short segments; ( d) result after simpliﬁcation.\n",
      "This deﬁnition is based on the characteristics of long and thin roads; according to\n",
      "experience, the aspect ratio of roads is generally greater than 10.\n",
      "The road vectorization algorithm is shown in Algorithm 1:Appl. Sci. 2022 ,12, 4705 9 of 21\n",
      "Algorithm 1: Road Vectorization\n",
      "Input: image Input Image\n",
      "neighbor Neighborhood Search Order\n",
      "dtLength Threshold\n",
      "Output: Lines Segments\n",
      "1: function FindLines(image, Lines, neighbor, dt)\n",
      "2: BEGIN\n",
      "3: while (ﬁndFirstPoint(image, ﬁrstPt))\n",
      "4: BEGIN\n",
      "5: line.push_back(ﬁrstPt);\n",
      "6: currPt = ﬁrstPt;\n",
      "7: while (ﬁndNextPoint(neighbor, image, currPt, nextPt))\n",
      "8: BEGIN\n",
      "9: line.push_back(nextPt);\n",
      "10: currPt = nextPt;\n",
      "11: END\n",
      "12: currPt = ﬁrstPt;\n",
      "13: while (ﬁndNextPoint(neighbor, image, currPt, nextPt))\n",
      "14: BEGIN\n",
      "15: line.push_front(nextPt);\n",
      "16: currPt = nextPt;\n",
      "17: END\n",
      "18: if(line.length() > dt)\n",
      "19: BEGIN\n",
      "20: line.simplify(T);\n",
      "21: Lines.push_back(line);\n",
      "22: END\n",
      "23: END\n",
      "24: END\n",
      "Among the functions above, ﬁndFirstPoint (image, ﬁrstPt) is used to globally search\n",
      "for the ﬁrst point ﬁrstPt, ﬁndNextPoint (neighbor, image, currPt, nextPt) is used to search\n",
      "for the next point nextPt, and line.simplify (T) is used to simplify the line segment according\n",
      "to the threshold T.\n",
      "Figure 7 shows the result of vectorization processing: (a) is a local road thinning result,\n",
      "(b) shows the road segments obtained by the tracking algorithm, (c) is the result of deleting\n",
      "overly short segments, and (d) is the result of the simplifying algorithm.\n",
      "(2) Road segment optimization and connection\n",
      "The vectorization operation mentioned above generates a series of separate road\n",
      "segments, and the offset problem occurring at the head and tail segments during the\n",
      "thinning process cannot be solved, as shown in Figure 8a,c. Therefore, the road segments\n",
      "generated need to be further optimized and connected to form the entire road.\n",
      "Considering the smoothness of roads, branches and offsets are eliminated by judging\n",
      "the direction of the endpoints and the neighboring nodes. The principle of endpoint\n",
      "modiﬁcation is shown in Figure 9, where the dotted line indicates the direction of vector\n",
      "BA. For each segment, two consecutive nodes Aand Badjacent to endpoint Care selected,\n",
      "and\u0012is the angle between the vectors BAand AC. Endpoint Cis retained when \u0012<10\u000e.\n",
      "Otherwise, it is deleted along with the overly short segment, shown in red in Figure 9.\n",
      "Figure 8b,d show the results of road segment optimization.Appl. Sci. 2022 ,12, 4705 10 of 21\n",
      "Appl. Sci. 2022, 12, x FOR PEER REVIEW   10 of 22 \n",
      " \n",
      "Considering  the smoothness  of roads, branches  and offsets are eliminated  by judging \n",
      "the direction  of the endpoints  and the neighboring  nodes. The principle  of endpoint  mod‐\n",
      "ification is shown in Figure 9, where the dotted line indicates  the direction  of vector BAതതതത. \n",
      "For each segment,  two consecutive  nodes A and B adjacent to endpoint  C are selected,  \n",
      "and θ is the angle between the vectors BAതതതത and ACതതതത. Endpoint  C is retained when θ൏\n",
      "10°. Otherwise,  it is deleted along with the overly short segment,  shown in red in Figure \n",
      "9. Figure 8b,d show the results of road segment optimization.  \n",
      "    \n",
      "(a)  (b)  (c)  (d) \n",
      "Figure 8. Optimizing  segments:  (a,c) show vectors with offset problems;  (b,d) are the respective  \n",
      "results of endpoint  modification.  \n",
      " \n",
      "Figure 9. Endpoint  modification.  \n",
      "To connect the optimized  segments  into an entire road, a road connection  algorithm  \n",
      "is designed.  The connection  rules include rules governing  the distance between endpoints  \n",
      "and the angle between vectors. If two segments  meet the connection  rules, the endpoints  \n",
      "are connected.  \n",
      "The rule governing  the distance between endpoints  is defined as follows: first, the \n",
      "distance between two endpoints  must be the shortest globally.  Second, considering  the \n",
      "continuous  and uninterrupted  characteristics  of a road, we again choose d୲ as the dis‐\n",
      "tance threshold  for connection.  The distance between two connected  endpoints  should be \n",
      "less than d୲. \n",
      "Furthermore,  considering  the smoothness  of a specific road, the rule governing  the \n",
      "angle between vectors is defined as shown in Figure 10. Black solid lines show the seg‐\n",
      "ments, black dotted lines show their extensions,  and blue dashed lines indicate the hori‐\n",
      "zontal‐right direction.  If the endpoints  A and B of the road segment meet the distance rule, \n",
      "then we find the nodes A′ and B′ adjacent to the endpoints  A and B. θଵ and θଶ ሺθଵ,θଶ∈\n",
      "ሺ0°, 180° ሿሻ are the angles between the vectors A’Aሬሬሬሬሬሬ⃗ and B’Bሬሬሬሬሬሬ⃗ and the horizontal ‐right di‐\n",
      "rection, respectively,  and θൌ |θଵെθ ଶ|. If θ ൐ 90° , the endpoints  are connected.  The con‐\n",
      "nection line is shown in red. \n",
      "Figure 8. Optimizing segments: ( a,c) show vectors with offset problems; ( b,d) are the respective\n",
      "results of endpoint modiﬁcation.\n",
      "Appl. Sci. 2022, 12, x FOR PEER REVIEW   10 of 22 \n",
      " \n",
      "Considering  the smoothness  of roads, branches  and offsets are eliminated  by judging \n",
      "the direction  of the endpoints  and the neighboring  nodes. The principle  of endpoint  mod‐\n",
      "ification is shown in Figure 9, where the dotted line indicates  the direction  of vector BAതതതത. \n",
      "For each segment,  two consecutive  nodes A and B adjacent to endpoint  C are selected,  \n",
      "and θ is the angle between the vectors BAതതതത and ACതതതത. Endpoint  C is retained when θ൏\n",
      "10°. Otherwise,  it is deleted along with the overly short segment,  shown in red in Figure \n",
      "9. Figure 8b,d show the results of road segment optimization.  \n",
      "    \n",
      "(a)  (b)  (c)  (d) \n",
      "Figure 8. Optimizing  segments:  (a,c) show vectors with offset problems;  (b,d) are the respective  \n",
      "results of endpoint  modification.  \n",
      " \n",
      "Figure 9. Endpoint  modification.  \n",
      "To connect the optimized  segments  into an entire road, a road connection  algorithm  \n",
      "is designed.  The connection  rules include rules governing  the distance between endpoints  \n",
      "and the angle between vectors. If two segments  meet the connection  rules, the endpoints  \n",
      "are connected.  \n",
      "The rule governing  the distance between endpoints  is defined as follows: first, the \n",
      "distance between two endpoints  must be the shortest globally.  Second, considering  the \n",
      "continuous  and uninterrupted  characteristics  of a road, we again choose d୲ as the dis‐\n",
      "tance threshold  for connection.  The distance between two connected  endpoints  should be \n",
      "less than d୲. \n",
      "Furthermore,  considering  the smoothness  of a specific road, the rule governing  the \n",
      "angle between vectors is defined as shown in Figure 10. Black solid lines show the seg‐\n",
      "ments, black dotted lines show their extensions,  and blue dashed lines indicate the hori‐\n",
      "zontal‐right direction.  If the endpoints  A and B of the road segment meet the distance rule, \n",
      "then we find the nodes A′ and B′ adjacent to the endpoints  A and B. θଵ and θଶ ሺθଵ,θଶ∈\n",
      "ሺ0°, 180° ሿሻ are the angles between the vectors A’Aሬሬሬሬሬሬ⃗ and B’Bሬሬሬሬሬሬ⃗ and the horizontal ‐right di‐\n",
      "rection, respectively,  and θൌ |θଵെθ ଶ|. If θ ൐ 90° , the endpoints  are connected.  The con‐\n",
      "nection line is shown in red. \n",
      "Figure 9. Endpoint modiﬁcation.\n",
      "To connect the optimized segments into an entire road, a road connection algorithm is\n",
      "designed. The connection rules include rules governing the distance between endpoints\n",
      "and the angle between vectors. If two segments meet the connection rules, the endpoints\n",
      "are connected.\n",
      "The rule governing the distance between endpoints is deﬁned as follows: ﬁrst, the\n",
      "distance between two endpoints must be the shortest globally. Second, considering the\n",
      "continuous and uninterrupted characteristics of a road, we again choose dtas the distance\n",
      "threshold for connection. The distance between two connected endpoints should be less\n",
      "than d t.\n",
      "Furthermore, considering the smoothness of a specific road, the rule governing the angle\n",
      "between vectors is defined as shown in Figure 10. Black solid lines show the segments,\n",
      "black dotted lines show their extensions, and blue dashed lines indicate the horizontal-right\n",
      "direction. If the endpoints A and B of the road segment meet the distance rule, then we find\n",
      "the nodes A0and B0adjacent to the endpoints A and B. \u00121and\u00122(\u00121,\u001222(0\u000e, 180\u000e])are the\n",
      "angles between the vectors!\n",
      "A0Aand!\n",
      "B0Band the horizontal-right direction, respectively , and\n",
      "\u0012=j\u00121\u0000\u00122j. If\u0012>90\u000e, the endpoints are connected. The connection line is shown in red.\n",
      "Appl. Sci. 2022, 12, x FOR PEER REVIEW   11 of 22 \n",
      " \n",
      "  \n",
      "(a)  (b) \n",
      "Figure 10. Angle rule between vectors: (a) the two vectors are on the same side of the horizontal  \n",
      "line; (b) the two vectors are on opposite  sides of the horizontal  line. \n",
      "The pseudocode  of the road connection  algorithm  in Algorithm  2: \n",
      "Algorithm  2: Road Connection  \n",
      "Input:  Lines Segments  after Tracking  Algorithm  \n",
      "Output: Lines Roads after Connection  Algorithm  \n",
      "1:   function  LinkLines(&Lines)  \n",
      "2:   BEGIN \n",
      "3:          Ver = FindVertex(Lines);  \n",
      "4:          Ver_theta  = FindVertexAzimuth(Lines);  \n",
      "5:          for i ← 0 to Ver.size()  \n",
      "6:          BEGIN  \n",
      "7:                min = 9999.0; \n",
      "8:                if (isinConnectedPt(i,  ConnectedPt))  continue;  \n",
      "9:                for j ← (i + 1) to Ver.size()  \n",
      "10:                BEGIN \n",
      "11:   if (isinSameLine(Ver(i),  Ver(j)))  continue;  \n",
      "12:                   temp_distance  = cal_Distance(Ver(i),Ver(j));  \n",
      "13:                   if (temp_distance  < min) \n",
      "14:                           BEGIN \n",
      "15:                               min = temp_distance;  \n",
      "16:                               flag = j; \n",
      "17:                          END \n",
      "18:   END \n",
      "19:   if ((abs(Ver_theta(i) ‐Ver_theta(flag))  > 90) AND (min < dt))  \n",
      "20:   BEGIN  \n",
      "21:                   Temp_line.pushback(Ver(i));  \n",
      "22:                   Temp_line.pushback(Ver(flag));  \n",
      "23:                   L1 = findLinefromVer(Ver(i));  \n",
      "24:                   L2 = findLinefromVer(Ver(flag));  \n",
      "25:                   JudgeConnectOrder(L1,L2,&Line1,Temp_line,&Line2);  \n",
      "26:                   Line1 = Line1.combine(Temp_line);  \n",
      "27:                   Line1 = Line1.combine(Line2);  \n",
      "Figure 10. Angle rule between vectors: ( a) the two vectors are on the same side of the horizontal line;\n",
      "(b) the two vectors are on opposite sides of the horizontal line.Appl. Sci. 2022 ,12, 4705 11 of 21\n",
      "The pseudocode of the road connection algorithm in Algorithm 2:\n",
      "Algorithm 2: Road Connection\n",
      "Input: Lines Segments after Tracking Algorithm\n",
      "Output: Lines Roads after Connection Algorithm\n",
      "1: function LinkLines(&Lines)\n",
      "2: BEGIN\n",
      "3: Ver = FindVertex(Lines);\n",
      "4: Ver_theta = FindVertexAzimuth(Lines);\n",
      "5: for i  0 to Ver.size()\n",
      "6: BEGIN\n",
      "7: min = 9999.0;\n",
      "8: if (isinConnectedPt(i, ConnectedPt)) continue;\n",
      "9: for j  (i + 1) to Ver.size()\n",
      "10: BEGIN\n",
      "11: if (isinSameLine(Ver(i), Ver(j))) continue;\n",
      "12: temp_distance = cal_Distance(Ver(i),Ver(j));\n",
      "13: if (temp_distance < min)\n",
      "14: BEGIN\n",
      "15: min = temp_distance;\n",
      "16: ﬂag = j;\n",
      "17: END\n",
      "18: END\n",
      "19: if ((abs(Ver_theta(i)-Ver_theta(ﬂag)) > 90) AND (min < dt))\n",
      "20: BEGIN\n",
      "21: Temp_line.pushback(Ver(i));\n",
      "22: Temp_line.pushback(Ver(ﬂag));\n",
      "23: L1 = ﬁndLinefromVer(Ver(i));\n",
      "24: L2 = ﬁndLinefromVer(Ver(ﬂag));\n",
      "25: JudgeConnectOrder(L1,L2,&Line1,Temp_line,&Line2);\n",
      "26: Line1 = Line1.combine(Temp_line);\n",
      "27: Line1 = Line1.combine(Line2);\n",
      "28: ChangeLine(L1,Line1, &Lines);\n",
      "29: deleteLine(L2,&Lines);\n",
      "30: ConnectedPt.pushback(ﬂag);\n",
      "31: END\n",
      "32: END\n",
      "33: END\n",
      "FindVertex (Lines) is used to ﬁnd the endpoint of the segment. FindVertexAzimuth\n",
      "(Lines) is used to calculate the angle between the end vector and the horizontal-right\n",
      "direction. ConnectedPt stores the number of connected vertices. The isinConnectedPt(i,\n",
      "connectedpt) function is used to determine whether the currently numbered vertex i has\n",
      "been connected; isinSameLine(Ver(i), Ver(j)) is used to judge whether the two vertices are\n",
      "on the same road segment; cal_Distance(Ver(i), Ver(j)) is used to calculate the Euclidean\n",
      "distance between two vertices; ﬁndLinefromVer(Ver(i)) is used to ﬁnd the corresponding\n",
      "road segment through vertex Ver(i); and JudgeConnectOrder(L1, L2, & Line1, Temp_line, &\n",
      "Line2) is used to determine the connection order. The combine function connects segments.\n",
      "ChangeLine(L1, Line1, & Lines) is used to reset lines and deleteLine(L2, & Lines) is used to\n",
      "delete redundant segments.\n",
      "Figure 11 shows the connection results of single roads, in which (a) and (c) depict two\n",
      "roads to be connected. The middle part of each road is divided into two sections due to\n",
      "tree occlusion, and this cannot be repaired by the morphological closing algorithm; (b) and\n",
      "(d) show the road connection results.Appl. Sci. 2022 ,12, 4705 12 of 21\n",
      "Appl. Sci. 2022, 12, x FOR PEER REVIEW   12 of 22 \n",
      " \n",
      "28:                   ChangeLine(L1,Line1,  &Lines); \n",
      "29:                   deleteLine(L2,&Lines);  \n",
      "30:                   ConnectedPt.pushback(flag);  \n",
      "31:                END \n",
      "32:   END \n",
      "33:   END \n",
      "FindVertex  (Lines) is used to find the endpoint  of the segment.  FindVertexAzimuth  \n",
      "(Lines) is used to calculate  the angle between the end vector and the horizontal ‐right di‐\n",
      "rection. ConnectedPt  stores the number of connected  vertices. The isinConnectedPt(i,  con‐\n",
      "nectedpt)  function is used to determine  whether the currently  numbered  vertex i has been \n",
      "connected;  isinSameLine(Ver(i),  Ver(j)) is used to judge whether the two vertices are on \n",
      "the same road segment;  cal_Distance(Ver(i),  Ver(j)) is used to calculate  the Euclidean  dis‐\n",
      "tance between two vertices; findLinefromVer(Ver(i))  is used to find the corresponding  \n",
      "road segment through vertex Ver(i); and JudgeConnectOrder(L1,  L2, & Line1, Temp_line,  \n",
      "& Line2) is used to determine  the connection  order. The combine  function connects  seg‐\n",
      "ments. ChangeLine(L1,  Line1, & Lines) is used to reset lines and deleteLine(L2,  & Lines) \n",
      "is used to delete redundant  segments.  \n",
      "Figure 11 shows the connection  results of single roads, in which (a) and (c) depict \n",
      "two roads to be connected.  The middle part of each road is divided into two sections due \n",
      "to tree occlusion,  and this cannot be repaired by the morphological  closing algorithm;  (b) \n",
      "and (d) show the road connection  results. \n",
      "      \n",
      "(a)  (b)  (c)  (d) \n",
      "Figure 11. The road connection  results: (a,c) depict the roads to be connected;  (b,d) are the respective  \n",
      "road connection  results. \n",
      "2.2.2.  Road Network  Generation  \n",
      "The road network is acquired  through road intersection  connections  and buffer gen‐\n",
      "eration. Road intersections  have three main shape types: ‘T’, ‘Y’, and ‘+’. ‘T’‐ and ‘Y’‐\n",
      "shaped intersections  are similar, as shown in Figure 12a, in which directions  1 and 2 with \n",
      "larger included  angles can be regarded  as the same single road, recorded  as road A. Di‐\n",
      "rection 3 is regarded  as a single road and is recorded  as road B. At this time, the extension  \n",
      "line (dashed line) of road B is constructed  to a certain length, which is equal to d୲, to de‐\n",
      "termine whether there is an intersection  with road A. If an intersection  exists (blue point), \n",
      "it is connected  to road B (blue line). \n",
      "There are two situations  for an intersection  with a ‘+’ shape. One is shown in Figure \n",
      "12b. The four directions  intersect at the same point, and the angles between directions  1 \n",
      "and 3 and between 2 and 4 are both close to 180°; this can be taken to indicate two single \n",
      "roads that should be extracted  separately,  and the roads A and B obtained  have a unique \n",
      "intersection  (blue point). No additional  processing  is required.  The other situation  is rela‐\n",
      "tively complicated,  as shown in Figure 12c. Directions  1 and 3 can be regarded  as the same \n",
      "road for extraction  to obtain road A, while directions  2 and 4 can only be extracted  as \n",
      "Figure 11. The road connection results: ( a,c) depict the roads to be connected; ( b,d) are the respective\n",
      "road connection results.\n",
      "2.2.2. Road Network Generation\n",
      "The road network is acquired through road intersection connections and buffer gener-\n",
      "ation. Road intersections have three main shape types: ‘T’, ‘Y’, and ‘+’. ‘T’- and ‘Y’-shaped\n",
      "intersections are similar, as shown in Figure 12a, in which directions 1 and 2 with larger\n",
      "included angles can be regarded as the same single road, recorded as road A. Direction 3 is\n",
      "regarded as a single road and is recorded as road B. At this time, the extension line (dashed\n",
      "line) of road B is constructed to a certain length, which is equal to dt, to determine whether\n",
      "there is an intersection with road A. If an intersection exists (blue point), it is connected to\n",
      "road B (blue line).\n",
      "Appl. Sci. 2022, 12, x FOR PEER REVIEW   13 of 22 \n",
      " \n",
      "individual  roads that generate  roads B and C. In this case, the intersection  can be regarded  \n",
      "as the superposition  of two ‘T’‐shaped roads formed by road A with either B or C. Then, \n",
      "processing  can be performed  according  to the ‘T’‐shaped intersection  connection  strategy.  \n",
      "Figure 13 shows the connection  results of the intersection  connection  algorithm.  It \n",
      "can be seen that the strategy described  above has a good connection  effect. \n",
      "    \n",
      "(a)  (b)  (c) \n",
      "Figure 12. Description  of road intersections:  (a) ‘T’‐ or ‘Y’‐shaped intersection;  (b) simple situation  \n",
      "of ‘+’‐shaped intersection;  (c) multiple  situations  of ‘+’‐shaped intersections.  \n",
      "    \n",
      "(a)  (c)  (e) \n",
      "    \n",
      "(b)  (d)  (f) \n",
      "Figure 13. Intersection  connection  results: (a,c,e) depict the road intersections  before the connection  \n",
      "is performed  and (b,d,f) are the results of the intersection  connection  algorithm.  \n",
      "The intersection  connection  algorithm  is shown in Algorithm  3: \n",
      "Algorithm  3: Intersection  Connection  \n",
      "Input:  Lines Roads \n",
      "Output: addLines  Newly added connection  roads \n",
      "1:   function  addJunctionLines(Lines,  addLines)  \n",
      "2:   BEGIN \n",
      "3:          Ver = FindVertex(Lines);  \n",
      "4:          Ver_theta  = FindVertexAzimuth(Lines);  \n",
      "5:          for i ← 0 to Ver.size()  \n",
      "Figure 12. Description of road intersections: ( a) ‘T’- or ‘Y’-shaped intersection; ( b) simple situation of\n",
      "‘+’-shaped intersection; ( c) multiple situations of ‘+’-shaped intersections.\n",
      "There are two situations for an intersection with a ‘+’ shape. One is shown in Figure 12b.\n",
      "The four directions intersect at the same point, and the angles between directions 1 and\n",
      "3 and between 2 and 4 are both close to 180\u000e; this can be taken to indicate two single\n",
      "roads that should be extracted separately, and the roads A and B obtained have a unique\n",
      "intersection (blue point). No additional processing is required. The other situation is\n",
      "relatively complicated, as shown in Figure 12c. Directions 1 and 3 can be regarded as the\n",
      "same road for extraction to obtain road A, while directions 2 and 4 can only be extracted as\n",
      "individual roads that generate roads B and C. In this case, the intersection can be regarded\n",
      "as the superposition of two ‘T’-shaped roads formed by road A with either B or C. Then,\n",
      "processing can be performed according to the ‘T’-shaped intersection connection strategy.\n",
      "Figure 13 shows the connection results of the intersection connection algorithm. It can\n",
      "be seen that the strategy described above has a good connection effect.Appl. Sci. 2022 ,12, 4705 13 of 21\n",
      "Appl. Sci. 2022, 12, x FOR PEER REVIEW   13 of 22 \n",
      " \n",
      "individual  roads that generate  roads B and C. In this case, the intersection  can be regarded  \n",
      "as the superposition  of two ‘T’‐shaped roads formed by road A with either B or C. Then, \n",
      "processing  can be performed  according  to the ‘T’‐shaped intersection  connection  strategy.  \n",
      "Figure 13 shows the connection  results of the intersection  connection  algorithm.  It \n",
      "can be seen that the strategy described  above has a good connection  effect. \n",
      "    \n",
      "(a)  (b)  (c) \n",
      "Figure 12. Description  of road intersections:  (a) ‘T’‐ or ‘Y’‐shaped intersection;  (b) simple situation  \n",
      "of ‘+’‐shaped intersection;  (c) multiple  situations  of ‘+’‐shaped intersections.  \n",
      "    \n",
      "(a)  (c)  (e) \n",
      "    \n",
      "(b)  (d)  (f) \n",
      "Figure 13. Intersection  connection  results: (a,c,e) depict the road intersections  before the connection  \n",
      "is performed  and (b,d,f) are the results of the intersection  connection  algorithm.  \n",
      "The intersection  connection  algorithm  is shown in Algorithm  3: \n",
      "Algorithm  3: Intersection  Connection  \n",
      "Input:  Lines Roads \n",
      "Output: addLines  Newly added connection  roads \n",
      "1:   function  addJunctionLines(Lines,  addLines)  \n",
      "2:   BEGIN \n",
      "3:          Ver = FindVertex(Lines);  \n",
      "4:          Ver_theta  = FindVertexAzimuth(Lines);  \n",
      "5:          for i ← 0 to Ver.size()  \n",
      "Figure 13. Intersection connection results: ( a,c,e) depict the road intersections before the connection\n",
      "is performed and ( b,d,f) are the results of the intersection connection algorithm.\n",
      "The intersection connection algorithm is shown in Algorithm 3:\n",
      "Algorithm 3: Intersection Connection\n",
      "Input: Lines Roads\n",
      "Output: addLines Newly added connection roads\n",
      "1: function addJunctionLines(Lines, addLines)\n",
      "2: BEGIN\n",
      "3: Ver = FindVertex(Lines);\n",
      "4: Ver_theta = FindVertexAzimuth(Lines);\n",
      "5: for i  0 to Ver.size()\n",
      "6: BEGIN\n",
      "7: extendline.push_back(Ver(i));\n",
      "8: extendpoint = cal_coordinate(Ver(i), Ver_theta(i), dt);\n",
      "9: extendline.push_back(extendpoint);\n",
      "10: for j  0 to Lines.size()\n",
      "11: BEGIN\n",
      "12: if (intersects(Lines(j), extendline))\n",
      "13: BEGIN\n",
      "14: intersect_geo = intersection(Lines(j), extendline);\n",
      "15: intersect = intersect_geo- > asPoint();\n",
      "16: if (Ver(i) 6=intersect)\n",
      "17: BEGIN\n",
      "18: temp_polyline.push_back(Ver(i));\n",
      "19: temp_polyline.push_back(intersect);\n",
      "20: addLines.pushback(temp_polyline);\n",
      "21: END\n",
      "22: END\n",
      "23: END\n",
      "24: END\n",
      "25: ENDAppl. Sci. 2022 ,12, 4705 14 of 21\n",
      "The function cal_coordinate(Ver(i), Ver_theta(i), dt) is used to calculate the vertex of\n",
      "the extension line of limited length; intersects(Lines (j), extendline) is used to determine\n",
      "whether there is an intersection; and intersection(Lines (j), extendline) is used to calculate\n",
      "the intersection.\n",
      "After multiple roads are extracted separately and all road endpoints are judged and\n",
      "connected, a buffer zone with a radius that is one-half the width of the road is generated to\n",
      "obtain the road network (see Figure 14).\n",
      "Appl. Sci. 2022, 12, x FOR PEER REVIEW   14 of 22 \n",
      " \n",
      "6:          BEGIN  \n",
      "7:             extendline.push_back(Ver(i));  \n",
      "8:             extendpoint  = cal_coordinate(Ver(i),  Ver_theta(i),  dt); \n",
      "9:             extendline.push_back(extendpoint);  \n",
      "10:                for j ← 0 to Lines.size()  \n",
      "11:                BEGIN \n",
      "12:                   if (intersects(Lines(j),  extendline))  \n",
      "13:                   BEGIN \n",
      "14:                      intersect_geo  = intersection(Lines(j),  extendline);  \n",
      "15:                      intersect = intersect_geo ‐ > asPoint();  \n",
      "16:                      if (Ver(i)≠intersect)  \n",
      "17:                         BEGIN \n",
      "18:   temp_polyline.push_back(Ver(i));   \n",
      "19:                            temp_polyline.push_back(intersect);  \n",
      "20:                            addLines.pushback(temp_polyline);  \n",
      "21:                         END \n",
      "22:                    END \n",
      "23:                END \n",
      "24:   END \n",
      "25:   END \n",
      "The function cal_coordinate(Ver(i),  Ver_theta(i),  dt) is used to calculate  the vertex of \n",
      "the extension  line of limited length; intersects(Lines  (j), extendline)  is used to determine  \n",
      "whether there is an intersection;  and intersection(Lines  (j), extendline)  is used to calculate  \n",
      "the intersection.  \n",
      "After multiple  roads are extracted  separately  and all road endpoints  are judged and \n",
      "connected,  a buffer zone with a radius that is one‐half the width of the road is generated  \n",
      "to obtain the road network (see Figure 14). \n",
      " \n",
      "Figure 14. A road network obtained  through a buffer zone. \n",
      "   \n",
      "Figure 14. A road network obtained through a buffer zone.\n",
      "2.2.3. Evaluation of the Extraction Results\n",
      "In this method, the precision, accuracy, recall, and intersection over union (IoU) [ 50,51]\n",
      "are used to evaluate extraction performance, see Equations (10)–(14). In the formulas, TP ,\n",
      "FP , FN, and TN are the number of true positives, false positives, false negatives, and true\n",
      "negatives for the road predictions.\n",
      "Precision =TP\n",
      "TP+FP, (10)\n",
      "Accuracy =TP+TN\n",
      "TP+TN+FP+FN, (11)\n",
      "Recall =TP\n",
      "TP+FN, (12)\n",
      "The IoU is the ratio of the intersection and the union of two bounding boxes. If A is\n",
      "the resulting extracted road and B is the corresponding ground truth, then the IoU is:\n",
      "IoU=A\\B\n",
      "A[B, (13)\n",
      "TP , FP , and FN are used to re-state the formula as follows:\n",
      "If the extraction result is identical to the ground truth, then IoU is equal to 1.\n",
      "IoU=TP\n",
      "TP+FN+FP, (14)\n",
      "3. Results\n",
      "3.1. Parameter Settings\n",
      "In this study, only three parameters need to be set in the workﬂow of road network\n",
      "extraction: the image resolution, regional growth threshold, and simplifying threshold.\n",
      "The image resolution can be obtained directly from the data source. Since the difference\n",
      "in gray value between urban buildings and roads is small, the regional growth threshold\n",
      "needs to be determined by the user according to the number of buildings shown in theAppl. Sci. 2022 ,12, 4705 15 of 21\n",
      "image. After a large number of experiments, the regional growth threshold was set to\n",
      "40 in suburbs with fewer buildings, and 20 in cities with more buildings. As shown in\n",
      "Equation (8), the simplifying parameter khas a signiﬁcant impact on the simpliﬁed result.\n",
      "Based on experiments, the simplifying parameter k is set to 3. Figure 15 shows the results\n",
      "of different simplifying thresholds for an image with the same type as Data 2; (a) is the\n",
      "tracking result, and (b–d) are the simpliﬁed results when the simplifying parameter k is 1,\n",
      "3, and 5, respectively. When k = 1, redundant points remain in the circle. When k = 5, some\n",
      "key points are missing, and the centerline in the circle deviates from the road. When k = 3,\n",
      "the result ﬁts the road centerline well. Hence, the simplifying parameter k is set to 3 here.\n",
      "Appl. Sci. 2022, 12, x FOR PEER REVIEW   15 of 22 \n",
      " \n",
      "2.2.3. Evaluation  of the Extraction  Results \n",
      "In this method, the precision,  accuracy,  recall, and intersection  over union (IoU) \n",
      "[50,51] are used to evaluate extraction  performance,  see Equations  (10)–(14).  In the formu‐\n",
      "las, TP, FP, FN, and TN are the number of true positives,  false positives,  false negatives,  \n",
      "and true negatives  for the road predictions.  \n",
      "Precision ൌ୘୔\n",
      "୘୔ା୊୔ ,  (10) \n",
      "Accurac yൌ୘୔ା୘୒\n",
      "୘୔ା୘୒ା୊୔ା୊୒ ,  (11) \n",
      "Recall ൌ୘୔\n",
      "୘୔ା୊୒ ,  (12) \n",
      "The IoU is the ratio of the intersection  and the union of two bounding  boxes. If A is \n",
      "the resulting  extracted  road and B is the corresponding  ground truth, then the IoU is: \n",
      "IoU ൌ୅∩୆\n",
      "୅∪୆ ,  (13) \n",
      "TP, FP, and FN are used to re‐state the formula as follows: \n",
      "If the extraction  result is identical  to the ground truth, then IoU is equal to 1. \n",
      "IoU ൌ୘୔\n",
      "୘୔ା୊୒ା୊୔ ,  (14) \n",
      "3. Results \n",
      "3.1. Parameter  Settings \n",
      "In this study, only three parameters  need to be set in the workflow  of road network \n",
      "extraction:  the image resolution,  regional growth threshold,  and simplifying  threshold.  \n",
      "The image resolution  can be obtained  directly from the data source. Since the difference  \n",
      "in gray value between urban buildings  and roads is small, the regional growth threshold  \n",
      "needs to be determined  by the user according  to the number of buildings  shown in the \n",
      "image. After a large number of experiments,  the regional growth threshold  was set to 40 \n",
      "in suburbs with fewer buildings,  and 20 in cities with more buildings.  As shown in Equa‐\n",
      "tion (8), the simplifying  parameter  k has a significant  impact on the simplified  result. \n",
      "Based on experiments,  the simplifying  parameter  k is set to 3. Figure 15 shows the results \n",
      "of different  simplifying  thresholds  for an image with the same type as Data 2; (a) is the \n",
      "tracking result, and (b–d) are the simplified  results when the simplifying  parameter  k is \n",
      "1, 3, and 5, respectively.  When k = 1, redundant  points remain in the circle. When k = 5, \n",
      "some key points are missing, and the centerline  in the circle deviates from the road. When \n",
      "k = 3, the result fits the road centerline  well. Hence, the simplifying  parameter  k is set to 3 \n",
      "here. \n",
      "    \n",
      "(a)  (b)  (c)  (d) \n",
      "Figure 15. Results of different  simplifying  thresholds:  (a) tracking result; (b) result when k = 1; (c) \n",
      "result when k = 3; (d) result when k = 5. \n",
      "Figure 15. Results of different simplifying thresholds: ( a) tracking result; ( b) result when k = 1;\n",
      "(c) result when k = 3; ( d) result when k = 5.\n",
      "3.2. Road Extraction Results for the Four Datasets\n",
      "Figure 16 shows a comparison between the road extraction result and the ground truth,\n",
      "where (a,e) correspond to Data 1, (b,f) correspond to Data 2, (c,g) correspond to Data 3, and\n",
      "(d,h) correspond to Data 4; (a–d) are extracted by the developed method to generate road\n",
      "networks, and (e–h) are ground truths that are manually labeled. The evaluation results\n",
      "are shown in Table 1. These extraction results have high accuracy and precision of Data\n",
      "1–3, showing that the method applies well in extracting roads from images at different\n",
      "resolutions. Among the four data, Data 1 was located in a rural area, with the simplest road\n",
      "conditions and the highest resolution. Therefore, the extraction result is very similar to\n",
      "the ground truth (Figure 16a,e). Data 2 is more complex than Data 1, but each road is also\n",
      "extracted relatively completely (Figure 16b,f). Because of many details in Data 2, the recall\n",
      "rate and IoU in the road extraction results are relatively low (Table 1). Data 3 is located in\n",
      "the city, and the road situation is the most complex. The results of Data 3 have the highest\n",
      "precision but the lowest accuracy (Table 1), meaning that most parts of roads are identiﬁed\n",
      "accurately but the differences between roads and surroundings are not distinguished well.\n",
      "Data 4 is located in a rural residential area with multiple roads of varying lengths and\n",
      "widths. The proposed method can extract the roads in the image very well by extracting the\n",
      "roads by grading, that is, ﬁrstly extracting the wide roads, and then extracting the narrow\n",
      "roads (Figure 16e,h). However, the width of the road at the same level in the image is also\n",
      "different, which makes the proposed method obtain limited extraction precision and IoU\n",
      "(Table 1). Moreover, all road intersections in the four data are correctly connected.Appl. Sci. 2022 ,12, 4705 16 of 21\n",
      "Appl. Sci. 2022, 12, x FOR PEER REVIEW   16 of 22 \n",
      " \n",
      "3.2. Road Extraction  Results for the Four Datasets \n",
      "Figure 16 shows a comparison  between the road extraction  result and the ground \n",
      "truth, where (a,e) correspond  to Data 1, (b,f) correspond  to Data 2, (c,g) correspond  to Data \n",
      "3, and (d,h) correspond  to Data 4; (a–d) are extracted  by the developed  method to generate  \n",
      "road networks,  and (e–h) are ground truths that are manually  labeled. The evaluation  re‐\n",
      "sults are shown in Table 1. These extraction  results have high accuracy  and precision  of \n",
      "Data 1–3, showing  that the method applies well in extracting  roads from images at differ‐\n",
      "ent resolutions.  Among the four data, Data 1 was located in a rural area, with the simplest \n",
      "road conditions  and the highest resolution.  Therefore,  the extraction  result is very similar \n",
      "to the ground truth (Figure 16a,e). Data 2 is more complex than Data 1, but each road is \n",
      "also extracted  relatively  completely  (Figure 16b,f). Because of many details in Data 2, the \n",
      "recall rate and IoU in the road extraction  results are relatively  low (Table 1). Data 3 is \n",
      "located in the city, and the road situation  is the most complex.  The results of Data 3 have \n",
      "the highest precision  but the lowest accuracy  (Table 1), meaning  that most parts of roads \n",
      "are identified  accurately  but the differences  between roads and surroundings  are not dis‐\n",
      "tinguished  well. Data 4 is located in a rural residential  area with multiple roads of varying \n",
      "lengths and widths. The proposed  method can extract the roads in the image very well by \n",
      "extracting  the roads by grading, that is, firstly extracting  the wide roads, and then extract‐\n",
      "ing the narrow roads (Figure 16e,h). However,  the width of the road at the same level in \n",
      "the image is also different,  which makes the proposed  method obtain limited extraction  \n",
      "precision  and IoU (Table 1). Moreover,  all road intersections  in the four data are correctly  \n",
      "connected.  \n",
      "     \n",
      "(a)  (b)  (c)  (d) \n",
      "      \n",
      "(e)  (f)  (g)  (h) \n",
      "Figure 16. Comparison  between the road extraction  results and the ground truth: (a–d) are the road \n",
      "extraction  results and (e–h) are the ground truth. \n",
      "Table 1. Evaluation  table of road extraction.  \n",
      "Data  Precision   Accuracy   Recall  IoU \n",
      "Data 1  88.54%  99.70%  88.97%  0.80 \n",
      "Data 2  87.08%  98.13%  77.06%  0.69 \n",
      "Data 3  98.10%  88.31%  88.68%  0.87 \n",
      "Data 4  80.70%  96.80%  81.56%  0.68 \n",
      "Figure 16. Comparison between the road extraction results and the ground truth: ( a–d) are the road\n",
      "extraction results and ( e–h) are the ground truth.\n",
      "Table 1. Evaluation table of road extraction.\n",
      "Data Precision Accuracy Recall IoU\n",
      "Data 1 88.54% 99.70% 88.97% 0.80\n",
      "Data 2 87.08% 98.13% 77.06% 0.69\n",
      "Data 3 98.10% 88.31% 88.68% 0.87\n",
      "Data 4 80.70% 96.80% 81.56% 0.68\n",
      "Ten roads in Data 2 were randomly selected for statistical analysis of extraction time\n",
      "(Table 2). Roads with a length of more than 4000 pixels can be extracted at one time in 7 s.\n",
      "In terms of extraction efﬁciency, the longer the road, the shorter the road extraction time\n",
      "per 1000 pixels. The average extraction time of roads per 1000 pixels is 1.81 s.\n",
      "Table 2. Evaluation table of road extraction efﬁciency.\n",
      "No. Road Length (pixel) Time (s) Time per 1000 pixels (s)\n",
      "1 2040 4.125 2.02\n",
      "2 2126 4.739 2.23\n",
      "3 2378 4.64 1.95\n",
      "4 2415 5.688 2.36\n",
      "5 2770 6.776 2.45\n",
      "6 4056 4.536 1.12\n",
      "7 4064 6.43 1.58\n",
      "8 4115 5.83 1.42\n",
      "9 4121 6.095 1.48\n",
      "10 4188 6.486 1.55\n",
      "Mean - - 1.81\n",
      "3.3. Comparison with Other Existing Methods\n",
      "We make comparisons with Gu’s road extraction method [ 41] in this study. Gu’s\n",
      "method is implemented in the following way: ﬁrst determine a seed point to obtain the\n",
      "initial contour through region growth (use the same threshold as the developed method),\n",
      "and then use the GVF–Snake method to perform iterative optimization to obtain the road\n",
      "boundary, where the number of iterations is 40. Gu’s method has many iterations, so itAppl. Sci. 2022 ,12, 4705 17 of 21\n",
      "is only suitable for road extraction in local areas. Three local typical roads were selected\n",
      "for comparison. For each road segment about the length of 300 pixels, the method takes\n",
      "around 2 s. This is less efﬁcient than the developed method (Table 2). Figure 16 shows the\n",
      "extraction results of the two methods.\n",
      "For Sample 1 and Sample 2, the recall of Gu’s method is higher than ours. However,\n",
      "the precision and IoU of the developed method are all better than those of Gu’s method\n",
      "(Table 3). As shown in Figure 17a,d, Gu’s method could distinguish wild country and\n",
      "artiﬁcial structures exposed to the sun, including roads, which led to high recall. However,\n",
      "it was easily inﬂuenced by the effect of ‘different objects with similar spectra’. Objects such\n",
      "as concrete ground and bare soil were extracted as roads, leading to low precision, accuracy,\n",
      "and IoU (Table 3). Because of the limited working area, the developed method could avoid\n",
      "over-extraction (Figure 17b,e) and thus reach better precision, accuracy, and IoU (Table 3).\n",
      "When shadows occlude some roads, such as those in Sample 3, Gu’s method could hardly\n",
      "identify these roads (Figure 16g), signiﬁcantly reducing its recall and IoU (Table 3). In\n",
      "the developed method, roads were thinned to centerlines and regenerated with a certain\n",
      "width, which weakened the inﬂuence of shadows (Figure 17h) and achieved an acceptable\n",
      "result (Table 3). Furthermore, the series of operations including morphological operation\n",
      "and vector simpliﬁcation in the developed method eliminated the burs and made the road\n",
      "edge smoother.\n",
      "Table 3. Performance of local road extraction by Gu’s method and the proposed method.\n",
      "Sample 1 Sample 2 Sample 3\n",
      "Gu’s Method Proposed Method Gu’s MethodProposed\n",
      "MethodGu’s MethodProposed\n",
      "Method\n",
      "Precision 0.64 0.96 0.57 0.79 0.76 0.84\n",
      "Accuracy 0.94 0.97 0.96 0.98 0.94 0.98\n",
      "Recall 0.88 0.70 0.99 0.94 0.25 0.86\n",
      "IoU 0.59 0.67 0.57 0.75 0.23 0.73\n",
      "Appl. Sci. 2022, 12, x FOR PEER REVIEW   18 of 22 \n",
      " \n",
      " \n",
      "Figure 17. Comparison  with Gu’s method: (a,d,g) are the results of Gu’s method; (b,e,h) are the \n",
      "results of the proposed  method; (c,f,i) are the ground truth. \n",
      "Table 3. Performance  of local road extraction  by Gu’s method and the proposed  method. \n",
      "  Sample 1  Sample 2  Sample 3 \n",
      "  Gu’s Method Proposed  Method Gu’s Method Proposed  Method Gu’s Method Proposed  Method \n",
      "Precision  0.64  0.96  0.57  0.79  0.76  0.84 \n",
      "Accuracy  0.94  0.97  0.96  0.98  0.94  0.98 \n",
      "Recall  0.88  0.70  0.99  0.94  0.25  0.86 \n",
      "IoU  0.59  0.67  0.57  0.75  0.23  0.73 \n",
      "4. Discussion  \n",
      "After applying  the method in four different  tested images, the results all have high \n",
      "accuracy  (Table 1). For simple roads in rural areas, such as Data 1, the proposed  method \n",
      "has high precision,  accuracy,  recall, and IoU (Table 1). For suburban  roads such as Data 2, \n",
      "the proposed  method can accurately  extract the main roads and obtain high precision  and \n",
      "accuracy  (Table 1). For urban areas such as Data 3 and rural residential  areas in Data 4, \n",
      "the method is robust to shadows  and still achieves  good extraction  results in areas with \n",
      "buildings  (Table 1, Figure 16). In terms of extraction  efficiency,  the longer the road, the \n",
      "shorter the road extraction  time per 1000 pixels (Table 2). This is mainly because most of \n",
      "the time consumption  in this method focuses on image processing  at the grid level, while \n",
      "the algorithm  at the vector level is very fast. The longer the road, the higher the extraction  \n",
      "efficiency—therefore,  it is very suitable for large‐area road extraction,  especially  for large \n",
      "labeled data required  for deep learning.  \n",
      "The recall rate and IoU in the road extraction  results for Data 2 and Data 4 are rela‐\n",
      "tively low (Table 1). For Data 2, the reason is mainly that shorter roads were mistakenly  \n",
      "deleted during vectorization  and subsequent  optimization.  Moreover,  this method mainly \n",
      "Figure 17. Comparison with Gu’s method: ( a,d,g) are the results of Gu’s method; ( b,e,h) are the\n",
      "results of the proposed method; ( c,f,i) are the ground truth.Appl. Sci. 2022 ,12, 4705 18 of 21\n",
      "4. Discussion\n",
      "After applying the method in four different tested images, the results all have high\n",
      "accuracy (Table 1). For simple roads in rural areas, such as Data 1, the proposed method\n",
      "has high precision, accuracy, recall, and IoU (Table 1). For suburban roads such as Data 2,\n",
      "the proposed method can accurately extract the main roads and obtain high precision and\n",
      "accuracy (Table 1). For urban areas such as Data 3 and rural residential areas in Data 4,\n",
      "the method is robust to shadows and still achieves good extraction results in areas with\n",
      "buildings (Table 1, Figure 16). In terms of extraction efﬁciency, the longer the road, the\n",
      "shorter the road extraction time per 1000 pixels (Table 2). This is mainly because most of\n",
      "the time consumption in this method focuses on image processing at the grid level, while\n",
      "the algorithm at the vector level is very fast. The longer the road, the higher the extraction\n",
      "efﬁciency—therefore, it is very suitable for large-area road extraction, especially for large\n",
      "labeled data required for deep learning.\n",
      "The recall rate and IoU in the road extraction results for Data 2 and Data 4 are relatively\n",
      "low (Table 1). For Data 2, the reason is mainly that shorter roads were mistakenly deleted\n",
      "during vectorization and subsequent optimization. Moreover, this method mainly focuses\n",
      "on the optimization of road centerlines and the generation of road networks. Hence, as\n",
      "shown in Figure 18, auxiliary roads are not considered here. This is why the recall and IoU\n",
      "are relatively low. For Data 4, the width of the same level road is different, which is the\n",
      "main reason for the limited precision and IoU.\n",
      "Appl. Sci. 2022, 12, x FOR PEER REVIEW   19 of 22 \n",
      " \n",
      "focuses on the optimization  of road centerlines  and the generation  of road networks.  \n",
      "Hence, as shown in Figure 18, auxiliary  roads are not considered  here. This is why the \n",
      "recall and IoU are relatively  low. For Data 4, the width of the same level road is different,  \n",
      "which is the main reason for the limited precision  and IoU. \n",
      "   \n",
      "(a)  (b)  (c) \n",
      "Figure 18. Comparison  of intersections  with auxiliary  roads: (a) original image; (b) extraction  re‐\n",
      "sult; (c) ground truth. \n",
      "We selected four tested images with different  resolutions  and obtained  them from \n",
      "different  remote‐sensing platforms.  The experiments  on rural, suburban,  urban, and rural \n",
      "residential  area images proved the universality  of the proposed  method. Compared  with \n",
      "the existing Gu’s methods,  the proposed  method also showed better performance  (Figure \n",
      "17, Table 3). This can provide an accurate and complete  way to extract roads at different  \n",
      "scales, especially  beneficial  for the remote‐sensing images of some areas with shadows  \n",
      "and intersections.  In addition,  because of the wide universality,  the proposed  method has \n",
      "great potential  in data labeling.  We realize that the proposed  method reduces the shadow \n",
      "effect; however,  this proposed  method still does not eliminate  it (Figure 17b,e,h). The \n",
      "method also ignores auxiliary  roads and is less effective when road widths are incon‐\n",
      "sistent, so further research efforts will be focused on refining the developed  method. \n",
      "5. Conclusions  \n",
      "In this study, a full‐flow processing  strategy including  all steps from road extraction  \n",
      "to road network generation  is proposed,  aiming at improving  the efficiency  of road net‐\n",
      "work extraction  and data labeling.  To this end, a new framework  with two main steps—\n",
      "single‐road extraction  and road network generation—is  constructed  by integrating  vari‐\n",
      "ous algorithms.  Among these algorithms,  the implementation  of new algorithms  such as \n",
      "endpoint  modifications,  road connections,  and road network generation  algorithms  was \n",
      "crucial for establishing  the whole road‐extraction  workflow.  Four high‐resolution  images \n",
      "with different  terrains and resolutions  are used to validate the proposed  framework,  and \n",
      "the results show that the strategy greatly improves  the road network extraction  effect. It \n",
      "has good accuracy  and universality  and can be used to perform road extraction  and road \n",
      "network update with high‐resolution  remote‐sensing images. The evaluations  of the ex‐\n",
      "traction results of the four images show that the road precision  and IoU both reach a high \n",
      "level. At the same time, the developed  method has better precision  and faster speed than \n",
      "the semi‐automatic  method. Additionally,  because of the wide universality,  the proposed  \n",
      "method has great potential  in data labeling.  Lastly, experiments  also show that this strat‐\n",
      "egy does not consider  some special road types and may miss extraction  of shorter roads, \n",
      "which deserve more attention  in future work. \n",
      "Author Contributions:  Conceptualization,  W.C. and K.Y.; methodology,  K.Y. and W.C.; software,  \n",
      "K.Y. and S.S.; investigation,  K.Y., Y.L. (Yuanjin  Li), and Y.L. (Yu Liu); writing, K.Y. and W.C.; writ‐\n",
      "ing—review  and editing, W.C., K.Y., and M.G.; funding acquisition,  W.C. All authors have read and \n",
      "agreed to the published  version of the manuscript.  \n",
      "Figure 18. Comparison of intersections with auxiliary roads: ( a) original image; ( b) extraction result;\n",
      "(c) ground truth.\n",
      "We selected four tested images with different resolutions and obtained them from\n",
      "different remote-sensing platforms. The experiments on rural, suburban, urban, and rural\n",
      "residential area images proved the universality of the proposed method. Compared with the\n",
      "existing Gu’s methods, the proposed method also showed better performance (Figure 17,\n",
      "Table 3). This can provide an accurate and complete way to extract roads at different\n",
      "scales, especially beneﬁcial for the remote-sensing images of some areas with shadows and\n",
      "intersections. In addition, because of the wide universality, the proposed method has great\n",
      "potential in data labeling. We realize that the proposed method reduces the shadow effect;\n",
      "however, this proposed method still does not eliminate it (Figure 17b,e,h). The method also\n",
      "ignores auxiliary roads and is less effective when road widths are inconsistent, so further\n",
      "research efforts will be focused on reﬁning the developed method.\n",
      "5. Conclusions\n",
      "In this study, a full-ﬂow processing strategy including all steps from road extrac-\n",
      "tion to road network generation is proposed, aiming at improving the efﬁciency of road\n",
      "network extraction and data labeling. To this end, a new framework with two main\n",
      "steps—single-road extraction and road network generation—is constructed by integrating\n",
      "various algorithms. Among these algorithms, the implementation of new algorithms such\n",
      "as endpoint modiﬁcations, road connections, and road network generation algorithms was\n",
      "crucial for establishing the whole road-extraction workﬂow. Four high-resolution imagesAppl. Sci. 2022 ,12, 4705 19 of 21\n",
      "with different terrains and resolutions are used to validate the proposed framework, and\n",
      "the results show that the strategy greatly improves the road network extraction effect.\n",
      "It has good accuracy and universality and can be used to perform road extraction and\n",
      "road network update with high-resolution remote-sensing images. The evaluations of the\n",
      "extraction results of the four images show that the road precision and IoU both reach a high\n",
      "level. At the same time, the developed method has better precision and faster speed than\n",
      "the semi-automatic method. Additionally, because of the wide universality, the proposed\n",
      "method has great potential in data labeling. Lastly, experiments also show that this strategy\n",
      "does not consider some special road types and may miss extraction of shorter roads, which\n",
      "deserve more attention in future work.\n",
      "Author Contributions: Conceptualization, W.C. and K.Y.; methodology, K.Y. and W.C.; software,\n",
      "K.Y. and S.S.; investigation, K.Y., Y.L. (Yuanjin Li) and Y.L. (Yu Liu); writing, K.Y. and W.C.; writing—\n",
      "review and editing, W.C., K.Y. and M.G.; funding acquisition, W.C. All authors have read and agreed\n",
      "to the published version of the manuscript.\n",
      "Funding: Research presented in this paper was funded by National Natural Science Foundation of\n",
      "China (NSFC: U2033216) and the Foundation of Key Laboratory of Aerospace Information Application\n",
      "of CETC: No. SXX19629X060. The authors gratefully acknowledge this support.\n",
      "Institutional Review Board Statement: Not applicable.\n",
      "Informed Consent Statement: Not applicable.\n",
      "Data Availability Statement: The aerial image data used to support the ﬁndings of this study were\n",
      "supplied by the High-Resolution Comprehensive Trafﬁc Remote-Sensing Application program under\n",
      "grant No. 07-Y30B10-9001-14/16, thus cannot be made freely available. Similarly, the GF-2 satellite\n",
      "image data used to support the ﬁndings of this study were provided by the Innovation Fund Project\n",
      "of CETC key laboratory of aerospace information applications under grant No. SXX19629X060, and\n",
      "so cannot be made freely available.\n",
      "Acknowledgments: The authors are grateful to Rongrong Wu, Jia Li, Zhiwei Peng and Haoying Cui,\n",
      "who provided assistance and advice during various stages of this work.\n",
      "Conﬂicts of Interest: The authors declare no conﬂict of interest.\n",
      "References\n",
      "1. Zhu, Q.; Zhang, Y.; Wang, L.; Zhong, Y.; Guan, Q.; Lu, X.; Zhang, L.; Li, D. A global context-aware and batch-independent\n",
      "network for road extraction from VHR satellite imagery. ISPRS J. Photogramm. Remote Sens. 2021 ,175, 353–365. [CrossRef]\n",
      "2. Gao, L.; Song, W.; Dai, J.; Chen, Y. Road extraction from high-resolution remote sensing imagery using reﬁned deep residual\n",
      "convolutional neural network. Remote Sens. 2019 ,11, 552. [CrossRef]\n",
      "3. Yang, X.; Li, X.; Ye, Y.; Lau, R.Y.K.; Zhang, X.; Huang, X. Road detection and centerline extraction via deep recurrent convolutional\n",
      "neural network U-Net. IEEE Trans. Geosci. Remote Sens. 2019 ,57, 7209–7220. [CrossRef]\n",
      "4. Wang, C.; Zourlidou, S.; Golze, J.; Sester, M. Trajectory analysis at intersections for trafﬁc rule identiﬁcation. Geo-Spat. Inf. Sci.\n",
      "2020 ,24, 75–84. [CrossRef]\n",
      "5. Gurung, P . Challenging infrastructural orthodoxies: Political and economic geographies of a Himalayan road. Geoforum 2021 ,120,\n",
      "103–112. [CrossRef]\n",
      "6. Alamgir, M.; Campbell, M.J.; Sloan, S.; Goosem, M.; Clements, G.R.; Mahmoud, M.I.; Laurance, W.F. Economic, socio-political\n",
      "and environmental risks of road development in the tropics. Curr. Biol. 2017 ,27, 1130–1140. [CrossRef]\n",
      "7. Qi, Y.; Chodron Drolma, S.; Zhang, X.; Liang, J.; Jiang, H.; Xu, J.; Ni, T. An investigation of the visual features of urban street\n",
      "vitality using a convolutional neural network. Geo-Spat. Inf. Sci. 2020 ,23, 341–351. [CrossRef]\n",
      "8. Metz, D. Economic beneﬁts of road widening: Discrepancy between outturn and forecast. Transp. Res. Part A Policy Pract. 2021 ,\n",
      "147, 312–319. [CrossRef]\n",
      "9. Chaudhuri, D.; Kushwaha, N.K.; Samal, A. Semi-Automated road detection from high resolution satellite images by directional\n",
      "morphological enhancement and segmentation techniques. IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 2012 ,5, 1538–1544.\n",
      "[CrossRef]\n",
      "10. Wang, J.; Qin, Q.; Gao, Z.; Zhao, J.; Ye, X. A new approach to urban road extraction using high-resolution aerial image. ISPRS Int.\n",
      "J. Geo-Inf. 2016 ,5, 114. [CrossRef]\n",
      "11. Wang, T.; Du, L.; Yi, W.; Hong, J.; Zhang, L.; Zheng, J.; Li, C.; Ma, X.; Zhang, D.; Fang, W.; et al. An adaptive atmospheric\n",
      "correction algorithm for the effective adjacency effect correction of submeter-scale spatial resolution optical satellite images:\n",
      "Application to a WorldView-3 panchromatic image. Remote Sens. Environ. 2021 ,259, 112412. [CrossRef]Appl. Sci. 2022 ,12, 4705 20 of 21\n",
      "12. Máttyus, G.; Luo, W.; Urtasun, R. DeepRoadMapper: Extracting Road Topology from Aerial Images. In Proceedings of the 2017\n",
      "IEEE International Conference on Computer Vision (ICCV), Venice, Italy, 22–29 October 2017; pp. 3458–3466. [CrossRef]\n",
      "13. Zhao, J.Q.; Yang, J.; Li, P .X.; Lu, J.M. Semi-automatic Road Extraction from SAR Images Using EKF and PF. Int. Arch. Photogramm.\n",
      "Remote Sens. Spat. Inf. Sci. 2015 ,XL-7/W4 , 227–230. [CrossRef]\n",
      "14. Guan, H.; Lei, X.; Yu, Y.; Zhao, H.; Peng, D.; Marcato Junior, J.; Li, J. Road marking extraction in UAV imagery using attentive\n",
      "capsule feature pyramid network. Int. J. Appl. Earth Obs. 2022 ,107, 102677. [CrossRef]\n",
      "15. Ekim, B.; Sertel, E.; Kabadayı, M.E. Automatic road extraction from historical maps using deep learning techniques: A regional\n",
      "case study of turkey in a German World War II Map. ISPRS Int. J. Geo-Inf. 2021 ,10, 492. [CrossRef]\n",
      "16. Kuo, C.-L.; Tsai, M.-H. Road characteristics detection based on joint convolutional neural networks with adaptive squares. ISPRS\n",
      "Int. J. Geo-Inf. 2021 ,10, 377. [CrossRef]\n",
      "17. Yang, M.; Yuan, Y.; Liu, G. SDUNet: Road extraction via spatial enhanced and densely connected UNet. Pattern Recognit. 2022 ,\n",
      "126, 108549. [CrossRef]\n",
      "18. Zhang, X.; Ma, W.; Li, C.; Wu, J.; Tang, X.; Jiao, L. Fully Convolutional Network-Based Ensemble Method for Road Extraction\n",
      "from Aerial Images. IEEE Geosci. Remote Sens. Lett. 2020 ,17, 1777–1781. [CrossRef]\n",
      "19. Heipke, C.; Rottensteiner, F. Deep learning for geometric and semantic tasks in photogrammetry and remote sensing. Geo-Spat.\n",
      "Inf. Sci. 2020 ,23, 10–19. [CrossRef]\n",
      "20. Wu, S.; Du, C.; Chen, H.; Xu, Y.; Guo, N.; Jing, N. Road extraction from very high resolution images using weakly labeled\n",
      "openstreetmap centerline. ISPRS Int. J. Geo-Inf. 2019 ,8, 478. [CrossRef]\n",
      "21. Bakhtiari, H.R.R.; Abdollahi, A.; Rezaeian, H. Semi automatic road extraction from digital images. Egypt. J. Remote Sens. Space Sci.\n",
      "2017 ,20, 117–123. [CrossRef]\n",
      "22. Miao, Z.; Wang, B.; Shi, W.; Zhang, H. A semi-automatic method for road centerline extraction from VHR images. IEEE Geosci.\n",
      "Remote Sens. Lett. 2014 ,11, 1856–1860. [CrossRef]\n",
      "23. Nunes, D.M.; Medeiros, N.D.G.; Santos, A.D.P .D. Semi-automatic road network extraction from digital images using object-based\n",
      "classiﬁcation and morphological operators. Bol. Ci ênc. Geod. 2018 ,24, 485–502. [CrossRef]\n",
      "24. Chen, L.; Rottensteiner, F.; Heipke, C. Feature detection and description for image matching: From hand-crafted design to deep\n",
      "learning. Geo-Spat. Inf. Sci. 2020 ,24, 58–74. [CrossRef]\n",
      "25. Yuan, X.; Shi, J.; Gu, L. A review of deep learning methods for semantic segmentation of remote sensing imagery. Expert Syst.\n",
      "Appl. 2021 ,169, 114417. [CrossRef]\n",
      "26. Arya, D.; Maeda, H.; Ghosh, S.K.; Toshniwal, D.; Sekimoto, Y. RDD2020: An annotated image dataset for automatic road damage\n",
      "detection using deep learning. Data Brief 2021 ,36, 107133. [CrossRef]\n",
      "27. Li, P .; He, X.; Qiao, M.; Miao, D.; Cheng, X.; Song, D.; Chen, M.; Li, J.; Zhou, T.; Guo, X.; et al. Exploring multiple crowdsourced\n",
      "data to learn deep convolutional neural networks for road extraction. Int. J. Appl. Earth Obs. 2021 ,104, 102544. [CrossRef]\n",
      "28. Yu, J.; Yu, F.; Zhang, J.; Liu, Z. High resolution remote sensing image road extraction combining region growing and road-unit.\n",
      "Geomat. Inf. Sci. Wuhan Univ. 2013 ,38, 761–764. [CrossRef]\n",
      "29. Li, J.; Wen, Z.Q.; Hu, Y.X.; Liu, Z.D. Road Extraction from Remote Sensing Images Based on Improved Regional Growth. Comput.\n",
      "Eng. Appl. 2016 ,209–213 , 238. [CrossRef]\n",
      "30. Wang, Z.; Yang, L.; Sheng, Y.; Shen, M. Pole-like Objects Segmentation and Multiscale Classiﬁcation-Based Fusion from Mobile\n",
      "Point Clouds in Road Scenes. Remote Sens. 2021 ,13, 4382. [CrossRef]\n",
      "31. Cao, F.; Xu, Y.; Zhu, B.; Li, R. Semi-automatic road centerline extraction from high-resolution remote sensing by image utilizing\n",
      "dynamic programming. J. Geomat. Sci. Technol. 2015 ,32, 615–618. [CrossRef]\n",
      "32. Gruen, A.; Li, H. Road extraction from aerial and satellite images by dynamic programming. ISPRS J. Photogramm. Remote Sens.\n",
      "1995 ,50, 11–20. [CrossRef]\n",
      "33. Lian, R.; Wang, W.; Mustafa, N.; Huang, L. Road Extraction Methods in High-Resolution Remote Sensing Images: A Comprehen-\n",
      "sive Review. IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 2020 ,13, 5489–5507. [CrossRef]\n",
      "34. Ghandorh, H.; Boulila, W.; Masood, S.; Koubaa, A.; Ahmed, F.; Ahmad, J. Semantic Segmentation and Edge Detection—Approach\n",
      "to Road Detection in Very High Resolution Satellite Images. Remote Sens. 2022 ,14, 613. [CrossRef]\n",
      "35. Hormese, J.; Saravanan, C. Automated road extraction from high resolution satellite images. Procedia Technol. 2016 ,24, 1460–1467.\n",
      "[CrossRef]\n",
      "36. Xiao, Y.; Tan, T.S.; Tay, S.C. Utilizing Edge to Extract Roads in High-Resolution Satellite Imagery. In Proceedings of the IEEE\n",
      "International Conference on Image Processing, Genova, Italy, 14 September 2005; Volume 1, p. I-637. [CrossRef]\n",
      "37. Chen, G.; Sui, H.; Tu, J.; Song, Z. Semi-automatic road extraction method from high resolution remote sensing images based on\n",
      "P-N learning. Geomat. Inf. Sci. Wuhan Univ. 2017 ,42, 775–781. [CrossRef]\n",
      "38. Tan, H.; Shen, Z.; Dai, J. Semi-automatic extraction of rural roads under the constraint of combined geometric and texture features.\n",
      "ISPRS Int. J. Geo-Inf. 2021 ,10, 754. [CrossRef]\n",
      "39. Wang, F.; Wang, W.; Xue, B.; Cao, T.; Gao, T. Road extraction from high-spatial-resolution remote sensing image by combining\n",
      "GVF snake with salient features. Acta Geod. Cartogr. Sin. 2017 ,46, 1978–1985. [CrossRef]\n",
      "40. Abdelfattah, R.; Chokmani, K. A Semi Automatic off-roads and Trails Extraction Method from Sentinel-1 Data. In Proceedings\n",
      "of the 2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS), Fort Worth, TX, USA, 23–28 July 2017;\n",
      "pp. 3728–3731. [CrossRef]Appl. Sci. 2022 ,12, 4705 21 of 21\n",
      "41. Gu, D.; Wang, X. Road extraction in remote sensing images based on region growing and GVF-Snake. Comput. Eng. Appl. 2010 ,\n",
      "46, 202–205. [CrossRef]\n",
      "42. Wei, Y.; Wang, Z.; Xu, M. Road structure reﬁned CNN for road extraction in aerial image. IEEE Geosci. Remote. Sens. Lett. 2017 ,14,\n",
      "709–713. [CrossRef]\n",
      "43. Wang, W.; Yang, N.; Zhang, Y.; Wang, F.; Cao, T.; Eklund, P . A review of road extraction from remote sensing images. J. Trafﬁc\n",
      "Transp. Eng. 2016 ,3, 271–282. [CrossRef]\n",
      "44. Wan, Y.; Wang, D.; Xiao, J.; Lai, X.; Xu, J. Automatic determination of seamlines for aerial image mosaicking based on vector\n",
      "roads alone. ISPRS J. Photogramm. Remote Sens. 2013 ,76, 1–10. [CrossRef]\n",
      "45. Mnih, V . Machine Learning for Aerial Image Labeling. Ph.D. Thesis, University of Toronto, Toronto, ON, Canda, 2013.\n",
      "46. Demir, I.; Koperski, K.; Lindenbaum, D.; Pang, G.; Huang, J.; Basu, S.; Hughes, F.; Tuia, D.; Raskar, R. DeepGlobe 2018: A\n",
      "Challenge to Parse the Earth through Satellite Images. In Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and\n",
      "Pattern Recognition Workshops (CVPRW), Salt Lake City, UT, USA, 18–22 June 2018; pp. 172–181.\n",
      "47. Zhang, T.Y.; Suen, C.Y. A fast parallel algorithm for thinning digital patterns. Commun. ACM 1984 ,27, 236–239. [CrossRef]\n",
      "48. Cao, X.; Liu, D.; Ren, X. Detection method for auto guide vehicle’s walking deviation based on image thinning and Hough\n",
      "transform. Meas. Control 2019 ,52, 252–261. [CrossRef]\n",
      "49. Saalfeld, A. Topologically consistent line simpliﬁcation with the Douglas-Peucker algorithm. Cartogr. Geogr. Inf. Sci. 1999 ,26,\n",
      "7–18. [CrossRef]\n",
      "50. Henry, C.; Azimi, S.M.; Merkle, N. Road segmentation in SAR satellite images with deep fully convolutional neural networks.\n",
      "IEEE Geosci. Remote Sens. Lett. 2018 ,15, 1867–1871. [CrossRef]\n",
      "51. Abdollahi, A.; Pradhan, B.; Alamri, A. SC-RoadDeepNet: A New Shape and Connectivity-Preserving Road Extraction Deep\n",
      "Learning-Based Network from Remote Sensing Data. IEEE Trans. Geosci. Remote Sens. 2022 ,60, 5617815. [CrossRef]\n",
      "[' Procedia Technology   24  ( 2016 )  1460 – 1467 Available online at www.sciencedirect.com\\nScienceDirect\\n2212-0173 © 2016 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license \\n(http://creativecommons.org/licenses/by-nc-nd/4.0/).\\nPeer\\n-review under responsibility of the organizing committee of ICETEST – 2015\\ndoi: 10.1016/j.protcy.2016.05.180 \\nInternational Conference on Emerging Trends in Engineering, Science and Technology \\n(ICETEST - 2015) \\nAutomated Road Extraction  From High Resolution Satellite Images  \\nJose Hormesea,*, Dr. C. Saravananb   \\naResearch  Scholar, Dept  of Computer Centre , NIT, Durgapur ,713209,  India  \\nbResearch Guide, Dept of Computer  Centre,  NIT, Durgapur,713209,  India  \\nAbstract  \\nThe importance of road extraction from satellite images arises from the fact that it greatly enhances the efficiency of map \\ngeneration and thus can be a big help in car navigations systems or any emergency (rescue) system that needs instant maps. \\nTherefore, increasing research is being dedicated and focused on the development of efficient methods to extract topographical \\nmeaningful features (like roads) from digital remote sensed im ages. The work deals with extraction of roads from satellite \\nimages. This is a challenging domain compared to extraction fr om aerial images as satellite images are noisy and of lower \\nresolution.  In this method, a Vectorization Approach for the automatic method of road extraction is being used where the image  \\nis segmented to identify the road network regions followed by a d ecision making and continuity procedure to correctly detect the \\nroads and the Vectorization step to identify the line segments or curved segments which represents the road. This method may be  \\nemployed for obtaining information for feeding large -scale Geographic Information System. In the automatic method of road \\nex\\ntraction the extracted roads are converted into road vectors in order to use these vector road maps in GIS. A semi -automated \\nsc\\nheme is used for scenarios where fully automated system fails. A combination of both methods can be devised for a full \\nfledged real business scenario  \\n© 2016 The Authors.  Published by E lsevier Ltd. \\nPeer-review under responsibility of the organizing committee of ICETEST – 2015 . \\nKeywords: Remote Sensed Images; Vectorization, Geographic Information System  \\n1. Introduction  \\nSatellite imagery  consists of photographs of Earth  or other planets made by means of artificial satellites. Satellite \\ni\\nmages have many applications in agriculture, geology, fo restry, biodiversity conservation, regional planning, \\neducation, intelligence  and warfare. Images can be in visible colours and in other spectra.  \\n \\n* Corresponding author. Tel : +919495064249  \\nE-mail address: josehormese@gmail.com  \\n \\n© 2016 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license \\n(http://creativecommons.org/licenses/by-nc-nd/4.0/).\\nPeer\\n-review under responsibility of the organizing committee of ICETEST – 20151461  Jose Hormese and C. Saravanan  /  Procedia Technology   24  ( 2016 )  1460 – 1467 \\nThe rapid  development  of sensor  tech nology  has enabled  higher  resolutions  for the remote  sensed  images namely \\nth\\ne satellite images  (e.g. Quick -Bird images  have  ground  resolution  of 0.6m).  Extensive  investigations  have  been  \\nconducted  in the past two decades  to reliably  extract features  like roads from  these highly  accurate  images. Satellite \\nimagery can be combined with vector or raster data in a GIS  provided that the imagery has been spatially rectified so \\nth\\nat it will properly align with other data sets.  \\nTo identify  roads from  high resolution  remote  sensing  images  and to distinguish  it with other  objects  like \\nbuildings,  rivers  and woods,  the color  information,  usually  in four or more  spectral  bands  can be used as an \\nimportant  feature.  Between  the diversity  of methods,  the decision  to choose  one or other  one depends  on the balance  \\nbetween  speed,  accuracy  and complexity  of the computer  algorithm.  Even  more,  such expected  accuracy  can be \\nrelated  to the quality  of input  data,  in terms of resolution  of the digital  image.  The satellite images  can be \\nrepresented  as raster  images  and digital raster  images  can be class ified as portrayals  of scenes,  with imperfect  \\nrenditions  of objects.  Imperfections in an imag e result  from  the imaging  system,  signal  noise,  atmospheric  scatter  \\nand shadows.  Thus,  the task of identifying  and extracting  the desired  information  or features  from  a raster  image  is \\nbased  on a criteria  developed  to determine  a particular  feature  (based  on its characteristics  within  any raster  image),  \\nwhile  ignoring  the presence  of other  features  and imperfections in the image.  Automatic  methods  of extraction  are \\nmore  complex  than Semi- automatic  methods  of extraction.  Automatic  methods  of extraction  require  ancillary  \\ninformation,  as compared  to Semi -automatic  methods  that extract roads based  on information  from  the input  image.  \\n2. Rela ted Works  \\nSemi automated  and automated road  extraction from satellite images  can save time and labour to a great  degree  \\nin updating road spatial  database.  Various road extraction  approaches  have  been  developed.  An Integrated  Method  \\nfor Urban  Main  Road  Centerline  Extraction  is based  on spatial  class ification  to segment  the images  based  on road \\nand non road groups  [1]. A multistage  framework  was designed  to extract road networks  based  on probabilistic  \\nSVMs and salient  features  [2]. Pixel  based  methods  can be based  to class ify road detection  where Edge  Detecto rs \\ncan be used to extract  potential  road points  [3]. In the  statistical  inference  method, linear  features  are modelled  as a \\nMarkov  point  process  or a geometric -stochastic  model  on the road width,  direction,  intensity  and background  \\nintensity  and maximum  a posteriori  probability  is used to estimate  the road network  [4]. Besides  these, active  \\ncontour  models,  known  as snakes,  are also used in semi  automatic  road extraction  [5].  \\nThe semi automatic  methods  of road extraction  require  some  road seeds  as starting  points,  which  are in general  \\nprovided  by users  and road segments  evolve  under  a certain  model.  Further, these  methods  use black -and-white  \\naerial  photographs  or the panchromatic  band  of high-resolution  satellite images  and therefore  the geometric  \\ncharacteristics  of the roads alo ne play a critical  role. Various road extraction  algo rithms  have  been  proposed  over the \\npast decades.  Mena  [6] and Das et al.  presented  overviews  of road detection  methods  in this area. Quackenbush  [7] \\ngave  a review  of linear feature  extraction from imagery  which  can be used for road extraction.  Automatic road \\ndetection  method  tests were  devised  by Mayer  et al. [8].  Based  on the level of road knowledge  used,  Poullis  and \\nYou [9]  class ified road detection  methods  into three  categories:  1) pixel -based;  2) region -based;  and 3) knowledge -\\nbased.  The pixel -based  methods  depend  on the information obtained from the  pixels.  Line [10],  [11],  and ridge  [12],  \\n[13] detectors  are used to extract  potential  road points.  Road  points are then  connected  to produce  road segments  \\nand also used as input to a higher  level processing  phase.   \\nRoad  extraction  methods  have been  proposed  by several  authors  from different  viewpoints.  Based  on \\nhomogeneous  polygonal  areas around  each pixel,  Hu et al. [14]  defined  the pixel  footprint to extract  road areas.  \\nSimilarly,  Zhang  et al. [15]  applied this detector  to extract  roads in urban  areas.  Movaghati  et al. [16] applied  \\nparticle  filtering  and Kalman filtering  to extract road networks . Road  intersection  extraction  from  remotely  sensed \\nim\\nages was also studied  [17]–[19].  Although  road intersection extraction  alone cannot  be a complete  road network  \\ngeneration, it  is useful  for understanding  road network  topologies  and higher  processing.  \\nIn the semi automatic method the active contour model is being employed for  the extraction of roads.  In the active \\nco\\nntour model initial points (seed points) have to be specified from which the roads are detected. Depending on the \\nlength of the road to be extracted appropriate seed points ha ve to be g iven. The idea is  to plot the active contour \\nbetween the seed points. After filtering the image with a Gau ssian filter, the potential of the image is being computed \\nand it illustrates the influence of the edges of the informati on that has to be extracted. Even when the points don’t 1462   Jose Hormese and C. Saravanan  /  Procedia Technology   24  ( 2016 )  1460 – 1467 \\nfall in the centre line of the road, they help to the program to get an idea of the direc tion. So, data between them can \\nbe interpolated. Since new initialization snake points are being interpo lated , when the distances between the points \\nare larger than an amount of pixels, the order that the user initialize the points is impor tant. This is because it is \\ndepended on the direction and distance between these gi ven points to interpolate new points between them. So, the \\nActive Contour algorithm  is implemented in a consecutive sequence such that the total segment of the road, within \\nth\\ne points selected by the user, will be extracted. The first s tep is to specify the seed po ints along the road to be \\nextracted from a given satellite image. The next step  is to find the Gradien t and Potential of the satel lite image which \\nh\\nelps in identifying the roads edges. Then the contours are identified through interpolation based on  the seed points \\ngiven by the user. After the contours are identified the next step is to evolve the active contour (snake) along the \\nseed points specified by the user which represents the road surface.  \\n3. Advantages  and D isadvantages  of the Present  Works  \\n In the semi  automatic  extraction tec hniques  once  the initial  points  or seed points  are given  which  gives  a \\npreliminary  approximation  of the feature  and hence  feature  extraction  can be done  more  accurately.  So it gives  \\nbetter  results  for any type of satellite  images.  This saves time and human effort in extracting roads from images. The  \\ndraw back is that manual  intervention  is needed  initially  to perform  the computation.  Approximations  have  to be \\nprovided  by the operator.  If the points  are not given  the Interpolation  Routine  can generate  points  out of track. With  \\nhigh quality  images  object  identification and feature  extraction  can be done  effectively.  It helps  in better  decision -\\nmakings  for Urban  Planning,  Traffic  Management  and Vehicle  Nav igation.  The Disadvantage  is that the output  is \\npurely  dependent  on the Resolution  of the images,  Input  road characteristics  and variations.   \\nA given input satel lite image pertaining to an ordinary area is shown as follows:  \\n \\n \\n \\nFig.1. Input Image  \\n \\nUsing the Semi automatic method the Gradient of sm oot hed image and potential is obtained as follows:  \\n \\n \\n                                                                                              \\nFig.2. Gradient and Potential  \\n \\nAfter the Gradient and Potential are obtained, th e Active Contour operation is applied.  \\n  1463  Jose Hormese and C. Saravanan  /  Procedia Technology   24  ( 2016 )  1460 – 1467 \\n \\n                                                                            \\nFig.3. Active Contour  \\n \\nIn the Semi Automatic approach the Active Contour detects th e road from the given initial seed points.  \\n \\n4. Vectorization Approach to Road Extraction  \\n \\nWith the increasing resolution of remote sensing imag es,  road network can be displayed as continuous and \\nhomogeneity regions with a certain width rather than trad itional thin lines. Therefore, road network extraction from \\nlarge scale images refers to reliable road  su rface detection which identifies ro ad segments from the RS Images. A \\nnovel automatic road network d etectio n approach based on the combination of segmentation and vectorization is \\nexplain\\ned, which includes three main steps:  (i) the image is segmented to roughly  identify the road network regions; \\n(ii) the decision making and continuity procedure to co rrectly detect the roads and ( iii) the Vectorization step to \\nidentify the line segments or curved segments which repr esent the roads segment. Lastly, the results from QuickBird \\nimages demonstrate the correctness and efficiency of the proposed process. These steps can be explained using the \\nfollowing flow diagram as shown : \\n \\n \\n \\nFig.4. Automated Road Extraction Flow Diagram  \\n \\nThe Proposal is to develop  an automatic  method  for road extraction from high resolution satellite images. In  the \\nvectorization approach  of road extraction  no seed points  have  to be given.  The vectorization approach is an \\nauto\\nmatic method in extracting road segments from satel lite images. The method adopted is to identify the road \\nsegments which are represented as continuous line segments as the  road could be of any arbitrary shape. The start \\nand end poin\\nts of each line segment is  identified and the road segments in the image are correctly extracted. The  \\nfirst stage  is to identify  the road network  regions  using  segmentation.  Then  a decision  makin g and continuity  \\nprocedure  is being  performed  in order  to correctly  detect the road.  As a road  may be of different shapes like straight  \\nroads  or curved  ones the vectorization step identifies the line segment  which  corresponds  to the road.  The method  1464   Jose Hormese and C. Saravanan  /  Procedia Technology   24  ( 2016 )  1460 – 1467 \\nautomatically  identifies the road segments in a high  resolution  satellite image  where  the output  is highly  dependent  \\non the image.  \\nThe first stage of the automatic method is to classify t he road from the given satellite image using segmentation \\nwhich consist of two stages namely the Null Gradient  Orientation method and the Edge Detection method. The Null \\nGradient Orientation method finds out the gradient at all pixels and then the eigen transform is being performed on \\nthe tensor and the result is such that there  will be two eigen values and two eigen  v ectors. The method is to choose \\nthose pixels for which at least one eigen value has a minimum which corresponds to a road. And also the Eigen  \\nvector corresponding to minimum Eigen  value gives the direction of road. The n ext stage is the edge detection using \\nthe Canny Operator which identifies the edges which shows superior results compared to other edge detection \\nmethods. Also two threshold values are fine tuned so that the small connected elements are rejected and only the \\nlarge connected areas are chosen.  In th e second stage the task is to d etect the line segments and the Hough \\nTransform is being used to detect th e line segments. The final stage is th e vectorization step where the road \\nsegments are clearly identified from the High Resolution Satellite Image.  \\n \\n4.1. Image Segmentation  \\nIn computer vision, segmentation  refers to the process of partitioning a digital image  into multiple segments  (sets \\nof pixels, also known as super -pixels). The goal of segmentation is to simplify and/or change the representation of \\nan i\\nmage into something that is more meaningful and ea sier to analyze. Image segmen tation is typically used to \\nlocate objects and boundaries (lines, curv es, etc.) in images. More precisely, im age segmentation is the process of \\nassigning a label to every pixel in an image such that pix els with the same label share certain visual charac teristics.  \\nThe result of image segmentation is a set of  segments that collectively cover th e entire image, or a set of contours  \\nextracted from the image (see edge detection). Each of th e pix els in a region is similar with respect to some \\ncharacteristic or computed property, such as color, intensity, or texture. Adjacent  regions are significantly different \\nw\\nith respect to the same characteristics . \\n4.2 Null Gradient Orientation  \\nThe Gradient of a functio n of two variables F(x,y) can be defined as  \\njyFixFF\\x08 \\x08\\nww\\x0eww \\x92                                                                                                                                       (1 ) \\nThe above equation can be collected as a collection of vectors poi n ting in the direction of increasing values of F.  \\nThe numerical gradients (differences) can be comp u ted for functions with any number of variables.  For a Function \\nof\\n N variables F(x,y,z,…) the gradient can be computed as  \\n  ...\\x0eww\\x0eww\\x0eww \\x92 kzFjyFixFF\\x08\\x08 \\x08\\n                                                                                                                   (2)      \\n \\nThe description for the above equations can be given as : FX = gradient  (F) where F is a vector returns the one -\\ndimensional numerical gradient of F. FX corresponds to wF/wx, the differences in the x  (column) direction. FY \\nco\\nrresponds to wF/wy, the differences in the y  (row) direction. The spacing between points in each direction is \\nassumed to be one.  [FX,FY,FZ,...] = gradient(F) where F has N dimensions returns the N components of the \\ng\\nradient of F. There are two ways to cont rol the spacing betwee n values in F:    \\nA single spacing value, h, specif ies the spacing between points in ever y direction. N spacing values (h1,h2,...) \\nspecifies the spacing for each dime nsion of F. Scalar spacing parameters sp ecify a constant spacing for each 1465  Jose Hormese and C. Saravanan  /  Procedia Technology   24  ( 2016 )  1460 – 1467 \\ndimension. Vector parameters specify the coordinates of th e values along corresponding dimensions of F. In this \\ncase, the length of the vector must match the size of the corresponding dimension. In vector calculus, the gradient of \\na scalar f\\nield is a vector field  which points in the direction of the greatest rate of  increase of the scalar field and \\nwhose magnitude  is the greatest rate of change.  The property of Road is that there is continuity in  one direction. So \\nth\\ne method adopted is to find the Gradient at all pixels. The Gradient is then repr esented as a Tensor. Eigen \\nTransforms are being performed on the Tensor which consists  of two eigen values and two eigen vectors. The idea is \\nto choose those pixels for which at -leas t one eigen value has a minimum value which corresponds to a road. Also \\nth\\ne eigen vector corresponding to minimum eigen value gives the direction of road.  \\n \\n4.3 Edge D etectio n  \\n \\nEdge detection  is a fundamental tool in image processing  and computer vision, particularly in the areas of feature \\ndetection  and feature extraction, which aim at identifying points in a digital image  at which the image brightness  \\nchanges sharply or more formally has discontinuities.  In the ideal case, the result of ap ply ing an edge detector to an \\nimage may lead to a set of connected curves that in dicate the boundaries of objects, the boundaries of surface \\nmarkings as well as curves that correspond to discontinuities  in surface orientation. Thus, applying an edge detection \\nalgorithm to an image may significantly reduce the amount of data to be processed  and may therefore filter out \\ninformation that may be regarded as less relevan t, while preserving the important structural proper ties of an image. \\nIf\\n the edge detection step is successful, the subsequent tas k of interpreting the information contents in the original \\nimage may therefore be substantially simplified. However, it is not always possible to ob tain such ideal edges from \\nreal life images of moderate complexity. Edges extracted from non -trivial images are often hampered by \\nf\\nragmentation, meaning that the edge curves are not connected, missing edge segments as well as false edges  not \\nco\\nrresponding to interesting phenomena in the image – thus complicating the subsequent task of interpreting the \\ni\\nmage data.   \\nThe edge detection algorithm being em plo yed is the Canny Edge Detection Algorithm which consists of multi \\nstages.   \\nThe algorithm runs in 5 separate steps:  \\nx Smoothing: Blurring of the image to remove noise.  \\nx Finding gradients: The edges should be marked where th e gradients of the image has large magnitudes.  \\nx Non-maximum suppression: Only local maxima should be marked as edges.  \\nx Double thresholding: Potential edges are determined by thresholding.  \\nx Edge tracking by hysteresis: Final ed ges are determined by suppressing all ed ges that are not connected to a very \\ncertain (strong) edge.  \\n \\n4.4 Decision Making based on Continuity  \\n \\nA lot of edges are proved not to  be roa ds through the procedure  of edge detection. Therefore road following or \\ntrack\\ning is one of the most important steps in road detec tion. The major goal of road tracking is to eliminate road -\\nlike but non -road pixels. Hough Transforms are being us e d to perform this step. In automated  analysis of digital \\ni\\nmages, a sub -problem often arises of detecting si mple shapes, such as straight lines, circles or ellipses. In many \\ncases an edge detector  can be used as a  pre-processing stage to obtain image poin ts or image pixels that are on the \\ndesired curve in the image space. Due to  imperfections in either the image data or the edge detector, however, there \\nmay be missing points or pixels on the desired curves as well as spatial deviations between the ideal \\nline/circle/e llipse and the noisy edge points as they are obtained f rom the edge detector. For these reasons, it is often \\nnon-trivial to group the extracted edge features to an appr op riate set of lines, circles or ellipses. The purpose of the \\nHough transform is to address this problem by making it possible to perform groupings of edge points into object \\ncandidates by performing an explicit voting proced ure over a set of parameterized image objects.  \\n \\n4.5 Vector Representation of Roads  1466   Jose Hormese and C. Saravanan  /  Procedia Technology   24  ( 2016 )  1460 – 1467 \\nVector graphics  is the use of geometrical primitives  such as point s, lines, curves, and shapes or polygon(s), which \\nare all based on\\n mathematical equations, to represent images  in computer graphics. Vector graphics formats are \\nco\\nmplementary to raster graphics, which is the re presentation of images as an array of pixels, as it is typically used \\nf\\nor the representation of photographic images. Computer disp lays are made up from grids of small rectangular cells \\ncalled pixels. The picture is built up from these cells.  The smaller and closer the cell s a re together, the better the \\nquality of the image, but the bigger the file needed to store th e data. If the number of pixels is kept constant, the size \\nof each pixel will grow and the image becomes grainy (pixellated) when  magnified, as the resolution of the eye \\nenab\\nles it to pick out individual pixels.  \\nVector graphics files store the lines, shapes and colours t hat make up an image as mathematical formulae. A \\nvector graphics program uses these mathematical formulae to  construct the screen image, building the best quality \\nimage possible, given the screen resolution. The mathemati cal formulae determine where the dots that make up the \\nimage should be placed for the best results when displaying the image. Si nce these formulae can produce an image \\nscalable to any size and detail, the quality of the image is li mited only by the resolution of the display, and the file \\nsize of vector data generating the image stays the same. Printing the image to paper will usually give a sharper, \\nhigher resolution output than prin ting it to the screen but can use exactly the same vector data file.  \\n \\n \\n5. Results  \\n \\nThe vectorization approach is an automatic method in  extracti ng road segments from satellite images. The \\nmethod adopted is to identify the road segments which are represented as continu ous line segments as the road could \\nbe of any arbitrary shape. The start and end points of each  line segment is identified and the road segments in the \\nimage are correctly extracted.  \\nFor the above given high resolution satellite image, the output obtained is as follows : \\n                                                                \\n \\n \\nFig.5. Output Image  \\n \\nThe output obtained shows that the image is a tile from mapped data set. The method successfully extracted the \\nroad and represented it with just a single line segment. The method gave the correct results as there were no \\nocclusions in the road . \\n \\n6. Conclusion  \\n \\nIn the automatic method of road extraction no seed poi n ts have to be given. The method automatically identifies \\nthe road segments in a high resolution satellite image wher e the output is highly dependent on the image. This \\nmethod is more suited in rural areas than in urban areas where man -made objects are less and it is poss ible to detect \\nthe roads more easily. Through the segmentation, decision  making based on continuity and vectorization procedure \\nthe raster satellite images can be converted to vector repr esentation and it is possible to extract roads from satellite \\nimages. In the case of complex road structures and also in  the case of occlusions the semi automated method gave \\nbetter results than an automated method. The significance  of the automated method is that human labour can be \\nminimized to a very large extent. F or a real large scale road extraction  work, a combination of both methods is being \\nprop\\nosed. The first stage employs the u tility of the automatic method where the road segments are identified and for 1467  Jose Hormese and C. Saravanan  /  Procedia Technology   24  ( 2016 )  1460 – 1467 \\nidentifying the missing parts of the road the semi automatic method is being employed. The combination of both the \\nmethods will save time as well as reduce human labour to a very large extent . \\n \\nReferences  \\n[1] Wenzhong  Shi, Zelang  Miao,  and Johan  Debayle,  “ An Integrated  Method  for Urban  Main  Road  Centerline  Extraction  from  Optical  Remote  \\nSensed  Imagery ”, IEEE  Transactions  on Geo-science and Remote  Sensing,  Vol. 52, No. 6, June 2014.  \\n[2] S. Das, T. T. Mirnalinee,  and K. Varghese,  “Use of salient features  for the design  of a multistage  framework  to extract  roads  from  high \\nresolution  multispectral  satellite  images, ” I EEE  Trans.  Geosci.  Remote  Sens.,  vol. 49, no. 10, pp. 3906 –3931,  Oct. 2011.  \\n[3] C. Unsalan  and B. Sirmacek,  “Road  network  detection  using  probabilistic  and graph  theoretical  methods, ” I EEE  Trans.  Geosci.  Remote  Sens.,  \\nvol. 50, no. 11, pp. 4441– 4453,  Nov.  2012.  \\n[4] Barzohar,  Mein,  Cooper,  David  B. “Automatic  finding  of main  roads  in aerial  images  by using  geometric  - stochastic  models and estimation ”, \\nIEEE  Computer  Vision  and Pattern  Recognition , pp. 459-464, 1993.  \\n[5] Grün,  A., Li, H., “Semi-automatic  linear  feature  extraction  by dynamic  programming  and LSB-snakes ”, Photogrammetric  Engineering  & \\nRemote Sensing , Vol. 63, No. 8, pp. 985-995, 1997.  \\n[6] J. B. Mena,  “State  of the art on automatic  road extraction  for GIS update: A  novel  classification, ” P attern  Recognit.  Lett.,  vol. 24, no. 16, pp. \\n3037 – 3058,  Dec. 2003.  \\n[7] L. J. Quackenbush,  “A review  of techniques  for extracting  linear  features  from  imagery, ” Ph otogramm.  Eng. Remote  Sens.,  vol. 70, no. 12, \\npp\\n. 1383 –1392,  Dec. 2004.  \\n[8] H. Mayer,  S. Hinz,  U. Bacher,  and E. Baltsavias,  “A test of automatic  road extraction  approaches, ” In t. Archives  Photogramm.,  Remote  Sens.,  \\nSpatial Inf.  Sci., vol. 36, no. 3, pp. 209–214, 2006.  \\n[9] C. Poullis  and S. You,  “Delineation  and geometric  modeling  of road networks, ” IS PRS J. Photogramm.  Remote  Sens.,  vol. 65, no. 2, pp. 165–  \\n181, Mar.  2010.  \\n[10] A. Gruen  and H. Li, “Semi -automatic  linear  feature  extraction  by dynamic programming  and LSB-snakes, ” Ph otogramm.  Eng. Remote  \\nSens.,  vol. 63, no.  8, pp. 985–995, Aug.  1997.  \\n[11] F. Dell’Acqua  and P. Gamba,  “Detection  of urban  structures  in SAR images  by robust  fuzzy clustering  algorithms:  The example  of street  \\ntracking, ” I EEE  Trans.  Geosci.  Remote  Sens.,  vol. 39, no. 10, pp. 2287 –2297, Oct.  2001.  \\n[12] R. Nevatia  and K. Babu, “Linear  feature  extraction  and description, ”Comput.  Graph.  Image  Process.,  vol. 13, no. 3, pp. 257–269, Jul. 1980.  \\n[13] K. Treash  and K. Amaratunga,  “Automatic  road detection  in gray scale aerial  images, ” AS CE J. Comput.  Civil Eng.,  vol. 14, no. 1, pp. 60–\\n69, 2000.  \\n[14] J. Hu, A. Razdan,  J. C. Femiani,  M. Cui, and P. Wonka,  “Road  network extraction and  intersection  detection  from  aerial  images by tracking  \\nroad footprints, ” I EEE Trans.  Geosci.  Remote  Sens.,  vol. 45, no. 12, pp. 4144 –4157,  Dec. 2007.  \\n[15] J. Zhang,  X. Lin, Z. Liu, and J. Shen,  “Semi -automatic  road tracking by  template  matching  and distance  transformation  in urban  areas, ” In t.  \\nJ. Remote  Sens.,  vol. 32, no. 23, pp. 8331– 8347,  Dec. 2011.  \\n[16] S. Movaghati,  A. Moghaddamjoo,  and A. Tavakoli,  “Road  extraction  from  satellite  images using  particle  filtering  and extended  Kalman  \\nfiltering, ” I EEE  Trans.  Geosci.  Remote  Sens.,  vol. 48, no. 7, pp. 2807 –2817,  Jul. 2010.  \\n[17] A. Barsi  and C. Heipke,  “Artificial  neural  networks  for the detection  of road junctions  in aerial  images, ” IS PRS  Arch.,  vol. 34, pt. 3/W8,  pp. \\n113–118, Sep. 2003.  \\n[18] M. Ravanbakhsh,  C. Heipke,  and K. Pakzad,  “Road  junction  extraction from  high resolution  aerial  imagery, ” Ph otogramm.  Rec.,  vol. 23, no. \\n124, pp.  405–423, Dec. 2008. \\n[19] J. J. Ruiz,  T. J. Rubio,  and M. A. Urena,  “Automatic  extraction  of road intersections  from  images  based  on texture  characterisation, ” Su rvey \\nRev., vol.  43, no. 321, pp. 212–225, Jul. 2011.  ', 'HAL Id: hal-01369906\\nhttps://inria.hal.science/hal-01369906\\nSubmitted on 13 Mar 2017\\nHAL is a multi-disciplinary open access\\narchive for the deposit and dissemination of sci-\\nentific research documents, whether they are pub-\\nlished or not. The documents may come from\\nteaching and research institutions in F rance or\\nabroad, or from public or private research centers.L’archive ouverte pluridisciplinaire HAL , est\\ndestinée au dépôt et à la diffusion de documents\\nscientifiques de niveau recherche, publiés ou non,\\némanant des établissements d’enseignement et de\\nrecherche français ou étrangers, des laboratoires\\npublics ou privés.\\nConvolutional Neural Networks for Large-Scale Remote\\nSensing Image Classification\\nEmmanuel Maggiori, Y uliya T arabalka, Guillaume Charpiat, Pierre Alliez\\nT o cite this version:\\nEmmanuel Maggiori, Y uliya T arabalka, Guillaume Charpiat, Pierre Alliez. Convolutional Neural\\nNetworks for Large-Scale Remote Sensing Image Classification. IEEE T ransactions on Geoscience and\\nRemote Sensing, 2017, 55, pp.645-657. \\uffff10.1109/tgrs.2016.2612821\\uffff. \\uffffhal-01369906\\uffffIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 1\\nConvolutional Neural Networks for Large-Scale\\nRemote Sensing Image Classiﬁcation\\nEmmanuel Maggiori, Student member, IEEE, Yuliya Tarabalka, Member, IEEE,\\nGuillaume Charpiat, and Pierre Alliez\\nAbstract —We propose an end-to-end framework for the dense,\\npixelwise classiﬁcation of satellite imagery with convolutional\\nneural networks (CNNs). In our framework, CNNs are directly\\ntrained to produce classiﬁcation maps out of the input images.\\nWe ﬁrst devise a fully convolutional architecture and demonstrate\\nits relevance to the dense classiﬁcation problem. We then address\\nthe issue of imperfect training data through a two-step training\\napproach: CNNs are ﬁrst initialized by using a large amount\\nof possibly inaccurate reference data, then reﬁned on a small\\namount of accurately labeled data. To complete our framework\\nwe design a multi-scale neuron module that alleviates the common\\ntrade-off between recognition and precise localization. A series\\nof experiments show that our networks take into account a large\\namount of context to provide ﬁne-grained classiﬁcation maps.\\nIndex Terms —Classiﬁcation, satellite images, convolutional\\nneural networks, deep learning.\\nI. I NTRODUCTION\\nTHE ANALYSIS of remote sensing images is of\\nparamount importance in many practical applications,\\nsuch as precision agriculture and urban planning. Recent\\ntechnological developments have signiﬁcantly increased the\\namount of available satellite imagery. Notably, the constel-\\nlation of Pl ´eiades satellites produces high spatial resolution\\nimages that cover the whole Earth in less than a day. The\\nlarge-scale nature of these datasets introduces new challenges\\nin image analysis. In this paper we address the problem of\\npixelwise classiﬁcation of satellite imagery.\\nThere is a vast literature on classiﬁcation approaches that\\ntake into account the spectrum of every individual pixel to\\nassign it to a certain class. Alternatively, more advanced\\ntechniques combine information from a few neighboring pixels\\nto enhance the classiﬁers’ performance, often referred to as\\nspectral-spatial classiﬁcation. These approaches rely on the\\nseparability of the different classes based on the spectrum\\nof a single pixel or of some neighboring pixels. In a large-\\nscale setting, however, these approaches are not effective.\\nOn the one hand, current large-scale satellite imagery does\\nnot use high spectral resolution sensors, making it difﬁcult\\nto distinguish object classes solely by their spectrum. On\\nthe other hand, due to the large spatial extent covered by\\nthe datasets, classes have a considerable internal variability,\\nwhich further challenges the class separability when simply\\nE. Maggiori, Y . Tarabalka and P. Alliez are with Univerist ´e Cˆote d’Azur,\\nTITANE team, Inria, 2004 Route des Lucioles, BP93 06902 Sophia Antipolis\\nCedex, France. E-mail: emmanuel.maggiori@inria.fr.\\nG. Charpiat is with Tao team, Inria Saclay– ˆIle-de-France, LRI, B ˆat. 660,\\nUniversit Paris-Sud, 91405 Orsay Cedex, France.\\nManuscript received ...; revised ...observing the spectral signatures of a restricted neighborhood.\\nWe argue that a more thorough understanding of the context\\nsuch as, e.g., the shape of objects, is required to aid the\\nclassiﬁcation process.\\nConvolutional neural networks (CNNs) [1] are therefore\\ngaining attention, due to their capability to automatically\\ndiscover relevant contextual features in image categorization\\nproblems. CNNs consist of a stack of learned convolution\\nﬁlters that extract hierarchical contextual image features, and\\nare a popular form of deep learning networks. They are already\\noutperforming other approaches in various domains such as\\ndigit recognition [2] and natural image categorization [3].\\nOur goal is to devise an end-to-end framework to clas-\\nsify satellite imagery with CNNs. The context of large-scale\\nsatellite image classiﬁcation introduces certain challenges that\\nwe must address in order to turn CNNs into a relevant\\nclassiﬁcation tool. Notably, we must (1) design a speciﬁc\\nneural network architecture for our problem, (2) acquire large-\\nscale training data and handle its eventual inaccuracies, and\\n(3) generate high-resolution output classiﬁcation maps.\\n1) CNN architecture: CNNs are commonly used for image\\ncategorization , i.e., for assigning the entire image to a class\\n(e.g., a digit [1] or an object category [3]). In remote sensing,\\nthe equivalent problem is to assign a category to an entire\\nimage patch, such as ‘residential’ or ‘agricultural’ area. Our\\ncontext differs in that we wish to conduct a dense pixelwise\\nlabeling. We must thus design a CNN that outputs a per-pixel\\nclassiﬁcation and not just a category for the entire input.\\n2) Imperfect training data: A sensitive point regarding\\nCNNs is the amount of training data required to properly learn\\nthe network parameters. A large source of free-access maps\\nis OpenStreetMap, a collaborative online mapping platform,\\nbut the availability of data is highly variable between areas.\\nIn some areas, the coverage is very limited or nonexistent,\\nand an irregular misregistration is prevalent throughout the\\nmaps. As we focus on the large-scale application of CNNs for\\nclassiﬁcation, we must explore the use of imperfect training\\ndata in order to make our framework applicable to a wide\\nrange of geographic areas.\\n3) High-resolution output: The power of CNNs to take a\\nlarge context to conduct predictions comes at the price of\\nlosing resolution for the output. This is because some degree\\nof downsampling of the feature maps along the network is\\nrequired in order to increase the amount of context without\\nan excessive number of learnable parameters. Such coarse\\nresolution translates into a fuzzy aspect around object edgesIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 2\\nand corners. One of our challenges is then to alleviate this\\ntrade-off.\\nA. Related Work\\nWe now review classiﬁcation methods and the use of CNNs\\nin remote sensing.\\nIn the context of spectral classiﬁcation, decision trees [4],\\nartiﬁcial neural networks [5], [6] and support vector ma-\\nchines [7] are some of the approaches that have been ex-\\nplored, both for multispectral and hyperspectral image analy-\\nsis. Spectral-spatial methods [8] use contextual information to\\nregularize the classiﬁcation maps. Different approaches have\\nbeen presented, for example, Liao et al. [9] sequentially apply\\nmorphological ﬁlters to model different kinds of structural\\ninformation and Tarabalka et al. [10] model spatial interactions\\nwith a graphical model. Neural networks have also been used\\nfor spectral-spatial classiﬁcation. In this direction, Kurnaz\\net al. [11] use such network to classify the concatenated\\nspectrum of pixels inside a sliding window, in order to label\\nmultispectral images. In a similar fashion, Lloyd et al. [12]\\ncompute a textural feature which is concatenated to the pixel\\nspectrum vector, prior the a neural network classiﬁcation. Lu\\nand Weng [13] provide a comprehensive survey on classiﬁca-\\ntion methods.\\nIn remote sensing, CNNs have been used to individually\\nclassify the pixels of hyperspectral images. This was achieved\\nby performing convolutions in the 1D domain of the spectrum\\nof each pixel [14], [15], [16]. Alternatively, a spectral-spatial\\napproach has been taken by convolving in the 1D ﬂattened\\nspectrum vector of a group of adjacent pixels [17], [18].\\nNote however that these approaches do not learn spatial\\ncontextual features such as the typical shape of the objects\\nof a class. Recent works have incorporated convolutions on\\nthe spatial domain after extracting the principal components\\nof the hyperspectral image [19], [20], [21], and the idea of\\nreasoning at multiple spatial scales has also been exploited,\\nnotably for hyperspectral classiﬁcation [22], [23] and image\\nsegmentation [24]. Let us remark that convolutional neural\\nnetworks have also been used for other remote sensing appli-\\ncations, such as road tracking [25], object detection [26] and\\nland use classiﬁcation [27], [28].\\nMnih [29] proposed a speciﬁc architecture to learn large-\\nextent spatial contextual features for aerial image labeling.\\nIt is derived from common image categorization networks\\nby increasing the output size of the ﬁnal layer. Instead of\\noutputting a single value to indicate the category, the ﬁnal layer\\nproduces an entire dense classiﬁcation patch. This network\\nsuccessfully learns contextual spatial features to better distin-\\nguish the object classes. However, this patchwise procedure\\nhas the disadvantage of introducing artifacts on the border of\\nthe classiﬁed patches. Moreover, the last layer of the network\\nintroduces an unnecessarily large number of parameters, ham-\\npering its efﬁciency.\\nB. Contributions\\nWe now summarize our contributions to address the issues\\npresented before and provide then a framework for satellite\\nimage classiﬁcation with CNNs.1) Fully convolutional architecture: We ﬁrst analyze the\\nCNN architecture proposed by Mnih [29] and the fact that it\\nhas a fully connected layer, i.e., connected to allthe outputs of\\nthe previous layer, to produce the output classiﬁcation patches.\\nWe point out that this architectural decision hampers both its\\naccuracy and efﬁciency.\\nWe then propose a new network architecture that is fully\\nconvolutional , i.e., that only involves a series of convolution\\nand deconvolution operations to produce the output clas-\\nsiﬁcation maps. This architecture solves the issues of the\\nprevious patch-based approach by construction. While such a\\nfully convolutional architecture imposes further restrictions to\\nthe neuronal connections than the fully connected approach,\\nthese restrictions reduce the number of trainable parameters\\nwithout losing generality. It has been seen multiple times in\\nthe literature that reducing the number of parameters under\\nsensible assumptions often implies a simpler error surface\\nand helps reaching better local minima. For example, con-\\nvolutional networks have fewer connections than multi-layer\\nperceptrons but perform better in practice for visual tasks [1],\\nand Mnih [29] showed that adding too many layers to a\\nnetwork resulted in poorer results.\\nWe compare the fully convolutional vs fully connected\\napproaches on a dataset of publicly available aerial color\\nimages over Massachusetts [29] created with the speciﬁc\\npurpose of evaluating CNN architectures.\\n2) Two-step training approach: To deal with the imper-\\nfections in training data we propose a two-step approach.\\nFirst, we train our fully convolutional neural network on raw\\nOpenStreetMap data to discover the generalities of the dataset.\\nSecond, we ﬁne-tune the resulting neural networks for a few\\niterations under a small piece of manually labeled image. Our\\nhypothesis is that, once the network is pre-trained on large\\namounts of imperfect data, we can boost its performance by\\n“showing” it a small amount of accurate labels. Our approach\\nis inspired by a common practice in deep learning: taking\\npre-trained networks designed to solve one problem and ﬁne-\\ntuning them to another problem.\\n3) Multi-scale architecture: We design a speciﬁc neuron\\nmodule that processes its input at multiple scales, while\\nkeeping a low number of parameters. This alleviates the\\naforementioned trade-off between the amount of context taken\\nand the resolution of the classiﬁcation maps. Our overall\\napproach constitutes then an end-to-end framework for satellite\\nimage labeling with CNNs. We evaluate it on a Pl ´eiades image\\ndataset over France, where the associated OpenStreetMap data\\nis signiﬁcantly inaccurate.\\nC. Organization of the Paper\\nIn the next section an introduction to convolutional neural\\nnetworks is presented. In Section III the fully convolutional\\narchitecture is described and evaluated. Section IV presents\\nthe two-step training approach and the multi-scale architec-\\nture, in order to use CNNs as an end-to-end framework for\\nsatellite image classiﬁcation. Finally, conclusions are drawn\\nin Section V.IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 3\\nII. C ONVOLUTIONAL NEURAL NETWORKS\\nIn machine learning an artiﬁcial neural network is a system\\nof interconnected neurons that pass messages to each other.\\nNeural networks are used to model complex functions and, in\\nparticular, as frameworks for classiﬁcation. In this work we\\ndeal with the so-called feed-forward networks, whose graph\\nof message passing between neurons is acyclic [30].\\nAn individual neuron takes a vector of inputs x=x1:::x n\\nand performs a simple operation to produce an output a. The\\nmost common neuron is deﬁned as follows:\\na=\\x1b(wx+b); (1)\\nwhere xdenotes a weight vector, ba scalar known as bias and\\n\\x1ban activation function. The weight vectors and the biases are\\nparameters that deﬁne the function computed by a network,\\nand the goal of training is to ﬁnd the optimal values for\\nthese parameters. When using at least one layer of nonlinear\\nactivation functions, one can prove that a sufﬁciently large\\nnetwork can represent any function, suggesting the expres-\\nsive power of neural networks. The most common activation\\nfunctions are sigmoids, hyperbolic tangents and rectiﬁed linear\\nunits (ReLU) [3]. ReLUs are known to offer some practical\\nadvantages in the convergence of the training procedure.\\nEven though any function can be represented by a sufﬁ-\\nciently large single layer of neurons, it is common to organize\\nthem in a set of stacked layers that transform the outputs of the\\nprevious layer and feed it to the next layer. This encourages\\nthe networks to learn hierarchical features, doing low-level\\nreasoning in the ﬁrst layers and performing higher-level tasks\\nin the last layers. For this reason, the ﬁrst and last layers are\\noften referred to as lower and upper layers respectively.\\nIn an image categorization problem, the input of our net-\\nwork is an image (or a set of features derived from an image),\\nand the goal is to predict the correct label associated with the\\nimage. Finding the optimal neural network classiﬁer reduces\\nto ﬁnding the weights and biases that minimize a loss L\\nbetween the predicted values and the target values in a training\\nset. If there is a set Lof possible classes, the labels are\\ntypically encoded as a vector of length jLjwith value ‘1’\\nat the position of the correct label and ‘0’ elsewhere. The\\nnetwork has then as many output neurons as possible labels. A\\nsoftmax normalization is performed on top of the last layer to\\nguarantee that the output is a probability distribution, i.e., the\\nvalues for every label are between zero and one and add to\\none. The multi-label problem is then seen as a regression on\\nthe desired output label vectors.\\nThe loss function Lquantiﬁes the misclassiﬁcation by\\ncomparing the target label vectors y(i)and the predicted label\\nvectors ^ y(i), forntraining samples i= 1:::n . In this work\\nwe use the common cross-entropy loss, deﬁned as:\\nL=\\x001\\nnnX\\ni=1jLjX\\nk=1y(i)\\nklog ^y(i)\\nk: (2)\\nThe cross-entropy loss has fast convergence rates when train-\\ning neural networks (compared with, for instance, the Eu-\\nclidean distance between yand^ y) and is numerically stable\\nwhen coupled with softmax normalization [30].Note that in the special case of binary labeling we can\\nproduce only one output (with targets ‘1’ for positive and\\n‘0’ for negative). In this case a sigmoid normalization and\\ncross-entropy loss are analogously used, albeit a multi-class\\nframework can also be used for two classes.\\nOnce the loss function is deﬁned, the parameters (weights\\nand biases) that minimize the loss must be solved for. Solving\\nis achieved by gradient descent by computing the derivative\\n@L\\n@w iof the loss function with respect to every parameter wi,\\nand updating the parameters with a learning rate \\x15as follows:\\nwi wi+\\x15@L\\n@wi: (3)\\nThe derivatives@L\\n@w iare obtained by backpropagation , which\\nconsists in explicitly computing the derivatives of the loss with\\nrespect to the last layer’s parameters and using the chain rule\\nto recursively compute the rest of the derivatives. In practice,\\nlearning is performed by stochastic gradient descent , i.e., by\\nestimating the loss (2) on a small subset of the training set,\\nreferred to as a mini-batch.\\nDespite the fact that neural networks can represent very\\ncomplex functions, the epigraph of the loss function Lcan\\nbe highly non-convex, making the optimization difﬁcult via a\\ngradient descent approach. To regularize this loss and improve\\ntraining, convolutional neural networks (CNNs) [1] are a\\nspecial type of neural networks that impose restrictions that\\nmake sense in the context of image processing. In these\\nnetworks, every neuron is associated to a spatial location (i;j)\\nwith respect to the input image. The output aijassociated with\\nlocation (i;j)is then computed as follows:\\naij=\\x1b((W\\x03X)ij+b); (4)\\nwhere Wdenotes a kernel with learned weights, Xthe input\\nto the layer and ‘\\x03’ the convolution operation. Note that this\\nis a special case of the neuron in Eq. 1 with the following\\nconstraints:\\n\\x0fThe connections only extend to a limited spatial neigh-\\nborhood determined by the kernel size;\\n\\x0fThe same ﬁlter is applied to each location, guaranteeing\\ntranslation invariance.\\nTypically multiple convolution kernels are learned in every\\nlayer, interpreted as a set of spatial feature detectors. The\\nresponses to every learned ﬁlter are therefore known as a\\nfeature map .\\nDeparting from the traditional fully connected layer, in\\nwhich every neuron is connected to all outputs of the previous\\nlayer, a convolutional layer dramatically reduces the number\\nof parameters by enforcing the aforementioned constraints.\\nThis results in a regularized loss function, easier to optimize,\\nwithout losing much generality.\\nNote that the convolution kernels are actually three-\\ndimensional because, in addition to their spatial extent, they go\\nthrough all the feature maps in the previous layers, or through\\nall the bands in the input image. Since the third dimension\\ncan be inferred from the previous layer it is rarely speciﬁed\\nin architecture descriptions, only the two spatial dimensions\\nbeing usually mentioned.IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 4\\nIn addition to convolutional layers, state-of-the-art networks\\nsuch as Imagenet [3] involve some degree of downsampling,\\ni.e., a reduction in the resolution of the feature maps. The\\ngoal of downsampling is to increase the so-called receptive\\nﬁeld of the neurons, which is the part of the input image\\nthat neurons can “see”. For the predictions to take into\\naccount a large spatial context, the upper layers should have\\na large receptive ﬁeld. This is achieved either by increasing\\nthe convolution kernel sizes or by downsampling feature\\nmaps to a lower resolution. The ﬁrst alternative increases the\\nnumber of parameters and memory consumption, making the\\ntraining and inference processes prohibitive. State-of-the-art\\nCNNs tend then to keep the kernels small and add some\\ndegree of downsampling instead. This can be accomplished\\neither by including pooling layers (e.g., taking the average or\\nmaximum of adjacent locations) or by introducing a so-called\\nstride , which amounts to skip some convolutions through, e.g.,\\napplying the ﬁlter once every four locations.\\nClassiﬁcation networks typically contain a fully connected\\nlayer on top of the convolutions/pooling. This layer is designed\\nto have as many outputs as labels, and produces the ﬁnal\\nclassiﬁcation scores.\\nThe overall success of CNNs lies mostly in the fact the\\nthe networks are forced by construction to learn hierarchical\\ncontextual translation-invariant features, which are particularly\\nuseful for image categorization.\\nIII. CNN S FOR DENSE CLASSIFICATION\\nIn this work we address the problem of dense classiﬁ-\\ncation, i.e., not just the categorization of an entire image,\\nbut a full pixelwise labeling into the different categories. We\\nﬁrst describe an existing approach, the patch-based network,\\npoint out its limitations and propose a fully convolutional\\narchitecture that addresses these limitations. We restrict our\\nexperiments to the binary labeling problem for the building\\nvsnot building classes, but our approach is extensible to\\nan arbitrary number of classes following the formulation\\ndescribed in Section II.\\nA. Patch-based Network\\nTo perform dense classiﬁcation of aerial imagery, Mnih\\nproposed a patch-based convolutional neural network [29].\\nTraining and inference are performed patch-wise: the network\\ntakes as input a patch of an aerial image, and generates\\nas output a classiﬁed patch. The output patch is smaller,\\nand centered in the input patch, to take into account the\\nsurrounding context for more accurate predictions. The way to\\ncreate dense predictions is to increase the number of outputs of\\nthe last fully connected classiﬁcation layer, in order to match\\nthe size of the target patch.\\nFig. 1(a) illustrates the patch-based architecture from [29].\\nThe network takes 64\\x0264patches (on color images of 1m2\\nspatial resolution) and predicts 16\\x0216centered patches of the\\nsame resolution. Three convolutional layers learn 64, 112 and\\n80 convolution kernels, of 12\\x0212,4\\x024and3\\x023spatial\\ndimensions, respectively. The ﬁrst convolution is strided (one\\n(a) Color\\n (b) Patch-based\\n (c) FCN\\nFig. 2: The patch-based predictions exhibit artifacts on the\\npatch borders while the FCN prevents them by construction.\\nconvolution every four pixels), which implies a downsampling\\nwith factor 4.\\nAfter the three convolutional layers, a fully connected layer\\ntransforms the high-level features of the last convolutional\\nlayer into a classiﬁcation map of 256 elements, matching the\\nrequired 16\\x0216output patch.\\nTraining is performed by selecting random patches from the\\ntraining set, and grouping them into mini-batches as required\\nby the stochastic gradient descent algorithm.\\nB. Limitations of the Patch-based Framework\\nWe now point out some limitations of the patch-based\\napproach discussed above, which motivate the design of an\\nimproved network architecture. Let us ﬁrst analyze the role\\nof the last fully connected layer that constructs the output\\npatches. In the architecture of Fig. 1(a), the size of the feature\\nmaps in the last convolutional layer (before the last fully\\nconnected one) is 9\\x029. The resolution of these ﬁlters is 1/4\\nof the resolution of the input image, due to the 4-stride in\\nthe ﬁrst convolution. The output of the fully connected layer\\nis, however, a full-resolution 16\\x0216classiﬁcation map. This\\nmeans that the fully connected layer does not only compute\\nthe classiﬁcation scores, but also learns how to upsample\\nthem. Outputting a full-resolution patch is then the result of\\nupsampling and not of an intrinsic high-resolution processing.\\nWe also observe that the fully connected layer allows outputs\\nat different locations to have different weights with respect\\nto the previous layer. For example, the weights associated\\nto an output pixel at the top-left corner of a patch can be\\ndifferent to those of a pixel at the bottom right. In other\\nwords, the network can learn priors on the position inside\\na patch. This makes sense in some speciﬁc contexts such as\\nwhen labeling pictures of outdoor scenes: the system could\\nlearn a prior for the sky to be at the top of the image. In\\nour context, however, the partition of an image into patches\\nis arbitrary, hence the “in-patch location” prior is irrelevant\\nsince allowing different weights at different patch locations\\nmay yield undesirable properties. For example, feeding two\\nimage patches that are identical but rotated by 90 degrees\\ncould yield different classiﬁcation maps.\\nWhen training the network of Fig. 1(a) we expect that, after\\nprocessing many training cases, the fully connected layer will\\nend up learning a location-invariant function. Figs. 2(a)-(b)\\nillustrate a fragment of an output score map by using suchIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 5\\n(a) Patch-based\\n(b) Fully convolutional ( 16\\x0216output)\\nFig. 1: Convolutional neural network architectures (e.g., “64@ 14\\x0214” means 64 feature maps of size 14\\x0214).\\nan architecture. Notice the discontinuities at the border of\\nthe patches, which reveal that the network did not succeed\\nin learning to classify pixels independently of their location\\ninside the patch. While this issue is partly addressed in [29]\\nby smoothing the outputs with a conditional random ﬁeld, we\\nargue that avoiding such artifacts by construction is desirable.\\nIn addition, generating similar results regardless of image\\ntiling is an important property for large-scale satellite image\\nprocessing, and an active research topic [31], [32]. Another\\nconcern with the fully connected layer is that the receptive ﬁeld\\nof every patch output is not centered in itself. For example, a\\nprediction near the center of the output patch can “see” about\\n32 pixels in every direction around it. However, the prediction\\nat the top-left corner of the output patch considers a larger\\nportion of the image to the bottom and to the right than to the\\ntop and to the left. Considering that the division into patches\\nis arbitrary, this behavior is hard to justify.\\nA deeper understanding of the role played by every layer of\\nthe network, as described in this section, motivates the design\\nof a more suitable architecture from a theoretical point of view,\\nwith the additional goal of boosting the overall performance\\nof the approach.\\nC. Fully Convolutional Network\\nWe propose a fully convolutional neural network architec-\\nture (FCN) to produce dense predictions. We explicitly restrict\\nthe process to be location-independent, enforcing the outputs\\nto be the result of a series of convolutions only (see Fig. 1b).\\nA classiﬁcation network may be “convolutionalized” [33] as\\nfollows. We ﬁrst convert the fully connected layer that carries\\nout the classiﬁcation to a convolutional layer. The convolution\\nkernel is chosen so that its dimensions coincide with the\\nprevious layer. Thus, its connections are equivalent to a fully\\nconnected layer. The difference is that if we enlarge the input\\nimage, the output size is also increased, but the number of\\nparameters remains constant. This may be seen as convolving\\nthe whole original network around a larger image to evaluate\\nthe output at different locations.\\nTo increase the resolution of the output map, we then add a\\nso-called “deconvolutional” layer [33]. The goal of this layer is\\nto upsample the feature maps from the previous layer, which is\\nachieved by performing an interpolation from a set of nearby\\npoints. Such an interpolation is parametrized by a kernel\\nthat expresses the extent and amount of contribution from a\\npixel value to its neighboring positions, only based on their\\nlocations. For an effective interpolation, the kernels must be\\nFig. 3: “Deconvolution” layer for upsampling.\\nlarge enough to overlap in the output. The interpolation is then\\nperformed by multiplying the values of the kernel by every\\ninput and adding the overlapping responses in the output. This\\nprocess is illustrated by Fig. 3 for a 2x upsampling. Notice\\nthat the scaling step is performed based on a constant 4\\x024\\nkernel. In our framework, and as in previous work [33], the\\ninterpolation kernel is another set of learnable parameters of\\nthe network instead of being determined a priori, e.g., setting\\nthem to represent a bilinear interpolation. Note also that the\\nupsampled feature map has a central part computed by adding\\nthe contribution of two neighboring kernels and an outer\\nborder obtained solely by the contribution of one kernel (the\\ntwo leftmost and rightmost output columns in Fig. 3). The\\nouter border can be seen as an extrapolation of the input while\\nthe inner part can be seen as an interpolation. The extrapolated\\nborder can be cropped from the output to avoid artifacts.\\nAs compared to a patch-based approach, we can expect our\\nfully convolutional network to exhibit the following advan-\\ntages:\\n\\x0fElimination of discontinuities due to patch borders;\\n\\x0fImproved accuracy due to a simpliﬁed learning process,\\nwith a smaller number of parameters;\\n\\x0fLower execution time at inference, due to the fast GPU\\nexecution of convolution operations.\\nOur FCN network is constructed by convolutionalizing the\\nexisting patch-based network depicted by Fig. 1(a). We choose\\nan existing framework to beneﬁt from a mature architecture\\nand to carry out a rigorous comparison. The architectural\\ndecisions (i.e., the choice of the number of layers and ﬁlter\\nsizes) of the base network are described in [29].\\nFig. 1(b) depicts the resulting FCN. First, we pretend that\\nthe output patch of the original network is only of sizeIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 6\\n1\\x021, thus just focusing on a single output centered in its\\nreceptive ﬁeld. Second, we rewrite the fully connected layer\\nas a convolutional layer with one feature map and the spatial\\ndimensions of the previous layer ( 9\\x029). Third, we add a\\ndeconvolutional layer that upsamples its input by a factor of\\n4 (with a learnable kernel of size 8\\x028), in order to recover\\nthe input resolution. Notice that the tasks of classiﬁcation and\\nupsampling are now separated.\\nThis new network can take input images of different sizes,\\nwith the output size varying accordingly. For example, during\\nthe training stage we wish to output patches of size 16\\x0216in\\norder to emulate the learning process as was done in the patch-\\nbased network of Fig. 1(a). For this we require a patch input\\nof size 80\\x0280, as in the architecture of Fig. 1(b). Notice\\nthat the input is larger than the original 64\\x0264patches.\\nThis is not because we are taking more context to carry\\nout the predictions, but instead because every output is now\\ncentered in its context. At inference time we can take inputs\\nof arbitrary sizes and feed them to the network to construct\\nthe classiﬁcation maps, and the number of network parameters\\ndoes not vary.\\nIn the deconvolutional layer illustrated in Fig. 1(b), the\\noverlapping areas added to produce the output are depicted\\nin gray while the excluded extrapolation is in white.\\nD. Experiments on Fully Convolutional Networks\\nWe implemented the CNNs using the Caffe deep learning\\nframework [34]. In a ﬁrst experiment we apply our approach\\nto the Massachusetts Buildings Dataset [29]. This dataset\\nconsists of color images over the area of Boston with 1 m2\\nspatial resolution, covering an area of 340 km2for training,\\n9 km2for validation and 22.5 km2for testing. The images are\\nlabeled into two classes: building andnot building . A portion\\nof an image and its corresponding reference are depicted in\\nFigs. 4(a-b).\\nWe train the patch-based and fully convolutional networks\\n(Figs. 1(a) and 1(b) respectively) for 30,000 stochastic gra-\\ndient descent iterations, until we observe barely no further\\nimprovement on the validation set. The patches are sampled\\nuniformly from the whole training set, with mini-batches of\\n64 patches each and a learning rate of 0.0001. A momentum\\nand an L2 parameter penalty are introduced to regularize\\nthe learning process and avoid overﬁtting. Momentum adds\\na fraction of the previous gradient to the current one in order\\nto smooth the descent, while an L2 penalty on the learned\\nparameters discourages neurons to specialize too much on\\nparticular training cases [30]. The weights of these regularizers\\nare set to 0.9 and 0.0002 respectively. Further details on these\\nso-called hyperparameters and rationale for selecting them are\\nprovided by Mnih [29].\\nTo evaluate the accuracy of the classiﬁcation we use two\\ndifferent measures: pixelwise accuracy (proportion of correctly\\nclassiﬁed pixels, obtained through binary classiﬁcation of the\\noutput probabilities with threshold 0.5) and area under the\\nreceiver operating characteristics (ROC) curve [35]. The latter\\nquantiﬁes the relation between true and false positives at\\ndifferent thresholds, and is appropriate to evaluate the overall\\nquality of the fuzzy maps.Fig. 5(a) plots the evolution of the area under ROC curve\\nand pixelwise accuracy in the test set, across iterations.\\nThe FCN consistently outperforms the patch-based network.\\nFig. 5(b) shows ROC curves for the ﬁnal networks after\\nconvergence, the FCN exhibiting the best relation between true\\nand false positive rates. Fig. 4(c-d) depicts some visual results.\\nTo further illustrate the beneﬁts of neural networks over\\nother learning approaches we train a support vector machine\\n(SVM) with Gaussian kernel on 1,000 randomly selected\\npixels of each class. We train on the individual pixel spec-\\ntra without any feature selection. The SVM parameters are\\nselected by 5-fold cross-validation, as commonly performed\\nin remote sensing image classiﬁcation [10]. As shown by\\nFig. 4(e), the pixelwise SVM classiﬁcation often confuses\\nroads with buildings due to the fact that their colors are similar,\\nwhile neural networks better infer and separate the classes by\\ntaking into account the geometry of the context. The accuracy\\nof the SVM on the Boston test dataset is 0.6229 and its area\\nunder ROC curve is 0.5193, i.e., signiﬁcantly lower than with\\nCNNs, as shown in Fig. 5. If we wished to successfully use an\\nSVM for this task, we should design and select spatial features\\n(e.g., texture) and use them as the input to the classiﬁer instead.\\nThe ampliﬁed fragment in Fig. 2 shows that the border\\ndiscontinuity artifacts present in the patch-based scheme are\\nabsent in our fully convolutional setting. This behaves as\\nexpected considering that the issues described in Section III-B\\nare addressed by construction in the new architecture. This\\nconﬁrms that imposing sensible restrictions to the connections\\nof a neural network has a positive impact in the performance.\\nIn terms of efﬁciency the FCN also outperforms the patch-\\nbased CNN. At inference time, instead of carrying out the\\nprediction in a small patch basis, the input of the FCN is\\nsimply increased to output larger predictions, better beneﬁting\\nfrom the GPU parallelization of convolutions. The execution\\ntime required to classify the whole Boston 22.5 km2test set\\n(performed on an Intel I7 CPU @ 2.7Ghz with a Quadro\\nK3100M GPU) is 82.21 s with the patch-based CNN against\\n8.47 s with the FCN. The speedup is about 10x, a relevant im-\\nprovement considering the large-scale processing capabilities\\nrequired by new sensors.\\nIV. E ND-TO-ENDFRAMEWORK\\nIn remote sensing image analysis it is a common practice to\\ntrain classiﬁers on the spectrum of a small number (a couple\\nof hundreds) of isolated sample pixels [36]. Training relies\\non the trustworthiness of the reference data and on the fact\\nthat classes are reliably separable simply by observing the\\nspectral signature of the sampled pixels. While such training\\napproaches are popular, for example, in hyperspectral image\\nclassiﬁcation, our goals differ as we wish to automatically\\nlearn contextual features that can help better identify the\\nclasses in satellite imagery. Our goal requires more training\\ndata per se , as we must show the classiﬁer the many different\\ncontexts in which a pixel class can be embedded, and not just\\nits spectral values. In addition, is it well-known that massive\\ndata might be required to train neural networks, contrary to\\na common feature selection and classiﬁcation approach. ThisIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 7\\n(a) Color image\\n (b) Reference data\\n (c) Patch-based fuzzy map\\n (d) FCN fuzzy map\\n (e) SVM fuzzy map\\nFig. 4: Experimental results on a fragment of the Boston dataset.\\n0.5 1 1.5 2 2.5 3\\nx 1040.780.80.820.840.860.880.90.920.94\\nIterationsArea under ROC curve\\n  \\nFully convolutional\\nPatch−based\\n0.5 1 1.5 2 2.5 3\\nx 1040.80.820.840.860.880.90.92\\nIterationsPixel−wise accuracy\\n  \\nFully convolutional\\nPatch−based\\n(a) Performance evolution\\n0 0.1 0.2 0.3 0.4 0.50.50.60.70.80.91\\nFalse Positive rateTrue Positive rate\\n  \\nPatch−based\\nFully convolutional(b) ROC curves\\nFig. 5: Evaluation of patch-based and fully convolutional neural networks on the Boston test set.\\nled us to analyze and address the dependency of the algorithm\\non the availability and accuracy of the training data.\\nIn the experiments described in Section III-D, the Mas-\\nsachusetts Buildings dataset is used for training and testing.\\nThis dataset is a hand-corrected version of the OpenStreetMap\\n(OSM) vectorial map available over the area covered by\\nthe images. Despite the existence of some inaccuracies in\\nthe reference data, the coverage of OSM in that region is\\nsatisfactory and the errors are minor.\\nIn many other areas of Earth, however, the coverage of OSM\\nis limited. In the samples of Fig. 8 we observe large areas\\nwith missing data and a general misregistration of the vectorial\\nmaps with respect to the actual structures. In addition, the\\nmisregistration is not uniform and neighboring buildings are\\noften shifted in different directions. Note that in the samples of\\nFig. 8 the buildings have been delineated in OSM based on the\\nofﬁcial French cadaster records. However, even the cadaster\\nrecords are not always accurate up to the meter resolution.\\nFurthermore, satellite images undergo a series of corrections\\nbefore being aligned to the maps. For example, the use of\\ninexact elevation models for orthorectiﬁcation might introduce\\nmisregistrations throughout the images. As a result, the OSM\\nraw data is imperfect and thus not fully reliable.\\nThe reference data obtained from OSM, as shown by Fig. 8,\\nprovides a rough idea of the location of the buildings, but\\nrarely outlines them. In such a setting, convolutional neural\\nnetworks would hardly learn that building boundaries are\\nlikely to fall on visible edges, since this is not what the refer-\\nence data depicts. Under these circumstances, we expect the\\npredictions not to be very conﬁdent, especially on the border of\\nthe objects. As we will illustrate in Section IV-C, this yields a“blobby” and overly fuzzy aspect to predictions obtained with\\nthe network of Section III-C on more challenging datasets.\\nOur ﬁrst contribution in this section is a novel approach for\\ntackling the issue of inaccurate labels for CNN training. For\\nthis we propose a two-step approach: 1) the network is ﬁrst\\ntrained on raw OSM data, 2) it is then ﬁne-tuned on a tiny\\npiece of manually labeled image.\\nThis method provides us with a means to deal with the\\ninaccuracy of training data, by increasing the conﬁdence\\nand sharpness of the predictions. However, we still cannot\\nexpect it to provide highly precise boundaries with the fully\\nconvolutional architecture as described in Section III-C. This\\nis because such network includes a downsampling step, re-\\nquired to capture the long-range spatial dependencies that\\nhelp recognize the classes. However, downsampling makes\\nthe whole system lose spatial precision, and the deconvo-\\nlutional layer learns a way of naively upsampling the data\\nfrom a restricted number of neighbors, without reincorporating\\nhigher-resolution information. What is lost in spatial precision\\nthrough the network, is not recovered. This is a consequence\\nof a well-known trade-off between the receptive ﬁeld (how\\nmuch context is taken to conduct predictions) and the output\\nresolution (how ﬁne is the prediction) if we wish to keep a\\nreasonable number of trainable parameters [33]. Our second\\ncontribution is then a new architecture that incorporates infor-\\nmation at multiple scales in order to alleviate this trade-off.\\nOur architecture combines low-resolution long-range features\\nwith high-resolution local features that conduct predictions\\nwith a higher level of detail. This architecture, when combined\\nwith our two-step training approach, provides a framework that\\ncan be used end-to-end to classify satellite imagery.IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 8\\nA. Fine-tuning\\nFine-tuning is a very common procedure in the neural\\nnetwork literature. The idea is to adapt an existing pretrained\\nmodel to a different domain by executing a few training iter-\\nations on a new dataset. The notion of ﬁne-tuning is based on\\nthe intuition that low-level information/features can be reused\\nin different applications, without training from scratch. Even\\nwhen the ﬁnal classiﬁcation objective is different, it is also\\na relevant approach for initializing the learnable parameters\\nclose to good local minima, instead of initializing with random\\nweights. After proper ﬁne-tuning, low-level features tend to be\\nquite preserved from one dataset to another, while the higher\\nlayers’ parameters are updated to adapt the network to the new\\nproblem [37].\\nWhen ﬁne-tuning, the training set for the new domain is usu-\\nally substantially smaller than the one used to train the original\\nnetwork. This is because one assumes that some generalities\\nof both domains are well conveyed in the pretrained network\\n(e.g., edge detectors in different directions) and the ﬁne-tuning\\nphase is just needed to conduct the domain adaptation. When\\nthe training set used for ﬁne-tuning is very small, additional\\nconsiderations to avoid overﬁtting are commonly taken, such\\nas early stopping (executing just a few iterations on the new\\ntraining dataset), ﬁxing the weights at the lower layers or\\nreducing the learning rate.\\nWe now incorporate the idea of neural network ﬁne-tuning,\\nin order to perform training on imperfect data. Our approach\\nproceeds in two steps. In step one large amounts of training\\ndata are used to train a fully convolutional neural network.\\nThis raw training data is extracted directly from OSM, without\\nany hand correction. The goal of this step is to capture the\\ngeneralities of the dataset such as, e.g., the representative\\nspectrum of object classes.\\nIn step two, we ﬁne-tune the network by using a small\\npart of carefully labeled image. This phase is designed to\\ncompensate for the inaccuracy of labels obtained in step one,\\nby ﬁne-tuning the network on small yet consistent target\\noutputs. Assuming that most of the generalities have been\\ncaptured during the initial training step, the ﬁne-tuning step\\nshould locally correct the network parameters to output more\\naccurate classiﬁcations. The efforts of ﬁne-tuning are thus\\nlimited to manually labeling a small dataset, while the large\\ninaccurate dataset is automatically extracted from OSM.\\nB. Conducting Fine Predictions\\nThe resolution at which the networks proposed in Section III\\noperate yields probability maps that, once upsampled, are\\ncoarse in terms of spatial accuracy. A naive way to increase\\nthe resolution of the network would be to use higher-resolution\\nﬁlters, which requires to increase their dimensions if we\\nwant to preserve the receptive ﬁeld. For example, instead of\\napplying a 5\\x025ﬁlter at a fourth of the image resolution, one\\ncould use a 20\\x0220ﬁlter at full resolution, hence covering the\\nsame spatial extent. However, such an increase in ﬁlter sizes\\nis prohibitive, hampering the spatial and temporal efﬁciency\\nof the algorithm and producing less accurate results due to the\\ndifﬁculty of optimizing so many parameters.Nevertheless, we observed that we do not need full-\\nresolution ﬁlters to conduct accurate predictions. One requires\\na higher resolution only in the center of the convolution\\nﬁlters (assuming that the pixel we wish to predict is in the\\ncenter of the context of interest). A large spatial extent is\\nindeed required to capture contextual information, but it is\\nnot necessary to conduct this analysis at full resolution. For\\nexample, the presence of two parallel bands of grass can help\\nidentify a road (and distinguish it from, for instance, a building\\nwith a gray rooftop), but a precise localization of the grass is\\nnot necessary. On the contrary, at the center of the convolution\\nﬁlter, a higher-resolution analysis is required to speciﬁcally\\nlocate the boundary of the aforementioned road.\\nFig. 6 illustrates this observation. In Fig. 6(a) we observe\\nthe area around a pixel whose class we wish to predict, at full\\nresolution. A ﬁlter taking such an amount of context with that\\nresolution would be prohibitive in the number of parameters,\\nas well as unnecessary. Fig. 6(b) depicts the same context\\nat a quarter of the resolution. Notice that it is still possible\\nto visually infer that there is a road. However, identifying\\nthe precise location of the boundaries of the road becomes\\ndifﬁcult. Alternatively, Fig. 6(c) depicts a small patch but at\\nfull resolution. We can now better locate the precise boundary\\nof the object, but with so little context it is difﬁcult to identify\\nthat the object is indeed a road. Large ﬁlters at low resolution\\n- see Fig. 6(b) or small ﬁlters at high resolution - see Fig. 6(c),\\nwhich would both have a reasonable number of parameters,\\nare bad alternatives: the ﬁrst ﬁlter is too coarse and the second\\nﬁlter is using too little context.\\nWe propose convolutional ﬁlters that combine multiple\\nscales instead. In Fig. 6(d) the large-size low-resolution con-\\ntext of Fig. 6(b) is combined with the small high-resolution\\ncontext of 6(d). This provides us with a means to simultane-\\nously infer the class by observing the surroundings at a coarse\\nscale, and determine the precise boundary location by using\\na ﬁner context. This way, the amount of parameters are kept\\nsmall while the trade-off between recognition and localization\\nis alleviated.\\nLes us denote bySa set of levels of detail expressed as a\\nfraction of the original resolution. For example, S=f1;1=2g\\nis a set comprising two-scales: full resolution and half of the\\nfull resolution. We denote by xsa feature map xdownsampled\\nto a certain level s2S. For example, x1=2is a feature map\\ndownsampled to half of the original resolution. Inspired in\\nEquation 1, we design a special type of neuron that adds the\\nresponses to a set of ﬁlters applied at different scales of the\\nfeature maps in the previous layer:\\na=\\x1b X\\ns2Swsxs+b!\\n: (5)\\nNotice that individual ﬁlters wsare learned for every scale\\ns. Such a ﬁlter is easily implemented by using a combination\\nof elementary convolutional, downsampling and upsampling\\nlayers. Fig. 7 illustrates this process in the case of a two-\\nscale (S=f1;1=2g) module. In our implementation we\\naverage neighboring elements in a window for downsampling\\nand perform bilinear interpolation for upsampling, but otherIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 9\\n(a) Large context, high res-\\nolution\\n(b) Large context, low res-\\nolution\\n(c) Small context, high res-\\nolution\\n(d) Combined scales\\nFig. 6: Different types of context to predict a pixel’s class. A multi-scale context such as in (d) alleviates the trade-off between\\nclassiﬁcation accuracy and number of learnable parameters.\\nFig. 7: Two-scale convolutional module that simultaneously\\ncombines coarse large-range and ﬁne short-range reasoning.\\nFig. 8: Fragments of the Forez training set (red: building ).\\napproaches are also applicable. The kernel sizes of the con-\\nvolutions at both scales are set to be equal (e.g., 3\\x023), yet\\nthe amount of context taken varies from one path to the other\\ndue to the different scales. The addition is an elementwise\\noperation, followed by the nonlinear activation function.\\nC. Experiments on the End-to-End Classiﬁcation Framework\\nWe conduct our experiments on a Pl ´eiades image over the\\narea of Forez, France. An orthorectiﬁed color pansharpened\\nversion of the image is used, at a spatial resolution of 0.5\\nm2. Our training subset amounts to 22.5 km2. The criterion to\\nconstruct the training set was to choose ten 3000\\x023000 tiles\\nwith at least some coverage of OpenStreetMap (OSM). The\\nshape ﬁles were rasterized with GDAL1to create the binary\\nreference maps. Fig. 8 shows some fragments of the reference\\ndata. Inconsistent misregistrations and considerable omissions\\nare observed all over.\\n1http://www.gdal.org\\nFig. 9: Manually labeled tile for ﬁne-tuning (3000 \\x023000).\\nFig. 10: Fragment of the ﬁne-tuning tile. Red borders enclose\\nbuilding areas.\\nWe manually labeled a 2.25 km2tile for FCN ﬁne-tuning,\\nand a different 2.25 km2tile for testing. The manual labeling\\ntakes about two hours for each of the tiles. The entire tuning\\ntile is depicted by Fig. 9 and a close-up is shown in Fig. 10.\\nThe fully convolutional network (FCN) described in Section\\nIII-C, which was used for the Massachusetts dataset, is now\\ntrained with the Forez set, under a similar experimental setting.\\nNote that this FCN was designed for images which have a\\n1 m2resolution, while Pl ´eiades imagery features a 0.5 m2\\nresolution. In order for the architectural decisions of FCN to\\nbe valid in our new dataset, one must preserve the receptive\\nﬁeld size in terms of meters, not pixels. We thus downsample\\nPl´eiades images prior to entering the ﬁrst layer of the FCN,\\nand bilinearly upsample the output classiﬁcation maps. Even\\nthough a new network directly tailored to the Pl ´eiades reso-\\nlution could be designed, we favor this proven architecture to\\nconduct our experiments. The concepts described in this paperIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 10\\nMethod Accuracy AUC IoU\\nFCN 0.99126 0.99166 0.48\\nFCN + Fine-tuning 0.99459 0.99699 0.66\\nTwo-scale FCN 0.99129 0.98154 0.47\\nTwo-scale FCN + Fine-tuning 0.99573 0.99836 0.72\\nTABLE I: Performance evaluation on the Pl ´eiades test set.\\nare however general and can be used to design other networks.\\nAfter training on the raw OSM Forez dataset, we ﬁne-tune\\nthe weights on the manually labeled tuning tile. The training\\nhyper-parameters are kept similar in the ﬁne-tuning step, but\\nan early stopping criterion interrupts it after 200 iterations.\\nTo assess the performance of ﬁne-tuning we use as criteria\\npixelwise accuracy and area under the ROC curve (AUC), as\\ndescribed in Section III-D. Since there are many more non-\\nbuilding pixels than building pixels in this dataset, these accu-\\nracy measures might seem overly high, a well-known issue of\\npixelwise accuracy in imbalanced datsets [38]. We add then\\nthe intersection over union criterion (IoU), an object-based\\noverlap measure typically used for imbalanced datasets [38].\\nIn our case it is deﬁned as the number of pixels labeled as\\nbuildings both in the classiﬁed image and in the ground truth,\\ndivided by the total amount of pixels labeled as such in either\\nof them. These criteria are evaluated on the manually labeled\\ntest set, which is used neither for training nor for ﬁne-tuning.\\nThe ﬁrst two rows of Table I show that ﬁne-tuning enhances\\nthe quality of the predictions in terms of accuracy, AUC and\\nIoU. To conﬁrm the signiﬁcance of the accuracy, a McNemar’s\\ntest [39] proved that the improvement is not a result of mere\\nluck with a probability greater than 0.99999. Besides, the IoU\\nis improved by over a third with the ﬁne-tuned network.\\nFig. 11(a-d) shows the impact of ﬁne-tuning on several\\nampliﬁed fragments of the test set. A greater conﬁdence in\\nthe ﬁne-tuned network predictions is observed. The objects\\nexhibit better alignment to the objects of the image, albeit the\\nboundaries could better line up to the underlying edges.\\nFig. 12 illustrate the ﬁrst-layer convolutional ﬁlters learned\\nby the initial and ﬁne-tuned networks. We observe a combina-\\ntion of low- and high-frequency ﬁlters, a behavior typically\\nobserved in CNNs. We also observe edge and color blob\\ndetectors. These ﬁlter remain unchanged after ﬁne-tuning, even\\nthough no constraints are introduced to enforce this. Fine-\\ntuning corrects the weights in the high-level layers, which\\nsuggests that the initial low-level features were useful indeed,\\nbut the inaccuracy in the labels was introducing fuzziness in\\nthe upper layers of the network.\\nWe now evaluate the performance of a two-scale network.\\nThe FCN architecture described in Section III-C is replaced\\nby three two-scale stacked modules, with scales S=f1;1=4g.\\nWe selectS= 1=4as it corresponds to the degree of\\ndownsampling of the original FCN network, and S= 1 is\\nadded to reﬁne the predictions. The three modules learn 3\\x023\\nﬁlters in both scales. The ﬁrst two modules generate 64 feature\\nmaps and the last module generates a single map with the\\nbuilding/non-building prediction.\\nThe two-scale network is trained and ﬁne-tuned in a similar\\nsetting as the FCN network. The results summarized in the\\nlast two rows of Table I show that ﬁne-tuning signiﬁcantly\\nFig. 12: First layer ﬁlters before and after ﬁne-tuning.\\nenhances the classiﬁcation performance, and that the ﬁne-\\ntuned two-scale network outperforms the single scale network.\\nNotably, IoU goes from 0.48 to 0.72, implying that objects\\noverlap with the ground truth 50% better by adding a scale\\nand performing ﬁne-tuning. Note that if a scale is added but\\nno ﬁne-tuning is done, there is actually a slight decrease in\\nperformance. A possible explanation for this is that including a\\nﬁner scale adds even more confusion to the training algorithm\\nif only noisy misregistered labels are provided.\\nFigs. 11(e-f) illustrate the results on visual fragments of\\nthe test set. The two-scale network yields classiﬁcation maps\\nthat better correspond to the actual image objects, and exhibit\\nsharper angles and straighter lines. The entire classiﬁed test tile\\nfor the ﬁne-tuned two-scale network is depicted by Fig. 13c.\\nThe time required to generate this result corresponds to three\\nhours for training on the OSM dataset, two hours to manually\\nlabel an image tile and about a minute for ﬁne-tuning. The\\nprediction of the 3000\\x023000 test tile using the hardware\\ndescribed in Section III-D takes 3:2seconds, and it grows\\nlinearly in the size of the image. As in Section III-D, we ran\\nan SVM on the individual pixel values (see the classiﬁcation\\nmap in Fig. 13b). Accuracy is 0.9487 and IoU 0.19, yielding\\npoorer results than the presented CNN-based approaches.\\nAs validated by the experiments, the issue of not having\\nlarge amounts of high-quality reference data can be alleviated\\nby providing the network with a small amount of accurate\\ndata in a ﬁne-tuning step. Our multi-scale neurons combine\\nreasoning at different resolutions to effectively produce ﬁne\\npredictions, while keeping a reasonable number of parameters.\\nSuch a framework can be used end-to-end to perform the\\nclassiﬁcation task directly from input imagery. More scales\\ncan be easily be added and, besides the fact of being fully\\nconvolutional, there are little constraints on the architecture\\nitself, admitting a different number of classes, input bands or\\nnumber of feature maps.\\nV. C ONCLUDING REMARKS\\nConvolutional neural networks have become a popular clas-\\nsiﬁer in the context of image analysis due to their potential\\nto automatically learn relevant contextual features. Initially\\ndevised for the categorization of natural images, these net-\\nworks must be revisited and adapted to tackle the problem of\\npixelwise labeling in remote sensing imagery.\\nWe proposed a fully convolutional network architecture by\\nanalyzing a state-of-the-art model and solving its concerns by\\nconstruction. Despite their outstanding learning capability, theIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 11\\n(a) Color image\\n (b) Reference\\n (c) FCN\\n (d) FCN + Fine-tuning\\n (e) 2-scale FCN\\n (f) 2-scale FCN +\\nFine-tuning\\nFig. 11: Classiﬁed fragments of the Pl ´eiades test image. Fine-tuning increases the conﬁdence of the predictions, and the\\ntwo-scale network produces ﬁne-grained classiﬁcation maps.IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 12\\n(a) Color pansharpened input\\n (b) SVM on individual pixels\\n (c) FCN (two scales + ﬁne-tuning)\\nFig. 13: Binary classiﬁcation maps on the Forez test image.\\nlack of accurate training data might limit the applicability of\\nCNN models in realistic remote sensing contexts. We therefore\\nproposed a two-step training approach combining the use of\\nlarge amounts of raw OpenStreetMap data and a small sample\\nof manually labeled reference. The last ingredient we needed\\nto provide a usable end-to-end framework for remote sensing\\nimage classiﬁcation was to produce ﬁne-grained classiﬁcation\\nmaps, since typical CNNs tend to hamper the ﬁneness of the\\noutput as a side effect of taking large amounts of context. We\\nproposed a type of neuron module that simultaneously reasons\\nat different scales.\\nExperiments showed that our fully convolutional network\\noutperforms the previous model in multiple aspects: the accu-\\nracy of the results is improved, the visual artifacts are removed\\nand the inference time is reduced by a factor of ten. The use of\\nour architecture constitutes then a win-win situation in which\\nno aspect is compromised for the others. This was achieved\\nby analyzing the role played by every layer in the network in\\norder to propose a more appropriate architecture, showing that\\na deep understanding of how CNNs work is important for their\\nsuccess. Further experimentation showed that the two-step\\ntraining approach effectively combines imperfect training data\\nwith manually labeled data to capture the dataset’s generalities\\nand its precise details. Moreover, the multi-scale modules\\nincrease the level of detail of the classiﬁcation without making\\nthe number of parameters explode, attenuating the trade-off\\nbetween detection and localization.\\nOur overall framework shows then that convolutional neural\\nnetworks can be used end-to-end to process large amounts of\\nsatellite images and provide accurate pixelwise classiﬁcations.\\nAs future work we plan to extend our experiments to\\nmultiple object classes and study the possibility of directly\\ninputting non-pansharpened imagery, in order to avoid this\\npreprocessing step. We also plan to study the introduction of\\nshape priors in the learning process and the vectorization of\\nthe classiﬁcation maps.\\nACKNOWLEDGMENT\\nAll Pl ´eiades images are c\\rCNES (2012 and 2013), distri-\\nbution Airbus DS / SpotImage. The authors thank CNES for\\ninitializing and funding the study, and providing Pl ´eiades data.REFERENCES\\n[1] Yann LeCun, L ´eon Bottou, Yoshua Bengio, and Patrick Haffner,\\n“Gradient-based learning applied to document recognition,” Proceedings\\nof the IEEE , vol. 86, no. 11, pp. 2278–2324, 1998.\\n[2] Dan Ciresan, Ueli Meier, and J ¨urgen Schmidhuber, “Multi-column deep\\nneural networks for image classiﬁcation,” in IEEE CVPR , 2012, pp.\\n3642–3649.\\n[3] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton, “Imagenet\\nclassiﬁcation with deep convolutional neural networks,” in NIPS , 2012.\\n[4] Annamalai Senthil Kumar and Kantilal Majumder, “Information fusion\\nin tree classiﬁers,” International Journal of Remote Sensing , vol. 22,\\nno. 5, pp. 861–869, 2001.\\n[5] Jean Mas and Juan Flores, “The application of artiﬁcial neural networks\\nto the analysis of remotely sensed data,” International Journal of Remote\\nSensing , vol. 29, no. 3, pp. 617–663, 2008.\\n[6] Thomas Villmann, Erzsbet Mernyi, and Barbara Hammer, “Neural maps\\nin remote sensing image analysis,” Neural Networks , vol. 16, no. 34,\\npp. 389 – 403, 2003, Neural Network Analysis of Complex Scientiﬁc\\nData: Astronomy and Geosciences.\\n[7] Gustavo Camps-Valls and Lorenzo Bruzzone, “Kernel-based methods\\nfor hyperspectral image classiﬁcation,” IEEE Tran. Geosci. Remote\\nSens. , vol. 43, no. 6, pp. 1351–1362, 2005.\\n[8] Mathieu Fauvel, Yuliya Tarabalka, Jon Atli Benediktsson, Jocelyn\\nChanussot, and James C Tilton, “Advances in spectral-spatial classi-\\nﬁcation of hyperspectral images,” Proceedings of the IEEE , vol. 101,\\nno. 3, pp. 652–675, 2013.\\n[9] Wenzhi Liao, Mauro Dalla Mura, Jocelyn Chanussot, Rik Bellens,\\nand Wilfried Philips, “Morphological attribute proﬁles with partial\\nreconstruction,” IEEE Tran. Geosci. Remote Sens. , vol. 54, no. 3, pp.\\n1738–1756, 2016.\\n[10] Yuliya Tarabalka and Aakanksha Rana, “Graph-cut-based model for\\nspectral-spatial classiﬁcation of hyperspectral images,” in IEEE IGARSS .\\nIEEE, 2014, pp. 3418–3421.\\n[11] Mehmet Nadir Kurnaz, Zmray Dokur, and Tamer lmez, “Segmentation\\nof remote-sensing images by incremental neural network,” Pattern\\nRecognition Letters , vol. 26, no. 8, pp. 1096 – 1104, 2005.\\n[12] Christopher David Lloyd, Suha Berberoglu, Paul Curran, and Peter\\nAtkinson, “A comparison of texture measures for the per-ﬁeld classi-\\nﬁcation of mediterranean land cover,” International Journal of Remote\\nSensing , vol. 25, no. 19, pp. 3943–3965, 2004.\\n[13] Dengsheng Lu and Qihao Weng, “A survey of image classiﬁcation\\nmethods and techniques for improving classiﬁcation performance,” In-\\nternational journal of Remote sensing , vol. 28, no. 5, pp. 823–870, 2007.\\n[14] ME Midhun, Sarath R Nair, VT Prabhakar, and S Sachin Kumar, “Deep\\nmodel for classiﬁcation of hyperspectral image using restricted boltz-\\nmann machine,” in Proceedings of the 2014 International Conference\\non Interdisciplinary Advances in Applied Computing . ACM, 2014, p. 35.\\n[15] Tong Li, Junping Zhang, and Ye Zhang, “Classiﬁcation of hyperspectral\\nimage based on deep belief networks,” in IEEE ICIP , 2014.\\n[16] Viktor Slavkovikj, Steven Verstockt, Wesley De Neve, Soﬁe Van Hoecke,\\nand Rik Van de Walle, “Hyperspectral image classiﬁcation with convo-\\nlutional neural networks,” in Proceedings of the 23rd ACM international\\nconference on Multimedia . ACM, 2015, pp. 1159–1162.IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 13\\n[17] Yushi Chen, Xing Zhao, and Xiuping Jia, “Spectral-spatial classiﬁcation\\nof hyperspectral data based on deep belief network,” IEEE J. Sel. Topics\\nAppl. Earth Observ. in Remote Sens. , vol. 8, no. 6, June 2015.\\n[18] Yushi Chen, Zhouhan Lin, Xing Zhao, Gang Wang, and Yanfeng Gu,\\n“Deep learning-based classiﬁcation of hyperspectral data,” IEEE J. Sel.\\nTopics Appl. Earth Observ. in Remote Sens. , vol. 7, no. 6, pp. 2094–\\n2107, 2014.\\n[19] Jun Yue, Wenzhi Zhao, Shanjun Mao, and Hui Liu, “Spectral–spatial\\nclassiﬁcation of hyperspectral images using deep convolutional neural\\nnetworks,” Remote Sensing Letters , vol. 6, no. 6, pp. 468–477, 2015.\\n[20] Konstantinos Makantasis, Konstantinos Karantzalos, Anastasios\\nDoulamis, and Nikolaos Doulamis, “Deep supervised learning for\\nhyperspectral data classiﬁcation through convolutional neural networks,”\\ninIEEE IGARSS . IEEE, 2015, pp. 4959–4962.\\n[21] Wenzhi Zhao and Shihong Du, “Spectral–spatial feature extraction\\nfor hyperspectral image classiﬁcation: A dimension reduction and deep\\nlearning approach,” IEEE Tran. Geosci. Remote Sens. , vol. 54, no. 8,\\npp. 4544–4554, 2016.\\n[22] Wenzhi Zhao, Zhou Guo, Jun Yue, Xiuyuan Zhang, and Liqun Luo,\\n“On combining multiscale deep learning features for the classiﬁcation of\\nhyperspectral remote sensing imagery,” International Journal of Remote\\nSensing , vol. 36, no. 13, pp. 3368–3379, 2015.\\n[23] Wenzhi Zhao and Shihong Du, “Learning multiscale and deep repre-\\nsentations for classifying remotely sensed imagery,” ISPRS Journal of\\nPhotogrammetry and Remote Sensing , vol. 113, pp. 155–165, 2016.\\n[24] Essa Basaeed, Harish Bhaskar, Paul Hill, Mohammed Al-Mualla, and\\nDavid Bull, “A supervised hierarchical segmentation of remote-sensing\\nimages using a committee of multi-scale convolutional neural networks,”\\nInternational Journal of Remote Sensing , vol. 37, no. 7, 2016.\\n[25] Jun Wang, Jingwei Song, Mingquan Chen, and Zhi Yang, “Road network\\nextraction: a neural-dynamic framework based on deep learning and a\\nﬁnite state machine,” International Journal of Remote Sensing , vol. 36,\\nno. 12, pp. 3144–3169, 2015.\\n[26] Xueyun Chen, Shiming Xiang, Cheng-Lin Liu, and Chun-Hong Pan,\\n“Vehicle detection in satellite images by hybrid deep convolutional\\nneural networks,” IEEE Geoscience and remote sensing letters , vol.\\n11, no. 10, pp. 1797–1801, 2014.\\n[27] Igor ˇSevo and Aleksej Avramovi ´c, “Convolutional neural network based\\nautomatic object detection on aerial images,” IEEE Geoscience and\\nRemote Sensing Letters , vol. 13, no. 5, pp. 740–744, 2016.\\n[28] FPS Luus, BP Salmon, F Van Den Bergh, and BTJ Maharaj, “Multiview\\ndeep learning for land-use classiﬁcation,” IEEE Geoscience and Remote\\nSensing Letters , vol. 12, no. 12, pp. 2448–2452, 2015.\\n[29] V olodymyr Mnih, Machine learning for aerial image labeling , Ph.D.\\nthesis, University of Toronto, 2013.\\n[30] Christopher M Bishop, Neural networks for pattern recognition , Oxford\\nuniversity press, 1995.\\n[31] Julien Michel, David Yousseﬁ, and Manuel Grizonnet, “Stable mean-\\nshift algorithm and its application to the segmentation of arbitrarily large\\nremote sensing images,” IEEE Tran. Geosci. Remote Sens. , vol. 53, no.\\n2, pp. 952–964, 2015.\\n[32] Pierre Lassalle, Jordi Inglada, Julien Michel, Manuel Grizonnet, and\\nJulien Malik, “A scalable tile-based framework for region-merging\\nsegmentation,” IEEE Tran. Geosci. Remote Sens. , vol. 53, no. 10, pp.\\n5473–5485, 2015.\\n[33] Jonathan Long, Evan Shelhamer, and Trevor Darrell, “Fully convolu-\\ntional networks for semantic segmentation,” in CVPR , 2015.\\n[34] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan\\nLong, Ross Girshick, Sergio Guadarrama, and Trevor Darrell, “Caffe:\\nConvolutional architecture for fast feature embedding,” arXiv preprint\\narXiv:1408.5093 , 2014.\\n[35] C `esar Ferri, Jos ´e Hern ´andez-Orallo, and Peter A Flach, “A coherent\\ninterpretation of AUC as a measure of aggregated classiﬁcation perfor-\\nmance,” in ICML , 2011.\\n[36] Yuliya Tarabalka, Mathieu Fauvel, Jocelyn Chanussot, and J ´on Atli\\nBenediktsson, “Svm-and mrf-based method for accurate classiﬁcation of\\nhyperspectral images,” Geoscience and Remote Sensing Letters, IEEE ,\\nvol. 7, no. 4, pp. 736–740, 2010.\\n[37] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson, “How\\ntransferable are features in deep neural networks?,” in NIPS , 2014.\\n[38] Gabriela Csurka, Diane Larlus, Florent Perronnin, and France Meylan,\\n“What is a good evaluation measure for semantic segmentation?.,” in\\nBMVC , 2013, vol. 27, p. 2013.\\n[39] Quinn McNemar, “Note on the sampling error of the difference between\\ncorrelated proportions or percentages,” Psychometrika , vol. 12, no. 2,\\npp. 153–157, 1947.\\nEmmanuel Maggiori received the Engineering de-\\ngree in computer science from Central Buenos Aires\\nProvince National University (UNCPBA), Tandil,\\nArgentina, in 2014. The same year he joined\\nAYIN and STARS teams at Inria Sophia Antipolis-\\nM´editerran ´ee as a research intern in the ﬁeld of\\nremote sensing image processing. Since 2015, he\\nhas been working on his Ph.D. within TITANE team,\\nstudying machine learning techniques for large-scale\\nprocessing of satellite imagery.\\nYuliya Tarabalka (S’08–M’10) received the B.S.\\ndegree in computer science from Ternopil Ivan\\nPul’uj State Technical University, Ukraine, in 2005\\nand the M.Sc. degree in signal and image processing\\nfrom the Grenoble Institute of Technology (INPG),\\nFrance, in 2007. She received a joint Ph.D. degree\\nin signal and image processing from INPG and in\\nelectrical engineering from the University of Iceland,\\nin 2010.\\nFrom July 2007 to January 2008, she was a\\nresearcher with the Norwegian Defence Research\\nEstablishment, Norway. From September 2010 to December 2011, she was a\\npostdoctoral research fellow with the Computational and Information Sciences\\nand Technology Ofﬁce, NASA Goddard Space Flight Center, Greenbelt, MD.\\nFrom January to August 2012 she was a postdoctoral research fellow with\\nthe French Space Agency (CNES) and Inria Sophia Antipolis-M ´editerran ´ee,\\nFrance. She is currently a researcher with the TITANE team of Inria Sophia\\nAntipolis-M ´editerran ´ee. Her research interests are in the areas of image\\nprocessing, pattern recognition and development of efﬁcient algorithms. She\\nis Member of the IEEE Society.\\nGuillaume Charpiat is a researcher at Inria Saclay\\n(France) in the TAO team. He studied Mathemat-\\nics and Physics at the ´Ecole Normale Sup ´erieure\\n(ENS Paris), and then Computer Vision and Machine\\nLearning (at ENS Cachan), as well as Theoretical\\nPhysics. His PhD thesis, in Computer Science, ob-\\ntained in 2006, was on the topic of distance-based\\nshape statistics for image segmentation with priors.\\nHe then spent one year at the Max-Planck Institute\\nfor Biological Cybernetics (T ¨ubingen, Germany),\\non the topics of medical imaging (MR-based PET\\nprediction) and automatic image colorization. As a researcher at Inria Sophia-\\nAntipolis (France), he worked mainly on image segmentation and optimization\\ntechniques. Now at Inria Saclay he focuses on Machine Learning, in particular\\non building a theoretical background for neural networks.\\nPierre Alliez Pierre Alliez is Senior Researcher and\\nteam leader at Inria Sophia-Antipolis - Mediterranee.\\nHe has authored scientiﬁc publications and several\\nbook chapters on mesh compression, surface recon-\\nstruction, mesh generation, surface remeshing and\\nmesh parameterization. He is an associate editor\\nof the Computational Geometry Algorithms Library\\n(http://www.cgal.org) and an associate editor of the\\nACM Transactions on Graphics. He was awarded\\nin 2005 the EUROGRAPHICS young researcher\\naward for his contributions to computer graphics\\nand geometry processing. He was co-chair of the Symposium on Geometry\\nProcessing in 2008, of Paciﬁc Graphics in 2010 and Geometric Modeling and\\nProcessing 2014. He was awarded in 2011 a Starting Grant from the European\\nResearch Council on Robust Geometry Processing.', 'Deep Learning Based Large-Scale Automatic\\nSatellite Crosswalk Classiﬁcation\\nRodrigo F. Berriel, Andr ´e Teixeira Lopes, Alberto F. de Souza, and Thiago Oliveira-Santos\\nAbstract —High-resolution satellite imagery have been increas-\\ningly used on remote sensing classiﬁcation problems. One of the\\nmain factors is the availability of this kind of data. Even though,\\nvery little effort has been placed on the zebra crossing classiﬁca-\\ntion problem. In this letter, crowdsourcing systems are exploited\\nin order to enable the automatic acquisition and annotation of a\\nlarge-scale satellite imagery database for crosswalks related tasks.\\nThen, this dataset is used to train deep-learning-based models\\nin order to accurately classify satellite images that contains or\\nnot zebra crossings. A novel dataset with more than 240,000\\nimages from 3 continents, 9 countries and more than 20 cities\\nwas used in the experiments. Experimental results showed that\\nfreely available crowdsourcing data can be used to accurately\\n(97.11%) train robust models to perform crosswalk classiﬁcation\\non a global scale.\\nIndex Terms —Zebra crossing classiﬁcation, crosswalk classiﬁ-\\ncation, large-scale satellite imagery, deep learning\\nI. I NTRODUCTION\\nZEBRA crossing classiﬁcation and detection are important\\ntasks for mobility autonomy. Even though, there are\\nfew data available on where crosswalks are in the world.\\nThe automatic annotation of crosswalks’ locations worldwide\\ncan be very useful for online maps, GPS applications and\\nmany others. In addition, the availability of zebra crossing\\nlocations on these applications can be of great use to people\\nwith disabilities, to road management, and to autonomous\\nvehicles. However, automatically annotating this kind of data\\nis a challenging task. They are often aging (painting fading\\naway), occluded by vehicle and pedestrians, darkened by\\nstrong shadows, and many other factors.\\nA common approach to tackle these problems is using the\\ncamera on the phones to help the visually impaired people.\\nIvanchenko et al. [1] developed a prototype of a cell phone\\napplication that determines if any crosswalk is visible and\\naligns the user to it. Their system requires some thresholds to\\nbe tuned, which may decrease performance delivered off-the-\\nshelf. Another approach is a camera mounted on a car, usually\\naiming driver assistance systems. Haselhoff and Kummert [2]\\npresented a strategy based on the detection of line segments.\\nAs discussed by the authors, their method is constrained\\nto crosswalks perpendicular to the driving direction. Both\\nperspectives (from a person [1] or a car [2]) present a known\\nlimitation: there is a maximum distance in which crosswalks\\ncan be detected. Moreover, these images are quite different\\nfrom those used in the work hereby proposed, i.e., satellite\\nimagery. Nonetheless, Ahmetovic et al. [3] presented a method\\ncombining both perspectives. Their algorithm searches for ze-\\nbra crossings in satellite images. Subsequently, the candidatesare validated against Google Street View images. One of the\\nlimitations is that the precision of the satellite detection step\\nalone, as the one proposed in this work, is \\x1920% lower than\\nthe combined algorithm. In addition, their system takes 180ms\\nto process a single image. Banich [4] presented a neural-\\nnetwork-based model, in which the author used 11 features of\\nstripelets, i.e., pair of lines. The author states that the proposed\\nmodel cannot deal with crosswalks affected by shadows and\\ncan only detect white crosswalks. The aforementioned meth-\\nods [1]–[4] were evaluated on manually annotated datasets that\\nwere usually local (i.e., within a single city or a country) and\\nsmall (from 30 to less than 700 crosswalks).\\nAnother approach that has been increasingly investigated is\\nthe use of aerial imagery, specially satellite imagery. Heru-\\nmurti et al. [5] employed a circle mask template matching\\nand SURF method to detect zebra crossing on aerial images.\\nTheir method was validated on a single region in Japan and\\nthe method that detects most crosswalks took 739.2 seconds\\nto detect 306 crosswalks. Ghilardi et al. [6] presented a model\\nto classify crosswalks in order to help the visually impaired.\\nTheir model is based on an SVM classiﬁer fed with manually\\nannotated crosswalk regions. As most of the other related\\nworks, the dataset used in their work is local and small (900\\nsmall patches, and 370 of crosswalks). Koester et al. [7]\\nproposed an SVM based on HOG and LBPH features to detect\\nzebra crossings in aerial imagery. Although very interesting,\\ntheir dataset (not publicly available) was gathered manually\\nfrom satellite photos, therefore it is relatively small (3119\\nzebra crossings and \\x1912500 negative samples) and local. In\\naddition, their method shows a low generalization capability,\\nbecause a model trained in one region shows low recall when\\nevaluated in another (known as cross-based protocol), e.g., the\\nrecall goes from 95.7% to 38.4%.\\nIn this letter, we present a system able to automatically\\nacquire and annotate zebra crossings satellite imagery, and\\ntrain deep-learning-based models for crosswalk classiﬁcation\\nin large scale. The proposed system can be used to automati-\\ncally annotate crosswalks worldwide, helping systems used by\\nthe visually impaired, autonomous driving technologies, and\\nothers. This system is the result of a comprehensive study.\\nThis study assesses the quality of the available data, the most\\nsuitable model and its performance in several imagery levels\\n(city, country, continent and global). In fact, this study is\\nperformed on real-world satellite imagery (almost 250,000\\nimages) acquired and annotated automatically. Given the noisy\\nnature of the data, results are also compared with human\\nannotated data. The proposed system is able to train models\\nthat achieve 97.11% on a global scale.\\nc\\r2017 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including\\nreprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any\\ncopyrighted component of this work in other works.arXiv:1706.09302v2  [cs.CV]  5 Jul 2017Crosswalk Locations \\nDefine Paths \\nRegion of Interest \\nInput ConvNet \\nYes No Positive Samples \\nNegative Samples \\n Decode and Filter \\nOSM \\nOverpass API \\nGoogle Static \\nMaps API \\nGoogle Static \\nMaps API Google Maps \\nDirections API Fig. 1. System architecture. The input is a region (red dashed rectangle) or a set of regions of interest. Firstly, known crosswalk locations (red markers)\\nare retrieved using the OpenStreetMap (OSM). Secondly, using Google Maps Directions API, paths (blue dashed arrows) between the crosswalk locations\\nare deﬁned. Thirdly, these paths are decoded and the result locations are ﬁltered (only locations within the green area are accepted) in order to decrease the\\namount of wrongly annotated images. At this point, positive and negative samples can be downloaded from Google Static Maps API. Finally, this large-scale\\nsatellite imagery is used to train Convolutional Neural Networks (ConvNets) to perform zebra crossing classiﬁcation.\\nII. P ROPOSED METHOD\\nThe system comprises two parts: Automatic Data Acquisi-\\ntion and Annotation, and Model Training and Classiﬁcation.\\nAn overview of the proposed method can be seen in the\\nFigure 1. Firstly, the user deﬁnes the regions of interest\\n(regions where he wants to download crosswalks). The region\\nof interest is given by the lower-left and the upper-right\\ncorners. After that, crosswalk locations within these regions\\nare retrieved from the OpenStreetMap1. Subsequently, using\\nthe zebra crossing locations, positive and negative images\\n(i.e., images that contain and do not contain crosswalks on\\nit, respectively) are downloaded using the Google Static Maps\\nAPI2. As the location of the crosswalks are known, the im-\\nages are automatically annotated. Finally, these automatically\\nacquired and annotate images are used to train a Convolutional\\nNeural Network from the scratch to perform classiﬁcation.\\nEach process is described in details in the following subsec-\\ntions.\\nA. Automatic Data Acquisition and Annotation\\nTo automatically create a model for zebra crossing classiﬁ-\\ncation, the ﬁrst step is the image acquisition. Initially, regions\\nof interest are deﬁned by the user. These regions can either\\nbe deﬁned manually or automatically (e.g. based on a given\\naddress, city, etc.). The region is rectangular (with the True\\nNorth up) and is deﬁned by four coordinate points: minimum\\nlatitude, minimum longitude, maximum latitude, maximum\\nlongitude (or South-West-North-East), i.e., the bottom-left and\\ntop-right corners (e.g. 40.764498, -73.981447, 40.799976, -\\n73.949402 deﬁnes the Central Park, NY , USA region). For\\neach region of interest, zebra crossing locations are retrieved\\nfrom the OpenStreetMap (OSM) using the Overpass API3.\\nRegions larger than 1/4 degree in either dimension are likely\\n1http://www.openstreetmap.org\\n2http://developers.google.com/maps/documentation/static-maps/\\n3http://wiki.openstreetmap.org/wiki/Overpass APIto be split into multiple regions, as OpenStreetMap servers\\nmay reject these requests. Even though, most of the settled\\npart of the cities around the world meets this limitation (e.g.\\nthe whole city of Niter ´oi, RJ, Brazil ﬁts into a single region).\\nTo download the zebra crossings, the proposed system uses\\nthe tag highway=crossing of the OSM, which is one\\nof the most reliable and most used tag for this purpose. In\\npossession of these crosswalk locations, the system needs to\\nautomatically ﬁnd locations without zebra crossing to serve\\nas negative samples. As simple as it may look, negative\\nsamples are tricky to ﬁnd because of several factors. One\\nof these is the relatively low coverage of the zebra crossing\\naround the world. Another is the fact that crosswalks are\\nsusceptible to changes over the years. They may completely\\nfade away, they can be removed and streets can change. Along-\\nside these potential problems, OSM is built by volunteers,\\ntherefore open to contributions that may not be as accurate\\nas expected. Altogether these factors indicate how noisy the\\nzebra crossing locations may be. Therefore, picking up no-\\ncrosswalk locations must be done cautiously. The ﬁrst step\\nto acquire good locations for the negative samples is to ﬁlter\\nonly regions that contain roads. For that, the system queries\\nGoogle Maps Directions API for directions from a crosswalk\\nto another to ensure that the points will be within a road.\\nIn order to lower the number of requests to the Google\\nMaps Directions API, our system adds 20 other crosswalks as\\nwaypoints between two crosswalk points (the API has a limit\\nof up to 23 waypoints and 20 ensures this limit). The number\\nof waypoints does not affect the accuracy performance of the\\nﬁnal system since the same images would be downloaded but\\nrequiring more time. Google Maps Directions API responds\\nto the request with an encoded polyline. The decoded polyline\\ncomprises a set of points in the requested path. The second\\nstep consists of virtually augmenting the number of locations\\nusing a ﬁxed spacing of 1:5\\x0210\\x004degrees (approximately\\n16 meters). All duplicate points are removed on the third step.\\nFinally, the system also ﬁlters out all images too close or toofar away. Too close locations may contain crosswalks and\\ncould create false positives; and too far locations may have\\nnon-annotated crosswalks and could create false negatives.\\nTherefore, locations closer than 3\\x0210\\x004degrees or farther\\nthan 6\\x0210\\x004degrees or locations outside the region requested\\nare removed to decrease the occurrence of false positives and\\nfalse negatives.\\nAfter acquiring both positive and negative sample locations,\\nthe proposed system dispatches several threads to download\\nthe images using the Google Static Maps API. Each requested\\nimage is centered on the location to be requested. Also, the\\nimages are requested with a zoom factor equal to 20 and size\\nof200\\x02225pixels. This size was empirically deﬁned to have a\\ngood trade-off between the image size and the ﬁeld of view of\\nthe area. Bigger sizes would increase the probability of having\\ncrosswalks away of the queried location. As a result, each\\nimage covers \\x1922\\x0225meters. Some positive and negative\\nsamples can be seen in the Figure 2.\\nB. Model training and classiﬁcation\\nBefore initializing the model training, an automatic pre-\\nprocessing operation is required. Every image downloaded\\nfrom the Google Static Maps API contains the Google logo\\nand a copyright message on the bottom. In order to remove\\nthese features, 25 pixels are removed from the bottom of\\neach image, cropping the original images from 200\\x02225to\\n200\\x02200. In possession of all cropped images, both positive\\nand negative samples, the training of the model can begin.\\nTo tackle this large-scale problem, the proposed system uses\\na deep-learning-based model: a Convolutional Neural Network\\n[8]. The architecture of the model used by the system is the\\nVGG [9]. It was chosen after the evaluation of three different\\narchitectures: AlexNet [10], VGG and GoogLeNet [11] with\\n5, 16, and 22 convolutional layers, respectively. All models\\nstarted from pre-trained models on the ImageNet [12], i.e.,\\nﬁne-tuning. In addition, the input images were upsampled from\\n200\\x02200 to256\\x02256 using bilinear interpolation and the\\nsubtraction of the mean of the training set was performed.\\nMore details on the training conﬁguration are described in the\\nsection III. Also, the last layer of VGG was replaced by a\\nfully-connected layer comprising two neurons with randomly\\ninitialized weights, one for each class (crosswalk or no-\\ncrosswalk), and 10 times higher learning rate when compared\\nto the previous layers (due to ﬁne-tuning).\\nIII. E XPERIMENTAL METHODOLOGY\\nIn this section, we present the methodology used to evaluate\\nthe proposed system. First, the dataset is properly introduced\\nand described. Then, the metrics used to evaluate the proposed\\nsystem are presented. Finally, the experiments are detailed.\\nA. Dataset\\nThe dataset used in this work was automatically acquired\\nand annotated using the system hereby presented. The system\\ndownloads satellite images using the Google Static Maps API\\nand acquires the annotations using the OpenStreetMap. In\\nFig. 2. First row presents positive samples with varying layouts of crosswalks.\\nSecond row has some challenging positive cases (crosswalks with strong\\nshadows, occlusions, aging, truncated, etc.). Third row presents different\\nnegative examples. Last row has some challenging negative cases.\\ntotal, the dataset comprises 245,768 satellite images, 74,047\\nimages of which contain crosswalks (positive samples) and\\n171,721 do not contain zebra crossings (negative samples).\\nTo the best of our knowledge, this is the largest satellite\\ndataset for crosswalk-related tasks in the literature. In the wild,\\ncrosswalks can vary across different cities, different countries\\nand different continents. Alongside the design variations, they\\ncan be presented in a variety of conditions (e.g. occluded\\nby trees, cars, pedestrians; with painting fading away; with\\nshadows; etc.). In order to capture all this diversity, this dataset\\ncomprises satellite imagery from 3 continents, 9 countries,\\nand at least 20 cities. The cities were chosen considering the\\ndensity of available annotations and the size of the city. It was\\ngiven preference to big cities assuming that they are better\\nannotated. In total, these images add up to approximately\\n135,000 square kilometers, even though different images may\\npartially contain a shared area. Some samples of crosswalks\\nare shown in the Figure 2. A summary of the dataset can be\\nseen at the Table I. It is worth noting that, even though each\\npart of the dataset is named after a city, some of the selected\\nregions were large enough to partially include neighboring\\ntowns. A more detailed description of each part of the dataset,\\nregion locations and scripts used for the data acquisition are\\npublicly available4.\\nTABLE I\\nNUMBER OF IMAGES ON THE DATASETS GROUPED BY CONTINENTS\\nDescription Crosswalks No-Crosswalks Total\\nEurope 42,554 99,461 142,015\\nAmerica 15,822 36,811 52,633\\nAsia 15,671 35,449 51,120\\nTotal 74,047 171,721 245,313\\nB. Metrics\\nOn this classiﬁcation task, we reported the global accuracy\\n(hits per number of images) and the F1score (harmonic mean\\nof precision and recall).\\n4http://github.com/rodrigoberriel/satellite-crosswalk-classiﬁcationC. Experiments\\nSeveral experiments were designed to evaluate the pro-\\nposed system. Initially, three well-known Convolutional Neu-\\nral Network architectures were evaluated: AlexNet, VGG and\\nGoogLeNet. The images downloaded from the Google Static\\nMaps API were upsampled from 200\\x02200to256\\x02256using\\nbilinear interpolation. As a data augmentation procedure, the\\nimages were randomly cropped ( 224\\x02224 to the VGG and\\nGoogLeNet, and 227\\x02227to the AlexNet – as in the original\\nnetworks), and the images were randomly mirrored on-the-ﬂy.\\nIt is known that crosswalks vary across different cities,\\ncountries and continents. In this context, some experiments\\nwere designed based on the expectation that crosswalks be-\\nlonging to the same locality (e.g., same city, same country,\\netc.) are more likely to present similar features. Therefore, in\\nthese experiments, some models were trained and evaluated on\\nthe same locality, and they were named with the preﬁx “intra”:\\nintra-city, intra-country, intra-continent and intra-world. At the\\nsmaller scales (e.g., city and country) only some of the datasets\\nwere used (assuming the performance may be generalized), at\\nhigher levels all available datasets were used. These datasets\\nwere randomly chosen considering all cities with high anno-\\ntation density. Ultimately, the intra-world experiment was also\\nperformed.\\nBesides these intra-based experiments, in which samples\\ntend to have high correlation (i.e., present more similarities),\\ncross-based experiments were performed. The experiments\\nnamed with the preﬁx “cross” are those in which the model\\nwas trained with a dataset and evaluated in another with\\nthe same level of locality, i.e., trained using data from a\\ncity and tested in another city. Cross-based experiments were\\nperformed on the three levels of locality: city, country and\\ncontinent. Another experiment performed was the cross-level,\\ni.e., the model trained using an upper level imagery (e.g., the\\nworld model) was evaluated in lower levels (e.g., continents).\\nAll the experiments aforementioned share a common setup.\\nEach dataset was divided into train, validation and test sets,\\nwith 70%, 10% and 20%, respectively. For all the experiments,\\nincluding the cross-based ones (cross-level included), none of\\nthe images in the test set were seen by the models during\\ntraining or validation, i.e., train, validation and test sets were\\nexclusive. In fact, the cross experiments used only the test-set\\nof the respective dataset on which they were evaluated. This\\nprocedure enables fairer comparisons and conclusions about\\nthe robustness of the models, even though the entire datasets\\ncould have been used on the cross-based models. Regarding\\nthe training, all models were trained during 30 epochs and the\\nlearning rate was decreased three times by a factor of 10. The\\ninitial learning rate was set to 10\\x004to all three networks.\\nLastly, the annotations from the OpenStreetMap may not be\\nas accurate as required due to many factors (e.g., human error,\\nzebra crossing was removed, etc.). Even though, we assume\\nthe vast majority of them are correct and accurate. To validate\\nthat, an experiment using manually labeled datasets from the\\nthree continents (America, Europe, and Asia – 44,175 images\\nin total) was performed. The experiment was designed to have\\nthree results: error of the automatic annotation; performanceincrease when switching from automatic labeled training data\\nto manually labeled; and, correlation between the error of the\\nautomatic annotation and the error of the validation results.\\nIV. R ESULTS\\nSeveral experiments were performed and their accuracy and\\nF1score were reported. Initially, different architectures were\\nevaluated on two levels of locality. As can be seen in the\\nTable II, VGG achieved the best results. It is interesting to\\nnotice that AlexNet, a smaller model that can be loaded into\\nsmaller GPUs, also achieved competitive results.\\nTABLE II\\nDIFFERENT ARCHITECTURES USING INTRA -BASED PROTOCOL\\nArchitecture Level Dataset Accuracy F1score\\nAlexNetCity Milan 96.69% 94.56%\\nCity Turim 95.39% 92.51%\\nCountry Italy 96.06% 93.53%\\nVGGCity Milan 97.00% 95.10%\\nCity Turim 96.37% 94.15%\\nCountry Italy 96.70% 94.66%\\nGoogLeNetCity Milan 96.04% 93.41%\\nCity Turim 94.27% 90.78%\\nCountry Italy 95.22% 92.17%\\nRegarding the intra-based experiments, the chosen model\\nshowed to be very consistent across the different levels of\\nlocality. The model achieved 96.9% of accuracy (on average)\\nand the details of the results can be seen in the Table III.\\nTABLE III\\nINTRA -BASED RESULTS FOR THE VGG N ETWORK\\nLevel Dataset Accuracy F1score\\nCityMilan 97.00% 95.10%\\nTurim 96.37% 94.15%\\nCountryItaly 96.70% 94.66%\\nFrance 95.87% 93.12%\\nContinentEurope 96.72% 94.50%\\nAmerica 96.77% 94.55%\\nAsia 98.61% 97.71%\\nWorld World 97.11% 95.17%\\nAs expected, the cross-based models achieved a lower over-\\nall accuracy when compared to the intra-based. Nevertheless,\\nthese models were still able to achieve high accuracies ( 94:8%\\non average, see Table IV). This can be partially explained by\\nTABLE IV\\nCROSS -BASED RESULTS FOR THE VGG N ETWORK\\nLevel Train/Val Test Accuracy F1score\\nCityMilan Turim 93.47% 89.80%\\nTurim Milan 95.40% 92.36%\\nCountryItaly France 94.78% 91.16%\\nFrance Italy 94.78% 91.61%\\nContinentEurope Asia 96.62% 94.33%\\nAsia Europe 93.65% 88.94%inherent differences on the images between places far apart,\\ni.e., different solar elevation angles cause notable differences\\non the images; the quality of the images may differ between\\ncities; among other factors that tend to be captured by the\\nmodel during the training phase.\\nCross-level experiments reported excellent results. As al-\\nready discussed, none of these models had contaminated test\\nsets. Yet, on average, they achieved 96.1% of accuracy. The\\nrobustness of these models can be seen in the Table V, where\\nall the cross-level results were summarized.\\nTABLE V\\nCROSS -LEVEL RESULTS FOR THE VGG N ETWORK\\nTRAIN /VAL!TEST\\nCross-Level Train/Val Test Accuracy F1score\\nCountry!CityItaly Milan 97.17% 95.37%\\nItaly Turim 96.28% 94.04%\\nContinent!CountryAsia Portugal 92.71% 86.04%\\nAsia Italy 94.69% 91.43%\\nWorld!ContinentWorld Europe 96.72% 94.50%\\nWorld America 96.66% 94.35%\\nWorld Asia 98.65% 97.79%\\nLastly, results of manual annotations showed the proposed\\nsystem can automatically acquire and annotate satellite im-\\nagery with an average accuracy of 95:41% (4:04% false\\npositive and 4:83% false negative samples), see Table VI for\\ndetailed automatic annotation errors. In addition, results of the\\nmanual annotation also showed a small improvement (2.00%\\non average, see Table VI) on the accuracy when switching\\nfrom automatic labeled training data to manually labeled. This\\nimprovement is due to the decrease in noise of models trained\\nusing the manual labels. Some failure cases are shown in\\nFigure 3. As can be seen in the Table VI, there is a correlation\\nbetween the error of the automatic annotation and the accuracy\\nof the resulting models, i.e., an increase in the annotation\\nerror implies in a decrease in the accuracy of models using\\nautomatic data. The Table VI also shows that the absolute\\ndifferences between models validated with automatic data and\\nmanual data are not very high, at most 1.75% which is the\\ndifference for New York. This indicates that all our previous\\nresults of experiments evaluated with automatic data are valid,\\nand would not be much different if they were evaluated with\\nmanually annotated data.\\nTABLE VI\\nIMPACT OF MANUAL ANNOTATION ON THE ACCURACY FOR THE VGG\\nA: A UTOMATIC – M: M ANUAL\\nDatasetAnnotation\\nErrorTrainVal / Test\\nA / A A / M M / M\\nMilan 2.58% 97.00% 97.71% 98.91%\\nTurim 6.57% 96.37% 94.69% 98.32%\\nNew York 6.77% 95.49% 93.74% 96.62%\\nToyokawa 1.47% 98.16% 99.04% 99.33%\\nV. C ONCLUSION\\nIn this letter, a scheme for automatic large-scale satellite\\nzebra crossing classiﬁcation was proposed. The system au-\\nFig. 3. Failure cases. True Positive and True Negative are represented by\\ngreen and blue markers, respectively. False Positive and False Negative are\\nboth represented by the red markers.\\ntomatically acquires images of crosswalks and no-crosswalks\\naround the world using the Google Static Maps API, Google\\nMaps Directions API and OpenStreetMap. Additionally, deep-\\nlearning-based models are trained and evaluated using these\\nautomatically annotated images. Experiments were performed\\non this novel dataset with 245,768 images from 3 different\\ncontinents, 9 countries and more than 20 cities. Experimental\\nresults validated the robustness of the proposed system and\\nshowed an accuracy of 97.11% on the global experiment.\\nACKNOWLEDGMENT\\nWe would like to thank UFES for the support, CAPES for\\nthe scholarships, and CNPq (311120/2016-4) for the grant. We\\ngratefully acknowledge the support of NVIDIA Corporation\\nwith the donation of the Tesla K40 GPU used for this research.\\nREFERENCES\\n[1] V . Ivanchenko, J. Coughlan, and H. Shen, “Detecting and locating\\ncrosswalks using a camera phone,” in Computer Vision and Pattern\\nRecognition Workshops, 2008. CVPRW’08. IEEE Computer Society\\nConference on . IEEE, 2008, pp. 1–8.\\n[2] A. Haselhoff and A. Kummert, “On visual crosswalk detection for driver\\nassistance systems,” in 2010 IEEE Intelligent Vehicles Symposium , June\\n2010, pp. 883–888.\\n[3] D. Ahmetovic, R. Manduchi, J. M. Coughlan, and S. Mascetti, “Zebra\\nCrossing Spotter: Automatic Population of Spatial Databases for In-\\ncreased Safety of Blind Travelers,” in 17th International ACM SIGAC-\\nCESS Conference on Computers & Accessibility , 2015, pp. 251–258.\\n[4] J. D. Banich, “Zebra Crosswalk Detection Assisted By Neural Net-\\nworks,” Master’s thesis, Faculty of California Polytechnic State Uni-\\nversity, California, USA, 2016.\\n[5] D. Herumurti, K. Uchimura, G. Koutaki, and T. Uemura, “Urban\\nRoad Network extraction based on Zebra Crossing Detection from a\\nvery high resolution RGB aerial image and DSM data,” in Signal-\\nImage Technology & Internet-Based Systems (SITIS), 2013 International\\nConference on . IEEE, 2013, pp. 79–84.\\n[6] M. Ghilardi, J. Junior, and I. Manssour, “Crosswalk localization from\\nlow resolution satellite images to assist visually impaired people,” 2016.\\n[7] D. Koester, B. Lunt, and R. Stiefelhagen, “Zebra crossing detection\\nfrom aerial imagery across countries,” in International Conference on\\nComputers Helping People with Special Needs , 2016, pp. 27–34.\\n[8] Y . LeCun, B. Boser, J. S. Denker, R. E. Howard, W. Habbard, L. D.\\nJackel, and D. Henderson, “Handwritten digit recognition with a back-\\npropagation network,” in Advances in Neural Information Processing\\nSystems , San Francisco, CA, USA, 1990, pp. 396–404.\\n[9] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\\nlarge-scale image recognition,” arXiv preprint arXiv:1409.1556 , 2014.\\n[10] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet Classiﬁcation\\nwith Deep Convolutional Neural Networks,” in Advances in Neural\\nInformation Processing Systems , 2012, pp. 1097–1105.\\n[11] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,\\nV . Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,”\\ninProceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , 2015, pp. 1–9.\\n[12] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-\\nFei, “ImageNet Large Scale Visual Recognition Challenge,” Int. Journal\\nof Computer Vision , vol. 115, no. 3, pp. 211–252, 2015.', 'remote sensing  \\nArticle\\nDeep Learning Based Oil Palm Tree Detection\\nand Counting for High-Resolution Remote\\nSensing Images\\nWeijia Li1,2, Haohuan Fu1,2,3,*, Le Yu1,2and Arthur Cracknell4\\n1Ministry of Education Key Laboratory for Earth System Modeling, Department of Earth System Science,\\nTsinghua University, Beijing 100084, China; liwj14@mails.tsinghua.edu.cn (W.L.);\\nleyu@tsinghua.edu.cn (L.Y.)\\n2Joint Center for Global Change Studies (JCGCS), Beijing 100084, China\\n3National Supercomputing Center in Wuxi, Wuxi 214072, China\\n4Division of Electronic Engineering and Physics, University of Dundee, Dundee DDI 4HN, UK;\\napcracknell774787@yahoo.co.uk\\n*Correspondence: haohuan@tsinghua.edu.cn; Tel.: +86-158-1084-7817\\nAcademic Editors: Janet Nichol and Prasad S. Thenkabail\\nReceived: 5 November 2016; Accepted: 28 December 2016; Published: 30 December 2016\\nAbstract: Oil palm trees are important economic crops in Malaysia and other tropical areas.\\nThe number of oil palm trees in a plantation area is important information for predicting the yield\\nof palm oil, monitoring the growing situation of palm trees and maximizing their productivity,\\netc. In this paper, we propose a deep learning based framework for oil palm tree detection and\\ncounting using high-resolution remote sensing images for Malaysia. Unlike previous palm tree\\ndetection studies, the trees in our study area are more crowded and their crowns often overlap.\\nWe use a number of manually interpreted samples to train and optimize the convolutional neural\\nnetwork (CNN), and predict labels for all the samples in an image dataset collected through the\\nsliding window technique. Then, we merge the predicted palm coordinates corresponding to the\\nsame palm tree into one palm coordinate and obtain the ﬁnal palm tree detection results. Based on\\nour proposed method, more than 96% of the oil palm trees in our study area can be detected correctly\\nwhen compared with the manually interpreted ground truth, and this is higher than the accuracies of\\nthe other three tree detection methods used in this study.\\nKeywords: oil palm trees; deep learning; convolutional neural network (CNN); object detection\\n1. Introduction\\nOil palm trees are important economic crops. In addition to their main use to produce palm oil,\\noil palms are also used to generate a variety of products such as plywood, paper, furniture, etc. [ 1].\\nInformation about the locations and the number of oil palm trees in a plantation area is important in\\nmany aspects. First, it is essential for predicting the yield of palm oil, which is the most widely used\\nvegetable oil in the world. Second, it provides vital information to understand the growing situation of\\npalm trees after plantation, such as the age or the survival rate of the palm trees. Moreover, it informs\\nthe development of irrigation processes and maximizes productivity [2].\\nRemote sensing has played an important role in various studies on oil palm productivity, the age\\nof oil palm trees and oil palm mapping, etc. [ 3–8]. In recent years, high-resolution remote sensing\\nimages have become increasingly popular and important for many applications including automatic\\npalm tree detection. Previous palm tree or tree crown detection research has usually been based\\non traditional methods in the computer vision domain. For instance, a tree detection–delineation\\nalgorithm was designed for high-resolution digital imagery tree crown detection, which is based\\nRemote Sens. 2017 ,9, 22; doi:10.3390/rs9010022 www.mdpi.com/journal/remotesensingRemote Sens. 2017 ,9, 22 2 of 13\\non the local maximum ﬁlter and the analysis of local transects extending outward from a potential\\ntree apex [ 9]. Shafri et al. [ 10] presented an approach for oil palm tree extraction and counting from\\nhigh spatial resolution airborne imagery data, which is composed of many parts including spectral\\nanalysis, texture analysis, edge enhancement, segmentation process, morphological analysis and blob\\nanalysis. Ke et al. [ 11] reviewed various methods for automatic individual tree-crown detection and\\ndelineation from passive remote sensing, including local maximum ﬁltering, image binarization, scale\\nanalysis, and template matching, etc. Srestasathiern et al. [ 12] used semi-variogram computation and\\nnon-maximal suppression for palm tree detection from high-resolution multi-spectral satellite images.\\nMoreover, some researchers have also applied machine learning-based methods to palm tree\\ndetection studies. Malek et al. [ 2] used a scale-invariant feature transform (SIFT) and a supervised\\nextreme learning machine classiﬁer to detect palm trees from unmanned aerial vehicle (UAV) images.\\nManandhar et al. [ 13] used circular autocorrelation of the polar shape matrix representation of an image\\nas the shape feature and a linear support vector machine to standardize and reduce dimensions of\\nthe feature. This study also used a local maximum detection algorithm on the spatial distribution of\\nstandardized features to detect palm trees. Previous palm tree or tree crown detection studies have\\nfocused on detecting trees that are not very crowded and have achieved good detection results for their\\nstudy areas. However, the performance of some of these methods would deteriorate when detecting\\npalm trees in some of the regions of our study area. For instance, the local maximum ﬁlter based\\nmethod [ 9] cannot detect palm trees correctly in regions where the trees are very young and small, as\\nthe local maximum of each ﬁlter does not locate around the apex of young palm trees. The template\\nmatching method [ 10] is not suitable for regions where palm trees are very crowded and where their\\ncrowns overlap.\\nThe convolutional neural network (CNN), a widely used deep learning model, has achieved\\ngreat performance in many studies in the computer vision ﬁeld, such as image classiﬁcation [ 14,15],\\nface recognition [ 16,17], and pedestrian detection [ 18,19], etc. In recent years, deep learning based\\nmethods have also been applied to hyperspectral image classiﬁcation [ 20,21], large-scale land cover\\nclassiﬁcation [ 22], scene classiﬁcation [ 23–25], and object detection [ 26,27], etc. in the remote sensing\\ndomain and achieved better performance than traditional methods. For instance, Chen et al. [ 20]\\nintroduced the concept of deep learning and applied the stacked autoencoder method to hyperspectral\\nremote sensing image classiﬁcation for the ﬁrst time. Li et al. [ 22] built a classiﬁcation framework for\\nlarge-scale remote sensing image processing and African land cover mapping based on the stacked\\nautoencoder. Zou et al. [ 24] proposed a deep belief network based feature selection method for\\nremote sensing scene classiﬁcation. Chen et al. [ 26] proposed a hybrid deep convolutional neural\\nnetwork for vehicle detection in high-resolution satellite images. Vakalopoulou et al. [ 27] proposed\\nan automated building detection framework from very high-resolution remote sensing data based on\\ndeep convolutional neural networks.\\nIn this paper, we introduce the deep learning based method to oil palm tree detection for the ﬁrst\\ntime. We propose a CNN based framework for the detection and counting of oil palm trees using\\nhigh-resolution remote sensing images from Malaysia. The detection and counting of oil palm trees\\nin our study area is more difﬁcult than for the previous palm detection research mentioned above,\\nas the trees are very crowded and their crowns often overlap. In our proposed method, we collect\\na number of manually interpreted training and test samples for training the convolutional neural\\nnetwork and calculating the classiﬁcation accuracy. Secondly, we optimize the convolutional neural\\nnetwork through tuning its main parameters to obtain the best CNN model. Then, we use the best\\nCNN model obtained previously to predict the labels for all the samples in an image dataset that are\\ncollected through the sliding window technique. Finally, we merge the predicted palm tree coordinates\\ncorresponding to the same palm tree (spatial distance less than a certain threshold) into one coordinate,\\nand obtain the ﬁnal palm tree detection results. Compared with the manually interpreted ground\\ntruth, more than 96% of the oil palm trees in our study area can be detected correctly, which is higher\\nthan the accuracies of the other three tree detection methods used in this study. The detection accuracyRemote Sens. 2017 ,9, 22 3 of 13\\nof our proposed method is affected, to some extent, by the limited number of our manually interpreted\\nsamples. In our future work, more manually interpreted samples will be collected to further improve\\nthe overall performance of our proposed method.\\nThe rest of this paper is organized as follows. Section 2 presents the study area and the datasets\\nof this research; Section 3 describes the ﬂowchart and the details of our proposed method; Section 4\\nprovides the detection results of our proposed method and the performance comparison with other\\nmethods; and Section 5 presents some important conclusions of this research.\\n2. Study Area and Datasets\\nIn this research, a QuickBird image acquired on 21 November 2006 is used. The QuickBird satellite\\nhas one panchromatic (Pan) band with 0.6-m spatial resolution and four multi-spectral (MS) bands\\nwith 2.4-m spatial resolution. The Gram–Schmidt (GS) spectral sharpening fusion method [ 28], which\\nis implemented in the ENVI software (version 5.3, Exelis Visual Information Solutions, Boulder, CO,\\nUSA), was employed to integrate Pan and MS bands to obtain a higher sharpness and spectral quality\\n(0.6-m spatial resolution, four bands) dataset for further image processing and analysis.\\nThe study area of this research is located in the south of Malaysia, as shown in Figure 1. The manually\\ninterpreted samples used in this study were collected from two typical regions of our study area\\n(denoted by the blue rectangles in Figure 1). To evaluate the performance of our proposed method,\\nwe selected another three representative regions in our study area (denoted by the red squares in\\nFigure 1) and compared the detected images of these regions with the ground truth collected by\\nmanual interpretation.\\nRemote Sens. 2017 , 9, 22  3 of 13 \\n number of our manually interpreted samples. In  our future work, more manually interpreted \\nsamples will be collected to further improve the overall performance of our proposed method. \\nThe rest of this paper is organized as follows. Section 2 presents the study area and the datasets \\nof this research; Section 3 describes the flowchart  and the details of our proposed method; Section 4 \\nprovides the detection results of our proposed me thod and the performance comparison with other \\nmethods; and Section 5 presents some im portant conclusions of this research. \\n2. Study Area and Datasets \\nIn this research, a QuickBird image acquired on  21 November 2006 is used. The QuickBird \\nsatellite has one panchromatic (Pan) band with 0.6-m spatial resolution and four multi-spectral (MS) bands with 2.4-m spatial resolution. The Gram–Sch midt (GS) spectral sharpening fusion method \\n[28], which is implemented in the ENVI software (v ersion 5.3, Exelis Visual  Information Solutions, \\nBoulder, CO., USA), was employed to integrate Pan and MS bands to obtain a higher sharpness and \\nspectral quality (0.6-m spatial resolution, four bands) dataset for further image processing and analysis. \\nThe study area of this research is located in the south of Malaysia, as shown in Figure 1. The \\nmanually interpreted samples used in this study were collected from two typical regions of our study area (denoted by the blue re ctangles in Figure 1). To evaluate the performance of our proposed \\nmethod, we selected another three representative regions in our study area (denoted by the red \\nsquares in Figure 1) and compared the detected im ages of these regions with the ground truth \\ncollected by manual interpretation. \\n \\nFigure 1.  The study area of this research in the south of Peninsular Malaysia. The blue  rectangles \\nshow the two regions from which the manua lly interpreted samples are collected. The red squares \\nshow the three selected regions for evaluati ng the performance of our proposed method. \\n3. Methods \\n3.1. Overview \\nThe flowchart of our proposed method is shown in  Figure 2. First, th e convolutional neural \\nnetwork [14] was implemented based on the Tens orflow framework [29]. We used a number of \\ntraining samples collected previously by manual interpretation to train the CNN, and calculated the \\nclassification accuracy based on a number of te st samples collected independently of training \\nsamples. The main parameters of the CNN (e.g., the number of kernels in the first convolutional \\nFigure 1. The study area of this research in the south of Peninsular Malaysia. The blue rectangles show\\nthe two regions from which the manually interpreted samples are collected. The redsquares show the\\nthree selected regions for evaluating the performance of our proposed method.\\n3. Methods\\n3.1. Overview\\nThe ﬂowchart of our proposed method is shown in Figure 2. First, the convolutional neural\\nnetwork [ 14] was implemented based on the Tensorﬂow framework [ 29]. We used a number of\\ntraining samples collected previously by manual interpretation to train the CNN, and calculated the\\nclassiﬁcation accuracy based on a number of test samples collected independently of training samples.Remote Sens. 2017 ,9, 22 4 of 13\\nThe main parameters of the CNN (e.g., the number of kernels in the ﬁrst convolutional layer, the\\nnumber of kernels in the second convolutional layer and the number of hidden units in the fully\\nconnected layer) were adjusted continuously until we found the best combination of parameters of\\nwhich the overall accuracy was the highest on our test samples. By tuning the parameters, we achieved\\nthe best CNN model and saved it for further use. Secondly, the image dataset for palm tree detection\\nwas collected through the sliding window technique (the window size is 17 \\x0217 and the sliding\\nstep is three pixels). Then, we used the best CNN model obtained previously to predict the label for\\neach sample in the image dataset. Thirdly, for all samples that were predicted as “palm tree” class,\\nwe merged the coordinates corresponding to the same palm tree sample (spatial distance less than\\na certain threshold) into one coordinate, and obtained the ﬁnal palm tree detection results.\\nRemote Sens. 2017 , 9, 22  4 of 13 \\n layer, the number of kernels in the second convol utional layer and the number of hidden units in the \\nfully connected layer) were adjusted continuo usly until we found the best combination of \\nparameters of which the overall accuracy was th e highest on our test samples. By tuning the \\nparameters, we achieved the best CNN model and saved it for further use. Secondly, the image \\ndataset for palm tree detection was collected th rough the sliding window technique (the window \\nsize is 17 × 17 and the sliding step is three pi xels). Then, we used the best CNN model obtained \\npreviously to predict the label for each sample in the image dataset. Thirdly, for all samples that were predicted as “palm tree” class, we merged th e coordinates corresponding to the same palm tree \\nsample (spatial distance less than a certain thre shold) into one coordinate, and obtained the final \\npalm tree detection results. \\n \\nFigure 2. The flowchart of our proposed method. \\n3.2. CNN Training and Parameter Optimization \\nThe LeNet convolutional neural network used in this study is constructed of two convolutional \\nlayers, two pooling layers and a fully connected laye r, as shown in Figure 3. The input to the fully \\nconnected layer is the set of all features maps at the layer below. The fully connected layers \\ncorrespond to a traditional multilayer perception constructed by a hidden layer and a logistic regression layer. We use the Rectif ied Linear Unit (ReLU) as the activation function of the CNN. In \\nthis research, we manually interpreted 5000 palm tree samples and 4000 background samples from two regions of our study area (denoted by the blue rectangles in Figure 1). Then, we randomly select \\n7200 of these samples as the training dataset of the convolutional neural network, and the other 1800 \\nsamples as its test dataset. Only a sample with a palm located at its center will be labeled as “palm \\ntree”. Each sample corresponds to 17 × 17 pixels  with three bands (Red, Green and Blue) selected \\nfrom the original four bands. The main parameters of CNN are adjusted continuously until we find the best combination of parameters for which the overall accuracy is the highest from 1800 test \\nsamples. After parameter tuning, we achieve the best CNN model that will be used in the subsequent process of imag e dataset label prediction. Parameter \\noptimization  CNN training  \\nCNN model  High-resolution  \\nremote sensing image  \\nImage dataset  Training dataset  \\nTest dataset  \\nImage dataset  \\n label prediction  \\nSample merging  \\nFinal detection \\nresults  \\nFigure 2. The ﬂowchart of our proposed method.\\n3.2. CNN Training and Parameter Optimization\\nThe LeNet convolutional neural network used in this study is constructed of two convolutional\\nlayers, two pooling layers and a fully connected layer, as shown in Figure 3. The input to the fully\\nconnected layer is the set of all features maps at the layer below. The fully connected layers correspond\\nto a traditional multilayer perception constructed by a hidden layer and a logistic regression layer.\\nWe use the Rectiﬁed Linear Unit (ReLU) as the activation function of the CNN. In this research,\\nwe manually interpreted 5000 palm tree samples and 4000 background samples from two regions of\\nour study area (denoted by the blue rectangles in Figure 1). Then, we randomly select 7200 of these\\nsamples as the training dataset of the convolutional neural network, and the other 1800 samples as its\\ntest dataset. Only a sample with a palm located at its center will be labeled as “palm tree”. Each sample\\ncorresponds to 17 \\x0217 pixels with three bands (Red, Green and Blue) selected from the original four\\nbands. The main parameters of CNN are adjusted continuously until we ﬁnd the best combination\\nof parameters for which the overall accuracy is the highest from 1800 test samples. After parameter\\ntuning, we achieve the best CNN model that will be used in the subsequent process of image dataset\\nlabel prediction.Remote Sens. 2017 ,9, 22 5 of 13\\n7200 training samples 1800 test samplesPalm Background Palm Background……………………\\nConvolutional layer \\x01 Convolutional layer \\x01 Max-pooling layer \\x01 Max-pooling layer \\x01Fully connected layer \\x01\\nPalm\\n Backgroun d\\n Palm\\n Background\\n7200 training sample s\\n 1800 test sample sPalm tree Background Palm tree Background\\n7200 training samples 1800 test samples\\nFigure 3. The structure of the convolutional neural network (CNN).\\n3.3. Label Prediction\\nThe image dataset for label prediction is collected through the sliding window technique, as\\nshown in Figure 4. The size of the sliding window is 17 \\x0217 pixels, which is consistent with the\\nfeature size of our training and test samples. In addition, the sliding step (the moving distance of the\\nsliding window in each step) will have a great inﬂuence on the ﬁnal palm tree detection results. If the\\nsliding step is too large, many palm samples will be missed and will not be detected. On the other\\nhand, if the sliding step is too small, one palm sample might be detected repeatedly. Moreover, the\\nprocess of label prediction will become slower due to the increasing number of samples in the image\\ndataset, which is actually unnecessary and a waste of time. In this study, the sliding step is set as three\\npixels through experimental tests. After collecting all samples of the image dataset through the sliding\\nwindow technique, we use the best CNN model obtained in Section 3.2 to predict the label for each\\nsample in the image dataset.Remote Sens. 2017 ,9, 22 6 of 13\\n… \\n… \\nFigure 4. The sliding window technique.\\n3.4. Sample Merging\\nAfter the labels of all samples in the image dataset are predicted, we collect the spatial coordinates\\nof all the samples that are predicted as “palm tree” class. At this point, the number of predicted\\npalm tree coordinates could be larger than the actual number of palm trees because one palm tree\\nmight correspond to several predicted palm tree coordinates. To avoid this problem, the coordinates\\ncorresponding to the same palm tree sample will be merged into one coordinate iteratively, as shown in\\nFigure 5. Assuming that, in our study area, the spatial distance between two palm trees cannot be less\\nthan 8 pixels, the merging process will take six iterations. In each iteration, all groups of coordinates\\nwith the Euclidean distance less than a certain threshold (3, 4, 5, 6, 7, 8 pixels) will be merged into\\none coordinate. That is, the original group of coordinates will be replaced by their average coordinate.\\nThe remaining palm tree coordinates after the merging process represent the actual coordinates of\\ndetected palm trees.\\nFigure 5. Sample merging.Remote Sens. 2017 ,9, 22 7 of 13\\n4. Results\\n4.1. Classiﬁcation Accuracy and Parameter Optimization\\nIn this study, the classiﬁcation accuracy of our CNN model was assessed by 1800 test samples\\ncollected independently from 7200 training samples. The classiﬁcation accuracy can be affected by\\nmany parameters, such as the size of the convolutional kernel and the max-pooling kernel, the number\\nof kernels in each convolutional layer and hidden units in fully connected layers, etc. For our CNN\\nmodel, the size of the convolutional kernel is ﬁve, the size of the max-pooling kernel is two, the size\\nof mini-batch is 10 and the maximum number of iterations is 8000. We adjusted three important\\nparameters to optimize the model: the number of kernels in the ﬁrst convolutional layer, the number\\nof kernels in the second convolutional layer and the number of hidden units in the fully connected\\nlayer. Experimental results showed that we could obtain the highest overall accuracy of 95% after\\n7500 iterations when the number of kernels in two convolutional layers are set as 30 and 55 and the\\nnumber of hidden units in fully connected layers is set as 600.\\n4.2. Detection Results Evaluation\\nTo evaluate the performance of our proposed oil palm tree detection method quantitatively,\\nwe calculate the precision, recall and overall accuracy of the palm tree detection results through\\ncomparison with the ground truth. The precision is the probability that a detected oil palm tree is\\nvalid, as described in Formula (1); the recall is the probability that an oil palm tree in ground truth is\\ndetected, as described in Formula (2); the overall accuracy is the average of precision and recall, as\\ndescribed in Formula (3). A palm is regarded as detected correctly only if the distance between the\\ncenter of a detected palm and the center of a palm in ground truth is less than or equal to ﬁve pixels:\\nPrecision =The number of correctly detected palm trees\\nThe number of all detected objects, (1)\\nRecall =The number of correctly detected palm trees\\nThe number of palm trees in ground truth, (2)\\nOverall Accuracy =Precision +Recall\\n2. (3)\\nTable 1 shows that the overall accuracies of regions 1, 2 and 3 are 96.05%, 96.34% and 98.77%,\\nrespectively. In addition, for each of the three regions, the difference between the predicted number\\nof palm trees (the number of all detected objects) and the true number of palm trees (the number of\\npalm trees in ground truth) is less than 4%. These evaluation results show that our proposed method\\nis effective for both palm tree detection and counting.\\nTable 1. Detection results of convolutional neural network (CNN).\\nEvaluation Index Region 1 Region 2 Region 3\\nThe number of correctly detected palm trees 1651 1607 1683\\nThe number of all detected objects 1729 1695 1706\\nThe number of palm trees in ground truth 1709 1642 1702\\nPrecision 95.49% 94.81% 98.65%\\nRecall 96.61% 97.87% 98.88%\\nOverall accuracy 96.05% 96.34% 98.77%\\n5. Discussion\\nTo further evaluate our proposed palm tree detection method, we implemented three other\\nrepresentative existing palm trees or tree crown detection methods (i.e., Artiﬁcial Neural Network\\n(ANN), template matching, and local maximum ﬁlter) and compared their detection results withRemote Sens. 2017 ,9, 22 8 of 13\\nour proposed method. The procedure of the ANN based method is the same as our proposed\\nmethod, including the ANN training, parameter optimization, image dataset label prediction, and\\nsample merging.\\nThe local maximum ﬁlter based method [ 9] and the template matching based method [ 11] are\\ntwo traditional tree crown detection methods. For the template matching based method, we used\\n5000 manually labeled palm tree samples as the template dataset, and a 17 \\x0217 window to slide\\nthrough the whole image. We chose the CV_TM_SQDIFF_NORMED provided by OpenCV [ 30] as\\nour matching method. A sliding window will be detected as a palm tree if it matches any sample in\\nthe template dataset (the difference between the sliding window and the template calculated by the\\nCV_TM_SQDIFF_NORMED method is less than a threshold. In this study, the threshold is set as ﬁve\\nthrough experimental tests).\\nFor the local maximum ﬁlter based method, we ﬁrst applied a non-overlapping 10 \\x0210 local\\nmaximum ﬁlter to the absolute difference image of the NIR and red spectral bands. Then, we conducted\\ntransect sampling and a scaling scheme to obtain potential tree apexes, and adjusted the locations of\\ntree apexes to the new local maximum positions.\\nFinally, the outputs of the template matching based method and the local maximum ﬁlter based\\nmethod are post-processed (described in Section 3.4) to obtain the ﬁnal palm tree detection results.\\nFigures 6–8 show the detection images of each method for extracted areas of regions 1, 2 and 3,\\nrespectively. Each red circle denotes a detected palm tree. Each green square denotes a palm tree in\\nground truth that cannot be detected correctly. Each blue square denotes a background sample that is\\ndetected as a palm tree by mistake.\\nTables 2–4 show the detection results of ANN, template matching (TMPL), and local maximum\\nﬁlter (LMF), respectively. Table 5 summarizes the performance of all four methods in terms of the\\nnumber of correctly detected palm trees. Table 6 summarizes the performance of all four methods in\\nterms of precision, recall and overall accuracy (OA). The proposed method (CNN) performs better\\nthan any of the other three methods in the number of correctly detected palm trees and in OA.\\nGenerally, machine learning based approaches (i.e., CNN and ANN) perform better than traditional\\ntree crown detection methods (i.e., TMPL and LMF) in our study area, especially in region 1 and region\\n2. For example, the local maximum ﬁlter based method cannot detect palm trees correctly for regions\\nwhere palm trees are very young and small (see Figure 7d), as the local maximum of each ﬁlter does\\nnot locate around the apex of young palm trees. The template matching method is not suitable for\\nregions where the palm trees are very crowded and the canopies often overlap (see Figure 6c).\\nTable 2. Detection results of artiﬁcial neural network (ANN).\\nEvaluation Index Region 1 Region 2 Region 3\\nThe number of correctly detected palm trees 1648 1585 1679\\nThe number of all detected objects 1800 1725 1718\\nThe number of palm trees in ground truth 1709 1642 1702\\nPrecision 91.56% 91.88% 97.73%\\nRecall 96.43% 96.53% 98.64%\\nOverall accuracy 94.00% 94.21% 98.19%\\nTable 3. Detection results of template matching (TMPL).\\nEvaluation Index Region 1 Region 2 Region 3\\nThe number of correctly detected palm trees 1429 1392 1608\\nThe number of all detected objects 1532 1493 1684\\nThe number of palm trees in ground truth 1709 1642 1702\\nPrecision 93.28% 93.24% 95.49%\\nRecall 83.62% 84.77% 94.48%\\nOverall accuracy 88.45% 89.01% 94.99%Remote Sens. 2017 ,9, 22 9 of 13\\nTable 4. Detection results of local maximum ﬁlter (LMF).\\nEvaluation Index Region 1 Region 2 Region 3\\nThe number of correctly detected palm trees 1493 1397 1643\\nThe number of all detected objects 1719 1675 1761\\nThe number of palm trees in ground truth 1709 1642 1689\\nPrecision 86.85% 83.40% 93.30%\\nRecall 87.36% 85.08% 97.28%\\nOverall accuracy 87.11% 84.24% 95.29%\\nRemote Sens. 2017 , 9, 22  9 of 13 \\n Table 4.  Detection results of local maximum filter (LMF). \\nEvaluation Index Region 1 Region 2 Region 3\\nThe number of correctly detected palm trees 1493 1397 1643 \\nThe number of all detected objects 1719 1675 1761 \\nThe number of palm trees in ground truth 1709 1642 1689 \\nPrecision 86.85% 83.40% 93.30% \\nRecall 87.36% 85.08% 97.28% \\nOverall accuracy 87.11% 84.24% 95.29% \\n \\n(a) Convolutional neural network (b) Artificial neural network  \\n(c) Template matching  (d) Local maximum filter  \\nFigure 6.  Detection image of each method for region 1 (extracted area). Each red circle denotes a \\ndetected palm tree. Each green  square denotes a palm tree in ground truth that cannot be detected \\ncorrectly. Each blue  square denotes a background sample that is detected as a palm tree by mistake. \\n  \\nFigure 6. Detection image of each method for region 1 (extracted area). Each redcircle denotes\\na detected palm tree. Each green square denotes a palm tree in ground truth that cannot be detected\\ncorrectly. Each blue square denotes a background sample that is detected as a palm tree by mistake.Remote Sens. 2017 ,9, 22 10 of 13\\nTable 5. Summary of the number of correctly detected palm trees for all four methods.\\nMethods Region 1 Region 2 Region 3\\nCNN 1651 1607 1683\\nANN 1648 1585 1679\\nTMPL 1429 1392 1608\\nLMF 1493 1397 1643\\nTable 6. Summary of the precision, recall and overall accuracy (OA) of all four methods.\\nMethodsRegion 1 Region 2 Region 3\\nPrecision Recall OA Precision Recall OA Precision Recall OA\\nCNN 95.49% 96.61% 96.05% 94.81% 97.87% 96.34% 98.65% 98.88% 98.77%\\nANN 91.56% 96.43% 94.00% 91.88% 96.53% 94.21% 97.73% 98.64% 98.19%\\nTMPL 93.28% 83.62% 88.45% 93.24% 84.77% 89.01% 95.49% 94.48% 94.99%\\nLMF 86.85% 87.36% 87.11% 83.40% 85.08% 84.24% 93.30% 97.28% 95.29%\\nRemote Sens. 2017 , 9, 22  10 of 13 \\n Table 5.  Summary of the number of correctly dete cted palm trees for all four methods. \\nMethods Region 1 Region 2 Region 3\\nCNN 1651 1607 1683 \\nANN 1648 1585 1679 \\nTMPL 1429 1392 1608 \\nLMF 1493 1397 1643 \\nTable 6.  Summary of the precision, recall and overall accuracy (OA) of all four methods. \\nMethods Region 1 Region 2 Region 3 \\nPrecision Recall OA Precision Recall OA Precision Recall OA \\nCNN 95.49% 96.61% 96.05% 94.81% 97.87% 96.34% 98.65% 98.88% 98.77% \\nANN 91.56% 96.43% 94.00% 91.88% 96.53% 94.21% 97.73% 98.64% 98.19% \\nTMPL 93.28% 83.62% 88.45% 93.24% 84.77% 89.01% 95.49% 94.48% 94.99% \\nLMF 86.85% 87.36% 87.11% 83.40% 85.08% 84.24% 93.30% 97.28% 95.29% \\n \\n \\n(a) Convolutional neural network (b) Artificial neural network  \\n \\n(c) Template matching  (d) Local maximum filter  \\nFigure 7.  Detection image of each method for region 2 (extracted area). \\nFigure 7. Detection image of each method for region 2 (extracted area).Remote Sens. 2017 ,9, 22 11 of 13\\nRemote Sens. 2017 , 9, 22  11 of 13 \\n (a) Convolutional neural network (b) Artificial neural network  \\n(c) Template matching  (d) Local maximum filter  \\nFigure 8.  Detection image of each method for region 3 (extracted area). \\n6. Conclusions \\nIn this paper, we designed and implemented a deep  learning based framework for oil palm tree \\ndetection and counting from high-resolution remote sensing images. Three representative regions in \\nour study area are selected for assessment of ou r proposed method. Experimental results show the \\neffectiveness of our proposed method for palm tr ee detection and counting. First, the palm tree \\ndetection results are very similar to the manually labeled ground truth in general. Secondly, the overall accuracies of region 1, region 2 and region 3 are 96%, 96% and 99%, respectively, which are higher than the accuracies of the three other methods used in this research. Moreover, the difference between the predicted number of palm trees and the true number of palm trees is less than 4% for \\neach region of the study area. In our future work , the palm tree detection results should be further \\nimproved through enlarging the number of manu ally interpreted samples and optimizing our \\nproposed CNN based detection framework. We also wa nt to take the computation time of different \\ndetection methods into consideration, and explor e the deep learning based detection framework for \\nlarger scale palm tree detection studies. \\nFigure 8. Detection image of each method for region 3 (extracted area).\\n6. Conclusions\\nIn this paper, we designed and implemented a deep learning based framework for oil palm tree\\ndetection and counting from high-resolution remote sensing images. Three representative regions\\nin our study area are selected for assessment of our proposed method. Experimental results show\\nthe effectiveness of our proposed method for palm tree detection and counting. First, the palm tree\\ndetection results are very similar to the manually labeled ground truth in general. Secondly, the overall\\naccuracies of region 1, region 2 and region 3 are 96%, 96% and 99%, respectively, which are higher than\\nthe accuracies of the three other methods used in this research. Moreover, the difference between the\\npredicted number of palm trees and the true number of palm trees is less than 4% for each region of\\nthe study area. In our future work, the palm tree detection results should be further improved through\\nenlarging the number of manually interpreted samples and optimizing our proposed CNN based\\ndetection framework. We also want to take the computation time of different detection methods into\\nconsideration, and explore the deep learning based detection framework for larger scale palm tree\\ndetection studies.Remote Sens. 2017 ,9, 22 12 of 13\\nAcknowledgments: This work was supported in part by the National Natural Science Foundation of China (Grant\\nNos. 61303003 and 41374113), the National High-Tech R&D (863) Program of China (Grant No. 2013AA01A208),\\nthe Tsinghua University Initiative Scientiﬁc Research Program (Grant No. 20131089356), and the National Key\\nResearch and Development Plan of China (Grant No. 2016YFA0602200).\\nAuthor Contributions: Weijia Li, Haohuan Fu and Le Yu conceived of the study; Weijia Li wrote code, performed\\nthe analysis and wrote the article; Haohuan Fu performed the analysis and wrote the article; Le Yu and\\nArthur Cracknell performed the analysis and commented on the article.\\nConﬂicts of Interest: The authors declare no conﬂict of interest.\\nReferences\\n1. Suhaily, S.; Jawaid, M.; Khalil, H.A.; Mohamed, A.R.; Ibrahim, F. A review of oil palm biocomposites for\\nfurniture design and applications: Potential and challenges. BioResources 2012 ,7, 4400–4423.\\n2. Malek, S.; Bazi, Y .; Alajlan, N.; AlHichri, H.; Melgani, F. Efficient framework for palm tree detection in UA V images .\\nIEEE J. Sel. Top. Appl. Earth Obs. 2014 ,7, 4692–4703. [CrossRef]\\n3. Cracknell, A.P .; Kanniah, K.D.; Tan, K.P .; Wang, L. Evaluation of MODIS gross primary productivity and land\\ncover products for the humid tropics using oil palm trees in Peninsular Malaysia and Google Earth imagery.\\nInt. J. Remote Sens. 2013 ,34, 7400–7423. [CrossRef]\\n4. Tan, K.P .; Kanniah, K.D.; Cracknell, A.P . A review of remote sensing based productivity models and their\\nsuitability for studying oil palm productivity in tropical regions. Prog. Phys. Geogr. 2012 ,36, 655–679. [CrossRef]\\n5. Tan, K.P .; Kanniah, K.D.; Cracknell, A.P . Use of UK-DMC2 and ALOS PALSAR for studying the age of oil\\npalm trees in southern peninsular Malaysia. Int. J. Remote Sens. 2013 ,34, 7424–7446. [CrossRef]\\n6. Kanniah, K.D.; Tan, K.P .; Cracknell, A.P . UK-DMC2 satellite data for deriving biophysical parameters of oil\\npalm trees in Malaysia. In Proceedings of the IEEE International Geoscience and Remote Sensing Symposium,\\nMunich, Germany, 22–27 July 2012; pp. 6569–6572.\\n7. Cheng, Y.; Yu, L.; Cracknell, A.P .; Gong, P . Oil palm mapping using Landsat and PALSAR: A case study\\nin Malaysia. Int. J. Remote Sens. 2016 ,37, 5431–5442. [CrossRef]\\n8. Cracknell, A.P .; Kanniah, K.D.; Tan, K.P .; Wang, L. Towards the development of a regional version of MOD17\\nfor the determination of gross and net primary productivity of oil palm trees. Int. J. Remote Sens. 2015 ,36,\\n262–289. [CrossRef]\\n9. Pouliot, D.A.; King, D.J.; Bell, F.W.; Pitt, D.G. Automated tree crown detection and delineation in\\nhigh-resolution digital camera imagery of coniferous forest regeneration. Remote Sens. Environ. 2002 ,\\n82, 322–334. [CrossRef]\\n10. Shafri, H.Z.; Hamdan, N.; Saripan, M.I. Semi-automatic detection and counting of oil palm trees from high\\nspatial resolution airborne imagery. Int. J. Remote Sens. 2011 ,32, 2095–2115. [CrossRef]\\n11. Ke, Y.; Quackenbush, L.J. A review of methods for automatic individual tree-crown detection and delineation\\nfrom passive remote sensing. Int. J. Remote Sens. 2011 ,32, 4725–4747. [CrossRef]\\n12. Srestasathiern, P .; Rakwatin, P . Oil palm tree detection with high resolution multi-spectral satellite imagery.\\nRemote Sens. 2014 ,6, 9749–9774. [CrossRef]\\n13. Manandhar, A.; Hoegner, L.; Stilla, U. Palm Tree Detection Using Circular Autocorrelation of Polar Shape\\nMatrix. ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci. 2016 ,III-3, 465–472. [CrossRef]\\n14. Krizhevsky, A.; Sutskever, I.; Hinton, G.E. ImageNet classiﬁcation with deep convolutional neural networks.\\nIn Proceedings of the Advances in Neural Information Processing Systems, Lake Tahoe, NV , USA,\\n3–8 December 2012; pp. 1097–1105.\\n15. Ciregan, D.; Meier, U.; Schmidhuber, J. Multi-column deep neural networks for image classiﬁcation.\\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Rhode Island,\\nRI, USA, 16–21 June 2012; pp. 3642–3649.\\n16. Sun, Y.; Wang, X.; Tang, X. Deep learning face representation from predicting 10,000 classes. In Proceedings\\nof the IEEE Conference on Computer Vision and Pattern Recognition, Columbus, OH, USA, 24–27 June 2014;\\npp. 1891–1898.\\n17. Li, H.; Lin, Z.; Shen, X.; Brandt, J.; Hua, G. A convolutional neural network cascade for face detection.\\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Boston, MA, USA,\\n7–12 June 2015; pp. 5325–5334.Remote Sens. 2017 ,9, 22 13 of 13\\n18. Ouyang, W.; Wang, X. Joint deep learning for pedestrian detection. In Proceedings of the IEEE International\\nConference on Computer Vision, Sydney, Australia, 1–8 December 2013; pp. 2056–2063.\\n19. Zeng, X.; Ouyang, W.; Wang, X. Multi-stage contextual deep learning for pedestrian detection. In Proceedings of\\nthe IEEE International Conference on Computer Vision, Sydney, Australia, 1–8 December 2013; pp. 121–128.\\n20. Chen, Y.; Lin, Z.; Zhao, X.; Wang, G.; Gu, Y. Deep learning-based classiﬁcation of hyperspectral data. IEEE J.\\nSel. Top. Appl. Earth Obs. 2014 ,7, 2094–2107. [CrossRef]\\n21. Yue, J.; Zhao, W.; Mao, S.; Liu, H. Spectral–spatial classiﬁcation of hyperspectral images using deep\\nconvolutional neural networks. Remote Sens. Lett. 2015 ,6, 468–477. [CrossRef]\\n22. Li, W.; Fu, H.; Yu, L.; Gong, P .; Feng, D.; Li, C.; Clinton, N. Stacked Autoencoder-based deep learning for\\nremote-sensing image classiﬁcation: a case study of African land-cover mapping. Int. J. Remote Sens. 2016 ,\\n37, 5632–5646. [CrossRef]\\n23. Hu, F.; Xia, G.S.; Hu, J.; Zhang, L. Transferring deep convolutional neural networks for the scene classiﬁcation\\nof high-resolution remote sensing imagery. Remote Sens. 2015 ,7, 14680–14707. [CrossRef]\\n24. Zou, Q.; Ni, L.; Zhang, T.; Wang, Q. Deep learning based feature selection for remote sensing scene classification .\\nIEEE Geosci. Remote Sens. Lett. 2015 ,12, 2321–2325. [CrossRef]\\n25. Penatti, O.A.; Nogueira, K.; dos Santos, J.A. Do deep features generalize from everyday objects to remote\\nsensing and aerial scenes domains? In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition Workshops, Boston, MA, USA, 7–12 June 2015; pp. 44–51.\\n26. Chen, X.; Xiang, S.; Liu, C.L.; Pan, C.H. Vehicle detection in satellite images by hybrid deep convolutional\\nneural networks. IEEE Geosci. Remote Sens. Lett. 2014 ,11, 1797–1801. [CrossRef]\\n27. Vakalopoulou, M.; Karantzalos, K.; Komodakis, N.; Paragios, N. Building detection in very high resolution\\nmultispectral data with deep learning features. In Proceedings of the 2015 IEEE International Geoscience\\nand Remote Sensing Symposium (IGARSS), Milan, Italy, 26–31 July 2015; pp. 1873–1876.\\n28. Laben, C.A.; Brower, B.V . Process for Enhancing the Spatial Resolution of Multispectral Imagery Using\\nPan-Sharpening. U.S. Patent 6,011,875, 4 January 2000.\\n29. Abadi, M.; Agarwal, A.; Barham, P .; Brevdo, E.; Chen, Z.; Citro, C.; Corrado, G.S.; Davis, A.; Dean, J.;\\nDevin, M.; et al. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. 2015. Available\\nonline: https://arxiv.org/abs/1603.04467 (accessed on 31 August 2016).\\n30. Bradski, G.; Kaehler, A. Learning OpenCV: Computer Vision with the OpenCV Library ; O’Reilly Media, Inc.:\\nSebastopol, CA, USA, 2008.\\n©2016 by the authors; licensee MDPI, Basel, Switzerland. This article is an open access\\narticle distributed under the terms and conditions of the Creative Commons Attribution\\n(CC-BY) license (http://creativecommons.org/licenses/by/4.0/).', \"DETECTION AND EXTRACTION OF ROAD NETWORKS \\nFROM HIGH RESOLUTION SATELLITE IMAGES \\nRenatid Pdteri, Jtilien Celle and Thierry Ranchin \\nRemote Sensing & Modeling Group, Ecole des Mines de Paris \\nB.P. 207 - 06904 Sophia Antipolis cedex, France \\nrenaud.peteri@ensmp.fr \\nABSTRACT \\nThis article addresses the problem of road extraction from \\nnew high resolution satellite images. The proposed algo- \\nrithm is divided in two' sequential modules : a topologi- \\ncally corrcct graph of the road network is first extracted, and \\nroads are then extracted as surface elements. The graph of \\nthe network is extracted by a following algorithm which mi- \\nnimizes a cost function. The extraction algorithm makes use \\nof specific active contours (snakes) combined with a multi- \\nresolution analysis (MRA) for minimizing the problem of \\ngeometric noise. This reconstruction phase is composed of \\ntwo steps : the extraction of road segments and the extrac- \\ntion of road intersections. Results of the road network ex- \\ntraction are presented in order to illustrate the different steps \\nof the method and future prospects are exposed. \\n1. ROAD NETWORK EXTRACTION \\n1.1. State of the art \\nRoad extraction from remotely sensed images has been \\nthe purpose of many works in the image processing field, \\nand because of its complexity, is still a challenging topic. \\nThese methods arc based on generic tools of image proces- \\nsing, such as linear filtering ([l]), mathematical morphology \\n([2]), Markov fields ([3]), ncural networks ([4j), dynamic \\nprogramming ([5]j, or multiresolution analysis (161 ; [7]). \\nRoad models are common for all authors, i.e. the radiome- \\ntry along one road is relatively homogeneous and contrasted \\ncompared to its background. Moreoverthe width of ihe road \\nand its curvature are supposed to vary slowly, and the road \\nnetwork is supposed to be connex. Promising studies try to \\ntake thc context of the road into account in order to focus \\nthe extraction on the most promising regions (,[6] ; [SI). \\nThe recent possibility to have satellite images with a high \\nspatial resolution (I meter or less) has re-boosted the in- \\nterest for road extraction (especially for the applications in \\nThis work was suppoded hy a CNRSlffiA gran1 ofthe french Minis- \\nUy of Defence. 'The authors would like Io thank the firm G.I.M. (Geogro- \\npkic hfbrniotion Managemoil) forthe IKONOS image. urban areas). This increased resolution enables a more accu- \\nrate localization of the road sides as well as its extraction as \\na surface element. In return, it generates a higher complexity \\nof the image and an increase of geometric noise (vehicles, \\ntrees along the road, occlusions, . . .). \\n2. THE PROPOSED APPROACH \\n2.1. Description \\nA method has been developed in order to extract and \\ncharacterize the road network from high resolution images. \\nInputs of the algorithm, besides the high resolution satellite \\nimage, are models of roads (using roads properties defined \\nby [7]) and propcrties of road network (such as connexity). \\nOur algorithm is composed of two sequential modules (fig. I). \\nFig. 1. The methodology including topology management \\nand road reconstruction \\nFirstly, a topologically correct graph of the road network is \\nextracted. This step aims at giving correct spatial connec- \\ntions between roads as well as an approximation of their lo- \\ncation. The next step is the actual road reconstruction. Due \\nto the high resolution of the images, a surface reconstmc- \\ntion has to be performed. This step uses the previous step \\n0-7803-7750-8/03/$17.00 02003 IEEE 1 - 301 \\nAuthorized licensed use limited to: b-on: ISCTE. Downloaded on November 21,2023 at 13:35:08 UTC from IEEE Xplore.  Restrictions apply. of graph management as an initialization for the reconstruc- \\ntion. \\nIn the next sections, the different modules of the process are \\nmore precisely described. \\n2.2. Graph management \\nThis module intends to extract a topologically correct \\ngraph of the road network. It aims at giving correctly spa- \\ntial connections between roads as well as an approximation \\nof their location. The graph can come from a road database \\nor he extracted automatically. In this second case, the graph \\nextraction algorithm is based on the work of [9] : a follo- \\nwing algorithm selects the hest path for the potential road \\nby minimizing a cost function. The cost function evaluates \\nthe homogeneity of the local radiometly variance for seve- \\nral propagation directions. In order to overcome noise which \\nmay disturb the process, a decrease of the image resolution \\ncan be performed by applying a blurring filter. \\nAt this step, the extracted graph is topologically correct but \\nthe different polylines are not necessary well registered. \\nFrom the extracted graph, polylines are then sampled and \\npropagated along their normal direction in order to initia- \\nlize the surface reconstruction module. \\n2.3. Reconstruction module \\n2.3.1. Description \\nThe goal of this module is to reconstruct roads as sur- \\nface elements from the graph provided by the previous step. \\nThis module makes use of specific active contours (snaks) \\ncombined with a multiresolution analysis (MRA). The snake \\nimplementation is based on the pleedy algoritliiii described \\nby[lO].Theuseofthe MRA withthe Waveletrransformen- \\nables to perform a multiresolution edge detection (see [I I]). \\nIt also increases the algorithm convergence by minimizing \\nthe problem of noise (vehicles, ground markings,. . .). \\nTwo sequential steps compose this reconstruction phase : \\nthe extraction of road segments with parallel sides and the \\nextraction of road intersections. Indeed, these two objects \\npresent too many differences in both topology and shape to \\nbe processed in the same way. The frontier between the two \\nkinds of process is defined by a circle including the whole \\nintersection (fig. 2). \\n2.3.2. Extraction ofparallel load sides \\nFor extracting portions of road with parallel sides, a new \\nobject has been defined ; the DoirbleSnake. It is composed \\nof two branches which are two snakes evolving jointly. The \\nDorrbleSnake energy functional has a new term E,, that \\nconstrains the DoribleSnake to maintain a local parallelism \\nbetween its two branches. Moreover, their extremity points are forced to minimize their energy staying on the intersec- \\ntion circle. \\n‘The snake energy functional is : \\nE = [aiE&,, i- $E&, + ~~q~~~~.~ +@E;/] \\ni \\n(1) \\nwhere irepresents the point i of one of the branch. \\nE&,,, and E:uru are internal energies that control the SM- \\nke’s shape. A special attention is paid on the image energy \\nterm E;j,nagc, as it the one that attracts the snake to the \\nobject of interest. The image energy is computed with the \\nwavelet coefficients at different spatial resolutions : \\nEi ZJi”..,, = -Jlqjm2 + Iw;j.f(i)l? (2) \\nwhere W,;’f(i) are the coordinates ofthe wavelet trans- \\nform and j is the resolution in a dyadic analysis. \\nA more detailed description of the different energy terms \\ncan be found in ([12]) \\n2.3.3. Extraction of road intersections \\nOnce road segment extraction is finished, the intersec- \\ntion extraction starts. They are extracted by simple snakes \\nwhich are initialized by pairing extremity points of the Dorr- \\nble3rake.y (see fig. 2). This ZntersectionSnake has its extre- \\nmities fixed and is constrained not to go out of the circle. . \\nFig. 2. Initialization of the intersection snakes \\n2.3.4. Reconslniclion algorirhniflowchart \\nFig. 3 represents the different steps of the reconstruction \\nalgorithm. From the original image, a multiresolution am- \\nlysis is performed, giving several approximation images at \\ncoarser resolutions and several wavelet coefficient images. \\nDoubleSnakes are first applied on the coarsest resolution \\nimage, then on each intermediate resolution images until \\nthey run on the original resolution image. Coefficients of \\nI - 302 \\nAuthorized licensed use limited to: b-on: ISCTE. Downloaded on November 21,2023 at 13:35:08 UTC from IEEE Xplore.  Restrictions apply. the different energy terms are adapted to the image resolu- \\ntion. For instance, at the coarsest resolution, a high value \\nof the image term allows the snake to be attracted from far. \\nRefining the estimation on a finer resolution image is then \\ndone by releasing image constraints and increasing the im- \\nportance of the internal energy. \\nOnce DoubleSnukes have all minimized their energy, the In- \\nrersectio!rSnukes are initialized and the extraction starts in \\nthe same multiresolution process. \\nPig. 3. Reconstruction algorithm \\nThe algorithm stops when all snakes are in an equilibrium \\nstate. \\n3. RESULTS AND PROSPECTS \\n3.1. Real-case study \\nAs the graph module is currently under integration, only \\nthe reconstruction module will be illustrated in the follo- \\nwing example. Image of figure 4 comes from the IKONOS \\nsatellite which has spatial resolution of 1 meter in the pan- \\nchromatic mode. \\nFig. 4. Original image with the road graph This image includes a crossroad composed of two main roads \\non which the input graph (coming from a database or ma- \\nnually given) has been superposed. \\nImages 5, 6 and 7 represent the different steps of the algo- \\nrithm. On image 5, polylines of the input graph have been \\nsampled and propagated. The circle delimiting the two types \\nof the reconstruction process is also drawn. \\nFig. 5. Afler propagation of the polylines \\nAt step of image 6, portions of roads with parallel sides have \\nbeen extracted by Doub1eSmke.s after running on the dif- \\nferent resolutiou images. \\nFig. 6. After the parallel contour extraction \\nThe extraction final result afler the intersection process is \\nrepresented on image 7. \\nContours have globally been extracted with a good preci- \\nsion (except the sharp edge of the intersection, partly due \\nto its poor image force). The use of the multiresolution ap- \\nproach has prevented the snakes from being trapped by the \\ngeometric noise such as ground marking. \\nI - 303 \\nAuthorized licensed use limited to: b-on: ISCTE. Downloaded on November 21,2023 at 13:35:08 UTC from IEEE Xplore.  Restrictions apply. Fig. 7. After the intersection extraction \\n3.2. Conclusion and future prospects \\nThis article describes a new method for extracting the \\nroad network from high resolution satellite images. A topo- \\nlogically correct graph or the road network is first extrac- \\nted, and roads are then extracted as surface elements. ‘The \\ngraph of the network is extracted by a following algorithm \\n[Y] which minimizes a cost function. The extraction algo- \\nrithm makes use of specific snakes combined with a mul- \\ntiresolution analysis. Current works aim at increasing the \\nrobustness of the algorithm, particularly in noisy environ- \\nments such as urban areas. The use the wavelet coefficients \\nderiving from the MRA for extracting texture information \\nproper to roads could constraint the snake evolution. Some \\ncontextual information (such as building or car alignment) \\ncan also he a clue for the extraction. \\nResults on test images are encouraging. However, results \\nshould be validated and characterized using quantitative cri- \\nteria described in [13]. \\n4. REFERENCES \\n[I] J.F. Wang and P.J. Howarth, “Automated road network \\nextraction from landsat tm imagery,” in Pmceedings \\nqf the annual ASPRS/ACSM Conventionl. Baltimore. \\nMU, USA, 1987, vol I, pp 429-438 [2] 1. Destivdl, “Recherche automatique de rkseaux \\nIinCaires sur des images spot,” Societe! Franpise de \\nPhotogranimehie er de TeWiktecrion, vol. 66, pp. 5- \\n16, 1987. \\np] N. Merlet ,and J. ZBrubia, “New prospects in line de- \\ntection by dynamic programming,” IEEE Transactions \\now Pattern Analvsis and Machine Intelligence, vol. 18, \\nno. 4, pp. 426430, 1996. \\n141 U. Bhattacharya and S.K. Parui, “An improved back- \\npropagation neural network for detection of road-like \\nfeatures in satellite imagery;’ International Jorirnal of \\nRenroteSensiiig,vol. 18, no. 16,pp. 3379-3394, 1997. \\n[5] A. Gruen and H. Li, “Road extraction from aerial \\nand satellite images by dynamic programming:’ IS- \\nPRS Journal ofPhotograsmretw and Reinole Sensing, \\nvol. 50, no. 4, pp. 11-20, 1995. \\n[6] A. Baumgartner, C. Steger, H. Mayer, W. Eckstein, \\nand E. Heinrich, “Automatic road extraction based on \\nmulti-scale, grouping, and context;‘ Photogi’anrnietric \\nEngineering and Remote Sensing, vol. 65, no. 7, pp. \\n7777785,1999. \\n[7] 1. Couloigner and T. Ranchin, “Mapping of urban \\nareas : A multiresolution modeling approach for semi- \\nautomatic extraction of streets,“ Phologramsietric En- \\ngineering and Reniofe Sensing, vol. 66, no. 7, pp. 867- \\n874,2000. \\n[8] R. Ruskone, Extraction untomatique du re!seau routier \\npar inteqwetution locale du contate : application a la \\npmducfion de donnkes cartogvaphiques, Ph.D. thesis, \\nUniversite de Marne-la-Vallee, 1996. \\n[9] S. Airault and 0. Jamet, “Detection et restitution au- \\ntomatique du riseau routier sur des images &riennes,” \\nTrailernent du Signal, vol. 12, no. 2, pp. 189-200, \\n1995. \\n[IO] D.J. Williams and M. Shah, “A fast algorithm for ac- \\ntive contours and curvature estimation,“ in CVIP : \\nImage Understanding, January 1992, vol. 55, pp. 14- \\n26. \\n[I I] S.G. Mallat, A Wavelet ToiirofSignal Pmcessing, AP \\nProfessional, London, 1997. \\n[I21 R. Peteri and T. Ranchin, “Multiresolution snakes \\nfor urban road extraction from ikonos and quickhird \\nimages,’’ in 23nd EARSeL Annual Sviirposiurn ”Re- \\nniote Sensing in Transition ”, Ghent, Belgium, 2-5 \\nJune 2003, to appear. \\n[I31 R. Peteri, I. Couloigner, and T. Ranchin, “How to as- \\nsess quantitatively road extracted from high resolution \\nimagely 1,” Phofogramnetric Engrireering & Reiirofe \\nSensing, 2003, to appear. \\nI - 304 \\nAuthorized licensed use limited to: b-on: ISCTE. Downloaded on November 21,2023 at 13:35:08 UTC from IEEE Xplore.  Restrictions apply. \", '3386 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV A TIONS AND REMOTE SENSING, VOL. 10, NO. 7, JULY 2017\\nObject-Based Convolutional Neural Network for\\nHigh-Resolution Imagery Classiﬁcation\\nWenzhi Zhao, Shihong Du, and William J. Emery , Fellow, IEEE\\nAbstract —Timely and accurate classiﬁcation and interpretation\\nof high-resolution images are very important for urban planning\\nand disaster rescue. However, as spatial resolution gets ﬁner, it is in-\\ncreasingly difﬁcult to recognize complex patterns in high-resolution\\nremote sensing images. Deep learning offers an efﬁcient strategy\\nto ﬁll the gap between complex image patterns and their semantic\\nlabels. However, due to the hierarchical abstract nature of deep\\nlearning methods, it is difﬁcult to capture the precise outline of\\ndifferent objects at the pixel level. To further reduce this prob-\\nlem, we propose an object-based deep learning method to accu-\\nrately classify the high-resolution imagery without intensive hu-\\nman involvement. In this study, high-resolution images were used\\nto accurately classify three different urban scenes: Beijing (China),\\nPavia (Italy), and Vaihingen (Germany). The proposed method is\\nbuilt on a combination of a deep feature learning strategy and an\\nobject-based classiﬁcation for the interpretation of high-resolution\\nimages. Speciﬁcally, high-level feature representations extracted\\nthrough the convolutional neural networks framework have been\\nsystematically investigated over ﬁve different layer conﬁgurations.\\nFurthermore, to improve the classiﬁcation accuracy, an object-\\nbased classiﬁcation method also has been integrated with the deep\\nlearning strategy for more efﬁcient image classiﬁcation. Experi-\\nmental results indicate that with the combination of deep learning\\nand object-based classiﬁcation, it is possible to discriminate differ-\\nent building types in Beijing Scene, such as commercial buildings\\nand residential buildings with classiﬁcation accuracies above 90%.\\nIndex T erms —Convolutional neural network (CNN), deep\\nlearning, high-resolution image, image classiﬁcation.\\nI. I NTRODUCTION\\nREMOTE sensing imagery has provided a real-time and\\nlow-cost means to map urban land cover during the last\\nfew decades. The recent availability of submeter resolution im-\\nagery from advanced satellite sensors, such as WorldView-3\\nand GaoFen, can provide new opportunities for detailed urban\\nland cover mapping at the object level (such as commercial\\nbuildings and residential buildings). However, the complexity\\nManuscript received November 23, 2016; revised January 11, 2017 and Febru-\\nary 13, 2017; accepted March 5, 2017. Date of publication March 29, 2017;\\ndate of current version July 26, 2017. This work was supported by the National\\nNatural Science Foundation of China under Grant 41471315. (Corresponding\\nauthor: Shihong Du.)\\nW. Zhao and S. Du are with the Beijing Key Laboratory of Spatial In-\\nformation Integration and Its Applications, Institute of Remote Sensing and\\nGeographic Information System, Peking University, Beijing 100871, China\\n(e-mail: w.zhao@pku.edu.cn; dshgis@hotmail.com).\\nW. J. Emery is with the Colorado Center for Astrodynamics Research, Uni-\\nversity of Colorado, Boulder CO 80303 USA (e-mail: emery@colorado.edu).\\nColor versions of one or more of the ﬁgures in this paper are available online\\nat http://ieeexplore.ieee.org.\\nDigital Object Identiﬁer 10.1109/JSTARS.2017.2680324of urban imagery increases sharply as the observation scale gets\\nﬁner. More efﬁcient high-resolution image classiﬁcation meth-\\nods need to be proposed in order to efﬁciently perform urban\\nhigh-resolution imagery classiﬁcation.\\nRemote sensing imagery acquired by advanced satellite sen-\\nsors has the potential for detailed mapping of these urban\\nimages. However, rich spatial information presented in high-\\nresolution images also hinders accurate interpretation [1], [2].\\nIn fact, objects in urban areas are commonly composed of differ-\\nent construction materials, which can produce confused feature\\nrepresentations in the spectral and spatial domains. For one\\nthing, small objects are cojointly distributed around target ob-\\njects (such as antennas or chimneys on building roofs) which\\nresults in a strong intraclass variation as the spatial resolution\\ngets ﬁner. For another, man-made objects share similar spectral\\nproperties (such as parking lots and roofs with similar construc-\\ntion materials), which makes it even harder to accurately classify\\nhigh-resolution images [3], [4]. As a consequence, the spectral\\nand spatial responses from ground objects in urban scenes ex-\\nhibit complex patterns, especially for high-resolution images.\\nThe main purpose of this study is to transform the inefﬁcient\\nprocess of manually designed features into automatic feature\\nlearning by employing deep learning [5], for the efﬁcient clas-\\nsiﬁcation of high-resolution satellite imagery. However, due to\\nthe hierarchical structure of deep learning, deeply learned fea-\\ntures suffer greatly from abstraction and often fail to capture\\nthe objects’ contours at the pixel level. Therefore, pixel-level\\nsegments are combined to further improve the performances of\\ninterpretation in complex urban scenes [6]–[8].\\nThe proposed method is based on the analysis of deep features\\nextracted from high-resolution images with convolutional neural\\nnetworks (CNN). To account for the boundary information of ur-\\nban scenes, an object-based classiﬁcation method has been used\\nfor imagery interpretation for the combination of deeply learned\\nfeatures. Speciﬁcally, deep features have been computed over\\nthe ﬁxed receptive window with a ﬁve-layer CNN framework. In\\naddition, three different segment scales have been employed in\\nevaluating the effectiveness of object-based classiﬁcation with\\nextracted deep features.\\nThe remainder of this paper is organized as follows. Re-\\nlated work on high-resolution image feature exploration and\\nclassiﬁcation is outlined in Section II, and the datasets are de-\\nscribed in Section III. Detailed information about CNN-based\\ndeep learning feature exploration and the object-based classiﬁ-\\ncation are introduced in Section IV. Experimental results and\\nanalysis of the deep features as well as segmentation scales of\\n1939-1404 © 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\\nSee http://www.ieee.org/publications standards/publications/rights/index.html for more information.\\nAuthorized licensed use limited to: b-on: ISCTE. Downloaded on December 03,2023 at 19:35:37 UTC from IEEE Xplore.  Restrictions apply. ZHAO et al. : OBJECT-BASED CONVOLUTIONAL NEURAL NETWORK FOR HIGH-RESOLUTION IMAGERY CLASSIFICA TION 3387\\nobjects are discussed in Section V, followed by the conclusion in\\nSection VI.\\nII. R ELA TED WORK\\nA. Feature Design V ersus Feature Learning\\nOver the past few decades, intensive studies have focused on\\nhigh-resolution image classiﬁcation with handcrafted features\\nelaborated from both spectral and spatial domains. For spectral\\ninformation, the brightness of each image band is usually taken\\nas the primary feature for recognition of various objects in re-\\nmote sensing imagery. Later, semantic spectral features which\\ncontain physical meanings have been proposed for efﬁcient clas-\\nsiﬁcation of certain objects, where it strengthened the reﬂectance\\ndiscrepancy of different objects on speciﬁc wavelengths, such\\nas the normalized difference vegetation index (NDVI). Further\\nexploration of image texture demonstrated that texture-based\\ndescriptors characterize spectral variations that can provide sup-\\nplementary information for efﬁcient image classiﬁcation, such\\nas statistical descriptors based on the gray-level co-occurrence\\nmatrix (GLCM) [9], [10]. Compared with their spectral proper-\\nties, high-resolution imagery contains much richer information\\nin the spatial domain. To characterize these spatial features,\\nBenediktsson et al. proposed extended morphological proﬁles\\nfor high-resolution imagery classiﬁcation in urban areas [11]. It\\nefﬁciently captures spatial information by implementing mor-\\nphological operations (open and close) on high-resolution im-\\nages. Similarly, spatial ﬁlters (such as Gabor ﬁlters [12]) and\\nwavelet analysis [13] were also proposed for the extraction of\\nspatial features in the context of high-resolution images. How-\\never, it usually requires lots of experience and expert knowledge\\nfor end-users to deﬁne such elaborate features. Moreover, even\\nafter the complicated design process of various features, it is still\\ndifﬁcult to ﬁnd the most effective features for the recognition of\\ndifferent objects. To illustrate the complexity of high-resolution\\nimagery, a typical Worldview-2 image is presented here as Fig. 1.\\nDue to the strong intraclass variation of buildings (caused by\\na sunlit wall and chimney), traditional feature representations\\nhave shown much more mixture and variation than the deep\\nlearning ones. For this reason, it is necessary to explore more\\nefﬁcient and representative image features from high-resolution\\nimages in order to accurately recognize objects with complex\\npatterns, such as complex buildings in an urban area.\\nHow to develop automatic classiﬁcation schemes for feature\\nlearning has become one of the most signiﬁcant topics in image\\nclassiﬁcation over the last few years. Instead of handcrafted fea-\\nture design, Cheriyadat [14] introduced a sparse coding scheme\\nfor learning of high-resolution image features with predeﬁned\\nﬁlter banks. It indicated that handcrafted features have a certain\\nlevel of redundancy which leads to lower interpretation accura-\\ncies. Also, Tuia et al. [15] proposed a feature learning model by\\nusing sparse-constrained support vector machine (SVM). Re-\\ngardless of the compulsory step to select for predeﬁned features\\nof conventional methods, this method can automatically dis-\\ncover the relevant features in the potentially inﬁnite space of\\nimage features and choose them in a heuristic way. It concluded\\nthat the automatic learning scheme can keep image featuresmore compact, discriminative, and robust than human visual\\nselection methods, thus resulting in better classiﬁcation accu-\\nracies. In practical applications, it demonstrates that nonlinear\\nfeatures are more effective for class discrimination due to the\\nexistence of nonlinear class boundaries. Therefore, besides us-\\ning a sparse coding strategy, the exploration of image features\\nthrough nonlinear transformations also has been proven to be\\neffective [16]. Furthermore, Tuia et al. [17] proposed a sparse\\nand hierarchical feature learning model to ﬁnd efﬁcient data rep-\\nresentations for a good classiﬁcation. In this model, the selected\\nfeatures could be repeatedly used for ﬁltering at the next feature\\ngeneration step, thus, producing more representative features\\nwith higher nonlinearity. However, the above methods still re-\\nquire a certain level of experience for end-users, such as the\\ndeﬁnition of the possible feature space and a corresponding\\nselection criteria.\\nDeep learning [18], [19] is one of the signiﬁcant advances\\nin artiﬁcial intelligence, and it has shown great potential for\\ndiscriminative feature learning without human intervention. In-\\nspired by the hierarchical cognition process of the human brain,\\ndeep learning can automatically generate robust and represen-\\ntative features layer by layer in neural networks [20], [21]. Dis-\\ntinctly different from low-level feature representations, deeply\\nlearned features are generally more general and robust, and\\nit has demonstrated great effectiveness in image classiﬁcation,\\nsuch as face recognition [22] and scene classiﬁcation [23]. In the\\nremote sensing ﬁeld, several studies have focused on imagery\\nclassiﬁcation using deep learning models, such as stacked au-\\ntoencoder (SAE) and convolutional neural network (CNN). But,\\nthe original SAE focused on extracting one-dimensional spec-\\ntral features which probably is not sufﬁcient for high-resolution\\nimage interpretation. Therefore, Chen et al. [24] improved the\\nSAE model by introducing spatial features for the efﬁcient clas-\\nsiﬁcation of hyperspectral images. At the same time, the CNN\\nalgorithm became popular for high-resolution image classiﬁca-\\ntion due to its effectiveness in spatial feature exploration [3],\\n[25]–[27]. Furthermore, to ﬁt the character of remote sensing\\nimagery, Zhao and Du [28] proposed a multiscale strategy based\\non the CNN model to retrieve the information in high-resolution\\nimagery. Although the CNN-based method is efﬁcient for ro-\\nbust spatial feature extraction, two major ﬂaws of the basic CNN\\nframework for high-resolution image classiﬁcation also need to\\nbe clariﬁed. On one hand, the CNN framework feeds on la-\\nbeled image patches of ﬁxed sizes and outputs feature vectors\\nusing layer-wise activation and abstraction. Since the output\\nfeatures are highly abstract and usually without speciﬁc spatial\\narrangement, it is difﬁcult to predict the precise contour of target\\nobjects in image and producing blurred edge prediction results.\\nOn the other hand, deep features are learned from local images\\npatches, which, regardless of the contextual image information,\\ncould produce misclassiﬁcation results, as shown here in Fig. 2.\\nB. Per-Pixel V ersus Object-Based Classiﬁcation\\nAs the spatial resolution gets ﬁner, the detailed image data\\nexhibit complicated urban patterns in both spectral and spatial\\ndomains. Using the per-pixel classiﬁcation of high-resolution\\nAuthorized licensed use limited to: b-on: ISCTE. Downloaded on December 03,2023 at 19:35:37 UTC from IEEE Xplore.  Restrictions apply. 3388 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV A TIONS AND REMOTE SENSING, VOL. 10, NO. 7, JULY 2017\\nFig. 1. Measurements of different feature representatives on a Worldview-2 study site. The high-dimensional feature representations were project ed into two-\\ndimensional space for illustration. (a) Worldview-2 image; (b) GLCM feature representation; (c) EMAP feature representation; and (d) CNN feature r epresentation\\n(generated by ﬁve-layer CNN).\\nFig. 2. Classiﬁcation results of WorldView-2 image (after pan sharpening)\\nwith different strategies.\\nimages may lead to poorer interpretation results due to the “salt\\nand pepper effect” [29], as shown in Fig. 2(b). To reduce the\\npixel-level spectral heterogeneity and improve the classiﬁcation\\nperformance, an object-based approach can be used to efﬁciently\\ndelineate and classify high-resolution imagery [30]. Object-\\nbased classiﬁcation method takes image segments as building\\nblocks for the overall image analysis. In contrast to pixel-based\\napproaches, image objects mainly have two characteristics:\\n1) They are relatively homogeneous and 2) they can provide rich\\nimage features. On one hand, objects integrated with contextual\\ninformation can greatly reduce local spectral variation and over-\\ncome the so-called “pepper & salt” effect. On the other hand,\\nobjects in images opens a new gate to access the target of in-\\nterest, thus more discriminant semantic features can be deﬁned,\\nsuch as an objects’ shape and sizes. So far, object-based classi-\\nﬁcation method has shown great potential for efﬁcient mapping\\nof high-resolution images, especially for complex urban scenes.\\nHowever, the process of object-based classiﬁcation is usually\\nvery complicated and needs a certain level of expert knowledge\\nto produce satisfactory results. Speciﬁcally, three factors weigh\\nheavily in terms of object-based classiﬁcation accuracy: 1) the\\nsegmentation scale, 2) feature extraction and selection, and 3)\\nclassiﬁcation rules. A possible solution to these problems is to\\nincorporate deep learning with the object-based method for fea-\\nture self-learning and automatic classiﬁcation of complex urban\\nFig. 3. Outline of the proposed method. High-resolution images and their\\nreference maps are combined to train CNN frameworks. Segmentation results\\nare used to improve the performance of pixel-level boundary prediction.\\nhigh-resolution imagery without much supervision. The general\\noutline of this process is presented in Fig. 3.\\nIII. M ETHODOLOGY\\nA. Deep Feature Learning Through CNN\\nThe complexity of high-resolution images causes traditional\\nhuman-dependent classiﬁcation methods to fail due to the lim-\\nited representation power of handcrafted features. CNN as the\\ncore of deep learning has shown great potential for robust au-\\ntomatic feature extraction and complex object recognition in\\nhigh-resolution images [31]–[33].\\nTo obtain deep feature representations, two parts of trainable\\nparameters for a CNN framework should be determined, i.e.,\\nthe ﬁlters Wand the biases b, which are collectively denoted θ.\\nDuring the training stage, a CNN framework fwithLlay-\\ners feeds with training samples Xi,i∈{ 1,...,N }and is\\nformulated\\nf(X;θ)= WLhL−1+bl (1)\\nwhere hl,l∈{ 1,...,L −1}denotes the vector of hidden\\nunits at the lth layer. Particularly, h0represents the original\\ninput data, as shown in Fig. 4.\\nMore speciﬁcally, when training a CNN, the input image is\\nﬁrst connected with convolution layers by a set of convolution\\nkernel banks. For the convolutional layers, the kernels were\\ndenoted as Wland are combined with the bias terms blto\\nconvolute the input image. After that, a point-wise nonlinear\\nactivation function g(·)(typically the tan hfunction) is deployed\\nbefore the ﬁnal output of this layer. Then, the spatial pooling\\nlayer usually follows to generate the dominant features over\\nAuthorized licensed use limited to: b-on: ISCTE. Downloaded on December 03,2023 at 19:35:37 UTC from IEEE Xplore.  Restrictions apply. ZHAO et al. : OBJECT-BASED CONVOLUTIONAL NEURAL NETWORK FOR HIGH-RESOLUTION IMAGERY CLASSIFICA TION 3389\\nFig. 4. Framework of a traditional CNN. Convolution layer interspersed with\\npooling layer in this hierarchical structure. Full connection is found at the last\\nlayer and linked with the classiﬁer.\\nnonoverlapping windows for each feature map. To formulate\\nthe feed-forward process, we have\\nhl=pool (g(hl−1∗Wl+bl)). (2)\\nOnce the parameters θare trained, the unlabeled datasets Yj,\\nj∈{ 1,2,...,N }can be encoded by\\nFj=f(Yj,θ). (3)\\nAs mentioned above, deep features extracted by the CNN\\nframework are generally robust and effective for complex image\\npattern descriptions, especially for the case of high-resolution\\nurban scenes. Unlike the traditional CNN-based image classi-\\nﬁcation methods, we focus on exploring deep features as local\\npatch descriptors. However, deep features with high-level ab-\\nstractions naturally fail to detect the edges of complex objects\\non the pixel level. Object-based classiﬁcation methods interpret\\nhigh-resolution images with segmented objects which can pre-\\nserve objects’ edges and reduce spectral variation effects. There-\\nfore, it is widely suggested that deep features should combine\\nwith object-based image analysis methods for better classiﬁca-\\ntion performances.\\nB. Object-Based Classiﬁcation With Deep Features\\nAlthough, the CNN-based methods are efﬁcient for complex\\npattern description, the deep features generated from multilayer\\nactivations are usually highly abstracted, and thus, failed to\\npredict objects’ contours [34], [35]. In contrast to the patch-\\nbased image classiﬁcation methods (e.g., CNN-based method),\\nthe object-based methods can effectively classify image objects\\nusing image segments with precise boundaries. Segments are\\nhomogeneous regions which are generated by one or more ho-\\nmogeneity measurements in feature space (typically, spectral\\nspace). Therefore, image segments can transform the whole im-\\nage into meaningful regions while preserving the precise edges\\nof targets of interest. Furthermore, image segments have ad-\\nditional information compared to pixels or image patches in\\nterms of spatial and contextual extensions (objects’ shape and\\ntopologies). Thus, it is natural to combine the deep features\\nwith object-based classiﬁcation for the accurate interpretation\\nof high-resolution images.\\nIn this paper, highly abstracted deep features are combined\\nwith image segments for precise mapping of complex remote\\nsensing images. Speciﬁcally, the multiresolution segmentation\\nalgorithm is used to generate image objects with precise edges.\\nFig. 5. Flowchart of object-based high-resolution image classiﬁcation com-\\nbining with deep features.\\nFig. 6. Illustration of the CNN-based deep learning framework.\\nThen, with the shape constraint of the image objects, deep fea-\\ntures are combined with objects’ features on the pixel level and\\nused for image classiﬁcation. Finally, for each image object,\\nthe optimal statistical method was applied to determine the land\\ncover class for mapping.\\nSuppose an image Icontains Nimage objects Oi,i∈\\n{1,2,...,N }, and, there are Mpixels Ij,j∈{ 1,2,...,M }\\ninside object Oi. For each pixel Ij, the deep feature representa-\\ntion is denoted as Fj. Similarly, the object-based features (such\\nas, NDVI) are represented as Rj. For feature combination pur-\\npose, the deep features are stacked with object-based properties\\nUj=[Fj,Rj]for joint feature classiﬁcation of the original im-\\nagery at the pixel level. For this method, the label of an object\\nOiis predicted from the majority statistics of feature vectors Uj\\nusing a two-layer neural network:\\npj=W 2tanh(W 1Uj+b1) (4)\\ndi,a=1\\nt(Oi)/summationdisplay\\nj∈[1,M ]count (pj==a). (5)\\nMatrices W 1and W 2are the trainable parameters of the\\ntwo-layer neural network classiﬁer. pjis the predicted label at\\nlocation j, andt(Oi)is the pixel number of the object. The ﬁnal\\nlabel for each image object Oiis given by\\nli= arg max\\na∈classesdi,a. (6)\\nThe ﬂowchart of object-based deep learning classiﬁcation is\\ndepicted in Fig. 5.\\nIV . D A TASETS\\nThe datasets analyzed include three very different cities with\\ncomplex urban conditions: Beijing, Pavia, and V aihingen. The\\nﬁrst Beijing scene was acquired by Worldview-2 in 2010. The\\nsecond scene was acquired by the ROSIS sensor during a ﬂight\\ncampaign over Pavia, northern Italy, and was distributed in 2008\\nas part of a test of the airborne Intergraph/ZI Digital Mapping\\nAuthorized licensed use limited to: b-on: ISCTE. Downloaded on December 03,2023 at 19:35:37 UTC from IEEE Xplore.  Restrictions apply. 3390 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV A TIONS AND REMOTE SENSING, VOL. 10, NO. 7, JULY 2017\\nFig. 7. Study datasets and ground truth reference maps.\\nCamera Instrument. The V aihingen dataset was provided by the\\nGerman Association of Photogrammetry and Remote Sensing.\\nA. Description of Scenes\\nThe Beijing scene, as shown in Fig. 7(a), contains 1036 ×\\n1146 pixels, and it has eight multispectral bands with a spatial\\nresolution of 0.5 m (after pan sharpening). It contains typical\\nresidential buildings and examples of commercial structures\\nwith similar heights (about ﬁve or six ﬂoors) but of differ-\\nent sizes. The high-resolution Beijing image was chosen for\\ntwo reasons. First, it embodies the main difﬁculties of high-\\nresolution classiﬁcation, especially for the detailed mapping\\nof complex buildings. Second, the Beijing scene represents a\\ncommon Chinese urban landscape, including ﬂat building and\\nnarrow streets, which are obviously different from western style\\nof modern cities. The other two study areas, Pavia and V aihin-\\ngen are shown in Fig. 7(c) and (e). The Pavia scene, contains\\n1096 ×715 pixels and 102 hyperspectral bands, with a spatial\\nresolution of 1.3 m. It is composed of an elaborate urban lattice\\nwith complex structures showing a great variety in dimension,\\nshapes, and heights. The image size of V aihingen dataset is 892\\n×1498 (reduced half resolution for efﬁcient computation), with\\nvery high spatial resolution of 0.09 m per pixel. For better dis-\\ncrimination, the band combination of V aihingen dataset was set\\nto infrared (IR), red (R), and green (G). The urban scene of V ai-\\nhingen contains elements that differ from the other two, sinceit is mainly composed of small residential houses (two or three\\nﬂoors), roads, some sparse trees, and vegetated areas.\\nB. Classes, Training, and V alidation Set Deﬁnition\\nTo efﬁciently map different scenes, several land cover types\\nof interest have been identiﬁed. For the Beijing case, the pri-\\nmary goal was to discriminate between residential buildings and\\ncommercial buildings based on the differences in construction\\nstyle and size. These two kinds of buildings appear with com-\\nplex elements, such as a small chimney on the top of a building\\nand sunlit wall with great spectral variation. Moreover, roofs of\\ncommercial buildings are similar to roads in terms of spectral\\nreﬂectance and shapes, which further increased the difﬁculty of\\nperforming a successful image classiﬁcation. A further explo-\\nration was made to distinguish the different uses of the asphalt\\nsurfaces, which included parking lots and roads. Other classes\\nsuch as shadow, impervious, trees, and bare soil were also added\\nfor a total of eight classes. Since vegetation occupies a large por-\\ntion of urban scenes, three bands with the combining IR, R, and\\nG were chosen to discriminate vegetation from structures.\\nSimilar to the previous dataset, the Pavia center also has\\ncrowded residential buildings. Generally, the buildings change\\nrapidly in terms of sizes and spectral reﬂectance where the left\\npart of Pavia dataset contains with old constructions, while the\\neastern part of the city new buildings are mainly made of con-\\ncrete. It is possible to discriminate between asphalt (roads) and\\nbitumen (newly developed roofs). Other classes of interest were\\nalso identiﬁed, including self-blocking bricks, water, trees, shad-\\nows, meadows, and bare soil for a total of nine classes. In order\\nto reduce the computational cost and preserve the substantial\\ninformation across spectral bands, we reduced the dimensional-\\nity of Pavia dataset by using principle component (PC) analysis.\\nFollowing the previous studies [28], the ﬁrst three PCs hold\\nmore than 95% of the original spectral information. Thus, sim-\\nilar to the IR, R, and G bands, we selected the ﬁrst three PCs to\\nfeed the CNN model.\\nThe V aihingen scene has features different from the previous\\ntwo. It has a ﬁner spatial resolution that can resolve more com-\\nplex and challenging urban patterns, such as road markings and\\neven car windows. Buildings with large varieties in shapes and\\nsizes are interspersed with roads. It is also possible to distinguish\\nshort vegetation and trees at this level of resolution. Other com-\\nmon classes like water, roads, and buildings were also included\\nfor classiﬁcation.\\nThe reference map for each scene were reported in Fig. 7(b),\\n(d), and (f), respectively. Ground reference labels have been\\nacquired by careful visual interpretation. Both the V aihingen\\nscene and its reference map were distributed by the International\\nSociety for Photogrammetry and Remote Sensing (ISPRS) and\\nthe two-dimensional semantic labeling contest [36].1Instead of\\nusing the selected pixels, we have extended them into image\\npatches with the labeled pixels in the center before feeding\\nthem to the CNN with its deep features. In the experiments,\\n1The V aihingen dataset was provided by the German Society for Photogram-\\nmetry, Remote Sensing and Geoinformation (DGPF) [36], http://www.ifp.uni-\\nstuttgart.de/dgpf/DKEP-Allg.html.\\nAuthorized licensed use limited to: b-on: ISCTE. Downloaded on December 03,2023 at 19:35:37 UTC from IEEE Xplore.  Restrictions apply. ZHAO et al. : OBJECT-BASED CONVOLUTIONAL NEURAL NETWORK FOR HIGH-RESOLUTION IMAGERY CLASSIFICA TION 3391\\nTABLE I\\nBEIJING SCENE :CLASSES ,COLOR LEGEND ,TRAINING ,AND VALIDA TION\\nSAMPLES\\nTABLE II\\nPAV I A SCENE :CLASSES ,COLOR LEGEND ,TRAINING ,AND VALIDA TION\\nSAMPLES\\nTABLE III\\nVAIHINGEN SCENE :CLASSES ,COLOR LEGEND ,TRAINING ,AND VALIDA TION\\nSAMPLES\\nwe have subsampled the original image in order to reduce the\\ncomputation complexity.\\nTo demonstrate the robustness of the proposed method, for\\neach dataset, we randomly select 10% nonoverlapping sam-\\nples (NOS) for training and another 5% for the test. In or-\\nder to avoid the overlap phenomenon between samples, the\\noverlap threshold is set to 80% (rejected if the overlap area is\\nmore than 80% between two samples). Detailed information\\nabout the training samples (TR), test samples (TE), and to-\\ntal NOS are reported in Tables I, II, and III for the Beijing,\\nPavia, and V aihingen cases, respectively. The overall accuracy\\nand Kappa coefﬁcient were used to evaluate the classiﬁcation\\nperformance.V. E XPERIMENTS AND ANALYSIS\\nIn this section, we evaluate the capability of the object-based\\ndeep learning method to classify the complex urban scenes de-\\nscribed previously. As mentioned above, we combine the pow-\\nerful representations of deep features with shape-preserving\\nobject-based classiﬁcation method for accurate high-resolution\\nimage interpretation. More speciﬁcally, the CNN-based deep\\nfeature learning algorithm has been adopted to describe com-\\nplex urban patterns. The depth of the CNN can directly affect\\nthe abstraction level of our deep features. A detailed analysis\\nof this depth effect has been carried out by assigning differ-\\nent layer conﬁgurations on the CNN framework, as described\\nin Section V-A. Then, the effect of segmentation scales and\\nclassiﬁcation accuracies is illustrated in Section V-B. In addi-\\ntion, deep features are combined with image objects for efﬁ-\\ncient urban mapping, the classiﬁcation results are reported in\\nSection V-C.\\nA. Depth Effect on Deep Feature Learning\\nThe CNN-based deep learning framework has been used to\\nextract complex urban patterns in this work. Instead of hand-\\ncrafted features, the CNN network automatically learn effective\\nfeature representations from the hierarchical activation struc-\\nture. Two parameters, the input size of the training samples and\\nthe depth of the CNN play important roles in terms of classiﬁ-\\ncation accuracies.\\nFor the input training samples (also known as the receptive\\nﬁeld), it should have an appropriate spatial coverage for the\\nobjects of interest. To achieve this goal, we set the window\\nsize of training sample extraction to 18 ×18 pixels (about\\n9, 23.4, and 3.24 m, for Beijing, Pavia, and V aihingen scene,\\nrespectively). The majority of urban geographical objects have\\nthe dominant scales between about 3.0 and 24.0 m [37].\\nThe depth conﬁguration of the CNN framework controls the\\nrobustness of deep features in terms of their abstraction level.\\nThe representation power increases as the CNN involves more\\nlayers. However, due to the constraint of input training sam-\\nple sizes, the deepest conﬁguration of the CNN should be no\\nmore than ﬁve layers with the 3 ×3 kernels in our network.\\nFollowing to the previous studies [3], [28], we set the number\\nof features to 20 for each convolutional layer (except the last\\nlayer). Fig. 6 presents the detailed conﬁguration of our CNN-\\nbased deep learning framework. The learning rate was set to\\n0.001.\\nIn order to illustrate the importance of CNN depth when\\nclassifying high-resolution images, we separately trained CNN\\nmodels with different depth that varies from 1 to 5. That is, one-\\nlayer CNN only has a single convolution layer, teo-layer CNN is\\ncombined with two convolution layers, three-layer CNN has the\\ncombination of two convolution layers and one pooling layer,\\nfour-layer CNN constitutes by three convolution layers and one\\npooling layer, and ﬁve-layer CNN is with three convolution\\nlayers and two pooling layers. The classiﬁcation results using\\ndifferent CNN models are reported in Tables IV–VI. The overall\\nclassiﬁcation accuracy is calculated to measure the classiﬁcation\\nperformance of the CNN networks.\\nAuthorized licensed use limited to: b-on: ISCTE. Downloaded on December 03,2023 at 19:35:37 UTC from IEEE Xplore.  Restrictions apply. 3392 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV A TIONS AND REMOTE SENSING, VOL. 10, NO. 7, JULY 2017\\nTABLE IV\\nCLASSIFICA TION ACCURACIES (INPERCENTAGE )OFWORLDVIEW DATA S E T WITHDIFFERENT CONFIGURA TION CNN S\\nCNNs CB RB RD VG PL SD IM BS OA\\nOne-layer 60.64 95.27 35.40 81.19 13.68 4.43 0 0 68.45\\nTwo-layers 97.38 91.02 76.46 99.68 64.90 17.65 76.18 46.26 89.30\\nThree-layers 99.09 97.48 87.65 100 76.86 97.44 78.97 78.91 95.89\\nFour-layers 99.04 98.79 88.73 99.98 77.94 97.58 66.58 59.31 96.06\\nFive-layers 99.40 98.10 88.80 99.98 79.47 96.08 78.32 69.53 96.29\\nCB: Commercial building, RB: Residential building, RD: Road, VG: V egetation, PL: Parking lot, SD:\\nShadow, IM: Impervious, BS: Bare soil.\\nTABLE V\\nCLASSIFICA TION ACCURACIES (INPERCENTAGE )OFPAV I A DATA S E T WITHDIFFERENT CONFIGURA TION CNN S\\nCNNs W A TR ME BB BS AS BI TI SD OA\\nOne-layer 100 98.66 85.67 44.77 72.61 93.83 87.39 97.59 99.22 95.64\\nTwo-layers 99.96 95.60 92.28 53.45 80.17 95.16 84.96 96.82 93.58 95.72\\nThree-layers 99.98 96.86 93.88 82.72 89.92 95.64 89.06 99.14 95.77 97.77\\nFour-layers 99.96 96.74 92.32 85.81 90.18 92.29 84.71 98.31 94.46 97.10\\nFive-layers 99.95 97.50 90.18 90.69 92.90 93.91 89.41 98.42 94.74 97.69\\nW A: Water, TR: Tree, ME: Meadow, BB: Bricks, BS: Bare soil, AS: Asphalt, BI: Bitumen, TI: Tiles, SD: Shadow.\\nTABLE VI\\nCLASSIFICA TION ACCURACIES (INPERCENTAGE )OFPAV I A DATA S E T WITH\\nDIFFERENT CONFIGURA TION CNN S\\nCNNs BD W A TR GR CA RD OA\\nOne-layer 67.29 0.01 88.29 43.66 1.49 69.23 63.27\\nTwo-layers 75.50 90.40 90.39 42.36 3.64 75.98 75.64\\nThree-layers 86.52 94.84 91.92 66.75 38.46 78.47 83.14\\nFour-layers 82.68 75.15 90.13 54.10 26.48 89.53 82.13\\nFive-layers 86.60 95.99 94.29 62.58 40.49 90.38 87.14\\nBD: Buildings, W A: Water, TR: Trees, GR: Grass, CA: Cars, RD: Road.\\nFig. 8. In (A), (B), and (C) is shown the classiﬁcation results by using ﬁve-\\nlayer CNN network over Worldview-2, Pavia, and V aihingen complex urban\\nscene, respectively.\\nThe depth parameter of the CNN framework signiﬁcantly\\nimpacts the classiﬁcation performance in terms of accuracy. As\\nstated above, classiﬁcation accuracies become greater if feature\\nrepresentations get deeper and deeper. Therefore, the deepest\\nCNN conﬁguration will obtain the best classiﬁcation results. The\\nclassiﬁcation results using a ﬁve-layer conﬁguration of CNN are\\npresented in Fig. 8. To quantitatively evaluate the accuracy of\\nthe classiﬁcation results for these three datasets, we reported\\nFig. 9. Classiﬁcation results with 10, 20, 30 segmentation scales for\\nWorldview-2 (a)–(c), Pavia center (d)–(f), and V aihigen (g)–(i).\\nthe classiﬁcation accuracies and their variations over different\\ndepth, as shown in Tables IV–VI. In general, these tables indicate\\nthat the overall classiﬁcation accuracy increase as the CNN gets\\ndeeper. However, for the cars in V aihingen dataset and bare soil\\nin the scene of Worldview-2, there is a signiﬁcant drop with\\nthe deeper conﬁguration in terms of classiﬁcation accuracies.\\nThe reason for this phenomenon is probably that the bare soil\\ndoes not have a ﬁxed spatial pattern but mainly depends on\\nspectral reﬂectance for discrimination. The other classes with\\nﬁxed spatial patterns become well classiﬁed as the network gets\\nAuthorized licensed use limited to: b-on: ISCTE. Downloaded on December 03,2023 at 19:35:37 UTC from IEEE Xplore.  Restrictions apply. ZHAO et al. : OBJECT-BASED CONVOLUTIONAL NEURAL NETWORK FOR HIGH-RESOLUTION IMAGERY CLASSIFICA TION 3393\\ndeeper. We also noticed that the classiﬁcation results of the\\nV aihingen dataset suffer from the “pepper & salt” affect, which\\nalso reduced the classiﬁcation accuracy. Therefore, to further\\nincrease the classiﬁcation accuracy, it is advisable to combine\\nimage segments with deeply abstracted features by introducing\\naccurate boundary information.\\nB. Segmentation Scale Effects\\nAlthough deep features are efﬁcient in terms of complex pat-\\ntern description, they fail to predict image objects edges due\\nto the highly abstracted feature representations. To improve the\\nclassiﬁcation accuracy through deep learning, an object-based\\ncomplementary strategy is necessary. The object-based method\\novercomes local spectral variation and provides accurate edge\\nrealizations with the image segments. Furthermore, the image\\nobjects have access to geographic entities, thus object-based im-\\nage features with more semantic meanings could be utilized for\\nbetter discrimination of complex urban scenes.\\nThe multiresolution segmentation algorithm [38] is applied to\\ngenerate image objects through experiments. Three key param-\\neters need to be determined before the segmentation algorithm\\nis deployed, namely they are scale ( Ssc), shape ( Ssh), and com-\\npactness ( Scm). The scale parameter directly controls the size of\\nsegmented objects by matching the required level of detail. In\\nthe experiments, we set the shape parameter Sshand compact-\\nness parameter Scmto 0.1 and 0.5, respectively. To evaluate the\\nsegmentation scale effects on classiﬁcation results, we visually\\nchecked segmented objects by considering scale parameter Ssc\\nvarieties from 10 to 30. As reported in [39], the lowest scale\\nlevel presented the best classiﬁcation performance. Therefore,\\nto provide accurate edges for deep learning-based classiﬁcation,\\nwe set oversegmentation scales 10, 20, and 30 for each image\\ndataset. For each object, the mean brightness of each band and\\nthe NDVI are selected as object-based features.\\nWith three different segmentation scales, the classiﬁcation\\nresults of three datasets are reported in Fig. 9. To quantitatively\\nevaluate the performance of the object-based classiﬁcation re-\\nsults, we present the detailed information about segmented im-\\nage objects and the variation of classiﬁcation accuracies at dif-\\nferent segmentation scales. The number of segmented objects\\nfor each dataset is presented in Fig. 10(a). Classiﬁcation results\\nof each dataset are reported in Fig. 10(b)–(d). These results\\nindicate that the classiﬁcation accuracy signiﬁcantly drops as\\nthe segmentation scale gets larger. Therefore, we can conclude\\nthat oversegmentation is more suitable for deep feature-based\\nimage interpretation, especially for high-resolution complex ur-\\nban scenes.\\nC. Method Comparison\\nTo further illustrate the effectiveness of the object-based\\nCNN, we compared this proposed classiﬁcation method with\\nseveral traditional classiﬁcation methods by classifying all\\nthree datasets. More speciﬁcally, we compared the object-CNN\\n(OCNN) with SVM (spectral features), extended morphologi-\\ncal attribute proﬁles method (EMAP), spectral+EMAP , pixel-\\nbased CNN (PCNN), spectral and spatial feature classiﬁcation\\n(SSFC)[3], and the multiscale CNN (MCNN)[28]. During thecomparison, the EMAP features are built using the area (related\\nto the size of the regions) and standard deviation (which mea-\\nsures the homogeneity of the pixels enclosed by the regions)\\nattributes. The threshold values of area are chosen in the range\\nof{50,500 }and standard deviation ranging from 2.5% to 20%.\\nThe classiﬁcation maps are shown in Fig. 11, and the classiﬁca-\\ntion accuracies are reported in Tables VII–IX. For the traditional\\nmethods, we investigated the pure spectral information, EMAP\\nfeatures and spectral-EMAP features for image classiﬁcation.\\nThe default classiﬁer for traditional methods is linear SVM,\\nthe parameter of SVM classiﬁer selected by ﬁve-cross valida-\\ntion. However, due to the low-level feature similarities between\\ndifferent classes (such as roads and roofs), the predicted maps\\nwith traditional methods suffer a lot from misclassiﬁcation and\\nnoises. Different from using the traditional feature descriptors,\\nthe PCNN explored high-level feature representations with hier-\\narchical framework. Therefore, the PCNN-based classiﬁcation\\nresults are more robust and accurate. However, it fails to cap-\\nture the spectral information over different image bands. The\\nSSFC method was proposed to integrate the rich spectral infor-\\nmation and high-level spatial features. Thus, the SSFC method\\nachieved better classiﬁcation results, in terms of classiﬁcation\\naccuracies. The geographical objects in remote sensing images\\nare commonly displayed in different scales (e.g., roofs with\\ndifferent sizes). But, both PCNN and SSFC are not able to\\ncapture the useful information over scales. The MCNN was\\ndesigned to model image objects over different scales. As a\\nresult, the classiﬁcation results of MCNN are better than the\\nprevious two models. The PCNN, SSFC, and MCNN explored\\ndeep features from image patches, which have overlooked the\\nboundary information of image objects. To overcome this prob-\\nlem, the object-based CNN is presented. It combined the robust\\ndeep features and accurate boundaries from image segments\\nfor better classiﬁcation of high-resolution images. In this work,\\nwe chose a ﬁve-layer CNN for deep feature extraction and the\\nscale parameter for image objects generation was set to 10. The\\nclassiﬁcation results of OCNN demonstrated that the strategy of\\ncombining deep features and image objects is effective, in terms\\nof classiﬁcation accuracies.\\nAs shown in Fig. 11, the classiﬁcation maps directly classiﬁed\\nby using spectral information or EMAP features suffer greatly\\nfrom the “pepper and salt” effect. The CNN-based methods\\nproduce smoothing and accurate results, especially for the class\\nbuilding. Although the previous CNN methods are effective\\nfor complex spatial feature extraction, it shows less strength in\\ncapturing boundary information. For the complex classes (such\\nas buildings and roads), the classiﬁcation results of using PCNN,\\nSSFC, and MCNN are less promising. However, by integrating\\nimage objects and deep features, the classiﬁcation accuracies\\nfor all the datasets are increased sharply.\\nVI. D ISCUSSION\\nIn this study, we propose an object-based CNN for the clas-\\nsiﬁcation of high-resolution images. Feature extraction is one\\nof the biggest challenges for the analysis of remote sensing\\nimages. Traditional image classiﬁcation methods require nu-\\nmerous image features to be empirically designed and linked to\\nAuthorized licensed use limited to: b-on: ISCTE. Downloaded on December 03,2023 at 19:35:37 UTC from IEEE Xplore.  Restrictions apply. 3394 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV A TIONS AND REMOTE SENSING, VOL. 10, NO. 7, JULY 2017\\nFig. 10. Quantitative evaluation of segmentation scale on classiﬁcation accuracies. (a) Number of objects. (b) Worldview-2 classiﬁcation results . (c) Pavia center\\nclassiﬁcation results. (d) V aihingen classiﬁcation results.\\nFig. 11. Classiﬁcation results for the Worldview-2, Pavia center, and V aihingen datasets, respectively. (a) SVM classiﬁcation, (b) EMAP-based cla ssiﬁcation,\\n(c) spectral and the EMAP combined classiﬁcation, (d) pixel-based CNN classiﬁcation, (e) SSFC classiﬁcation, (f) MCNN classiﬁcation, and (g) objec t-based\\nclassiﬁcation.\\nAuthorized licensed use limited to: b-on: ISCTE. Downloaded on December 03,2023 at 19:35:37 UTC from IEEE Xplore.  Restrictions apply. ZHAO et al. : OBJECT-BASED CONVOLUTIONAL NEURAL NETWORK FOR HIGH-RESOLUTION IMAGERY CLASSIFICA TION 3395\\nTABLE VII\\nCLASSIFICA TION ACCURACIES OF THE WORLDVIEW -2 D ATA S E T\\nSVM EMAP SEMAP PCNN SSFC MCNN OCNN\\nCB 91.74 86.70 91.84 98.15 98.36 99.56 99.34\\nRB 67.68 91.23 94.17 95.66 96.37 97.92 98.71\\nRD 29.61 7.12 47.43 86.52 90.73 93.69 94.56\\nVG 33.28 83.81 94.45 99.36 98.13 99.97 99.67\\nPL 7.37 8.37 14.75 80.95 91.67 84.66 88.67\\nSD 62.68 45.57 4.83 76.83 89.20 98.81 99.03\\nIM 19.91 28.43 32.80 62.28 74.14 80.27 86.62\\nBS 5.27 18.48 23.85 46.03 45.52 74.90 66.58\\nAA 39.69 46.21 50.51 80.72 85.52 91.22 91.65\\nOA 66.47 74.97 82.16 93.78 95.29 97.11 97.45\\nKP 48.72 61.52 73.16 91.03 93.26 95.86 96.49\\nBoth spectral and EMAP feature representations were classiﬁed by the SVM\\nclassiﬁer. OCNN refers to the object-based CNN method.\\nTABLE VIII\\nCLASSIFICA TION ACCURACIES OF THE PAV I A CENTER DATA S E T\\nSVM EMAP SEMAP PCNN SSFC MCNN OCNN\\nW A 99.79 100 99.98 100 100 100 100\\nTR 92.95 98.99 93.18 97.50 95.56 99.10 99.53\\nME 79.06 24.40 82.78 97.09 99.58 98.70 99.81\\nBB 53.59 88.34 70.24 98.51 99.18 99.78 99.93\\nBS 41.18 85.27 65.42 97.94 99.94 99.98 99.97\\nAS 91.71 95.49 96.40 99.72 99.94 99.98 100\\nBI 81.47 90.30 81.62 96.25 99.20 99.89 99.82\\nTI 97.74 99.47 98.76 99.53 99.66 99.93 99.98\\nSD 69.58 92.21 96.23 99.37 99.93 100 100\\nAA 78.56 86.05 87.18 98.32 99.13 99.67 99.88\\nOA 92.98 96.44 95.65 99.34 99.61 99.90 99.95\\nKP 89.94 94.94 93.80 99.07 99.44 99.86 99.94\\nBoth spectral and EMAP feature representations were classiﬁed by the SVM clas-\\nsiﬁer. OCNN refers to the object-based CNN method.\\nTABLE IX\\nCLASSIFICA TION ACCURACIES OF THE VAIHINGEN DATA S E T\\nSVM EMAP SEMAP PCNN SSFC MCNN OCNN\\nBD 58.41 78.36 94.50 96.00 96.37 96.20 96.40\\nW A 43.61 68.64 79.80 98.15 98.85 94.69 97.73\\nTR 90.01 87.83 86.86 93.30 95.48 94.68 95.70\\nGR 11.99 35.51 12.94 83.25 86.55 88.02 81.87\\nCA 2.84 11.81 13.27 62.06 70.81 68.78 66.05\\nRD 85.70 80.30 74.37 92.05 91.16 93.23 94.27\\nAA 48.33 58.64 52.82 87.54 89.87 89.27 88.67\\nOA 66.60 74.64 75.51 92.77 92.41 92.60 93.84\\nKP 54.60 65.95 62.81 90.47 91.35 91.57 91.87\\nBoth spectral and EMAP feature representations were classiﬁed by the SVM clas-\\nsiﬁer. OCNN refers to the object-based CNN method.\\nthe characteristics of different images, which is time-consuming\\nand often fails to achieve accurate interpretation. Unlike these\\ntraditional methods, in this paper, the CNN as the character-\\nistic deep learning method was chosen for automatic feature\\nlearning. With the hierarchical structure of the CNN, image fea-\\ntures at higher levels can be automatically extracted and the\\nmethod has shown robustness and good accuracy in the pres-\\nence of complex targets. As the abstraction level increased,\\nthe extracted deep features demonstrated strong invariance in\\nterms of semantic content. However, the method often fails tocapture boundary information of the target and suffers from the\\nwell-known “pepper & salt” effect.\\nTo further improve the quality of our CNN-based classiﬁca-\\ntion results, we suggested to combine the CNN-based features\\nwith the low-level image segments for better descriptions of\\ntargets’ boundary and to reduce the “pepper & salt” effect. As\\ndemonstrated in our experiments, the combination of image\\nobjects and deep features is quite effective. For one thing, it\\nalleviates people from the time-consuming process of feature\\nselection. For another, the combination of object-based image\\ninterpretation and deep learning method provides accurate tar-\\ngets’ boundary information as well as semantic labels.\\nVII. C ONCLUSION\\nIn this paper, we propose an effective way to classify high-\\nresolution images by combining deep features and image ob-\\njects. Compared to the traditional classiﬁcation methods, the\\nproposed procedure utilizes deep CNN framework to automat-\\nically extract robust and discriminative features for complex\\nurban objects classiﬁcation (such as building roofs and cars).\\nTo evaluate the effectiveness of deep features, we tested the\\nCNN framework with ﬁve different layer conﬁgurations for the\\nclassiﬁcation of high-resolution imagery. However, as the CNN\\nframework gets deeper, the generated features become more\\nand more robust but often too abstract (overlooked the shape\\nof the target objects) to describe boundary information. Com-\\nplementary, the object-based classiﬁcation method can preserve\\nedge information in complex urban scenes which can be inte-\\ngrated with the highly abstracted deep features. As a solution, an\\nobject-based classiﬁcation method is combined with deep fea-\\ntures to promote the interpretation accuracy of high-resolution\\nimages. Experimental results indicate that the combination of\\ndeep learning and object-based classiﬁcation method is effective\\nfor mapping complex urban image datasets.\\nHowever, the proposed method has no access to the contextual\\ninformation at the global level. The relationships between image\\nobjects are also important to promote the classiﬁcation results.\\nTherefore, we focus on modeling the contextual information\\nwith deep feature-based image objects and for the improved\\nimage classiﬁcation.\\nREFERENCES\\n[1] D. Tuia, F. Ratle, F. Paciﬁci, M. F. Kanevski, and W. J. Emery, “Active\\nlearning methods for remote sensing image classiﬁcation,” IEEE Trans.\\nGeoscience Remote Sensing , vol. 47, no. 7, pp. 2218–2232, 2009.\\n[2] L. Bruzzone and L. Carlin, “A multilevel context-based system for clas-\\nsiﬁcation of very high spatial resolution images,” IEEE Trans. Geosci.\\nRemote Sens. , vol. 44, no. 9, pp. 2587–2600, Sep. 2006.\\n[3] W. Zhao and S. Du, “Spectral-spatial feature extraction for hyperspectral\\nimage classiﬁcation: A dimension reduction and deep learning approach,”\\nIEEE Trans. Geosci. Remote Sens. , vol. 54, no. 8, pp. 4544–4554, Aug.\\n2016.\\n[4] F. Zhang, B. Du, L. Zhang, and M. Xu, “Weakly supervised learning\\nbased on coupled convolutional neural networks for aircraft detection,”\\nIEEE Trans. Geosci. Remote Sens. , vol. 54, no. 9, pp. 5553–5563, Sep.\\n2016.\\n[5] Y . Bengio, “Learning deep architectures for ai,” F ound. Trends Mach.\\nLearn. , vol. 2, no. 1, pp. 1–127, 2009.\\n[6] U. C. Benz, P . Hofmann, G. Willhauck, I. Lingenfelder, and M. Heynen,\\n“Multi-resolution, object-oriented fuzzy analysis of remote sensing data\\nfor gis-ready information,” ISPRS J. Photogrammetry Remote Sens. ,\\nvol. 58, no. 3, pp. 239–258, 2004.\\nAuthorized licensed use limited to: b-on: ISCTE. Downloaded on December 03,2023 at 19:35:37 UTC from IEEE Xplore.  Restrictions apply. 3396 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV A TIONS AND REMOTE SENSING, VOL. 10, NO. 7, JULY 2017\\n[7] F. Zhang, B. Du, and L. Zhang, “Saliency-guided unsupervised feature\\nlearning for scene classiﬁcation,” IEEE Trans. Geosci. Remote Sens. ,\\nvol. 53, no. 4, pp. 2175–2184, Apr. 2015.\\n[8] W. Zhao and S. Du, “Scene classiﬁcation using multi-scale deeply\\ndescribed visual words,” Int. J. Remote Sens. , vol. 37, no. 17,\\npp. 4119–4131, 2016. [Online]. Available: http://dx.doi.org/10.1080/\\n01431161.2016.1207266\\n[9] A. Puissant , J. Hirsch, and C. Weber, “The utility of texture analysis\\nto improve perpixel classiﬁcation for high to very high spatial resolution\\nimagery,” Int. J. Remote Sens. , vol. 26, no. 4, pp. 733–745, 2005. [Online].\\nAvailable: http://dx.doi.org/10.1080/01431160512331316838\\n[10] W. Zhao, L. Luo, Z. Guo, J. Y ue, X. Y u, H. Liu, and J. Wei, “Road\\nextraction in remote sensing images based on spectral and edge analysis,”\\nSpectrosc. Spectral Anal. , vol. 35, no. 10, pp. 2814–2819, 2015.\\n[11] J. A. Benediktsson, J. A. Palmason, and J. R. Sveinsson, “Classiﬁcation\\nof hyperspectral data from urban areas based on extended morphological\\nproﬁles,” IEEE Trans. Geosci. Remote Sens. , vol. 43, no. 3, pp. 480–491,\\nMar. 2005.\\n[12] T. C. Bau, S. Sarkar, and G. Healey, “Hyperspectral region classiﬁcation\\nusing a three-dimensional gabor ﬁlterbank,” IEEE Trans. Geosci. Remote\\nSens. , vol. 48, no. 9, pp. 3457–3464, Sep. 2010.\\n[13] X. Huang, L. Zhang, and P . Li, “A multiscale feature fusion approach for\\nclassiﬁcation of very high resolution satellite imagery based on wavelet\\ntransform,” Int. J. Remote Sens. , vol. 29, no. 20, pp. 5923–5941, 2008.\\n[Online]. Available: http://dx.doi.org/10.1080/01431160802139922\\n[14] A. M. Cheriyadat, “Unsupervised feature learning for aerial scene classi-\\nﬁcation,” IEEE Trans. Geosci. Remote Sens. , vol. 52, no. 1, pp. 439–451,\\nJan. 2014.\\n[15] D. Tuia, M. V olpi, M. D. Mura, A. Rakotomamonjy, and R. Flamary,\\n“Automatic feature learning for spatio-spectral image classiﬁcation with\\nsparse svm,” IEEE Trans. Geosci.Remote Sens. , vol. 52, no. 10, pp. 6062–\\n6074, Oct. 2014.\\n[16] J. Li et al. , “Multiple feature learning for hyperspectral image classiﬁca-\\ntion,” IEEE Trans. Geosci. Remote Sens. , vol. 53, no. 3, pp. 1592–1606,\\nMar. 2015.\\n[17] D. Tuia, R. Flamary, and N. Courty, “Multiclass feature learning for\\nhyperspectral image classiﬁcation: Sparse and hierarchical solutions,”\\nJ. Photogrammetry Remote Sens. , vol. 105, pp. 272–285, 2015.\\n[18] G. E. Hinton, S. Osindero, and Y .-W. Teh, “A fast learning algorithm\\nfor deep belief nets,” Neural Comput. , vol. 18, no. 7, pp. 1527–1554,\\n2006.\\n[19] Y . LeCun, Y . Bengio, and G. Hinton, “Deep learning,” Nature , vol. 521,\\nno. 7553, pp. 436–444, 2015.\\n[20] Q. V . Le, “Building high-level features using large scale unsupervised\\nlearning,” in Proc. IEEE Int. Conf. Acoust. Speech Signal Process. , 2013,\\npp. 8595–8598.\\n[21] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\\nwith deep convolutional neural networks,” in Proc. Adv. Neural Inf. Pro-\\ncess. Syst. Conf. , 2012, pp. 1097–1105.\\n[22] Y . Sun, Y . Chen, X. Wang, and X. Tang, “Deep learning face representation\\nby joint identiﬁcation-veriﬁcation,” in Proc. Adv. Neural Inf. Process. Syst.\\nConf. , 2014, pp. 1988–1996.\\n[23] L.-J. Li, H. Su, L. Fei-Fei, and E. P . Xing, “Object bank: A high-level image\\nrepresentation for scene classiﬁcation & semantic feature sparsiﬁcation,”\\ninProc. Adv. Neural Inf. Process. Syst. Conf. , 2010, pp. 1378–1386.\\n[24] Y . Chen, Z. Lin, X. Zhao, G. Wang, and Y . Gu, “Deep learning-based\\nclassiﬁcation of hyperspectral data,” IEEE J. Sel. Top. Appl. Earth Observ.\\nRemote Sens. , vol. 7, no. 6, pp. 2094–2107, Jun. 2014.\\n[25] W. Zhao, Z. Guo, J. Y ue, X. Zhang, and L. Luo, “On combining multiscale\\ndeep learning features for the classiﬁcation of hyperspectral remote sens-\\ning imagery,” Int. J. Remote Sens. , vol. 36, no. 13, pp. 3368–3379, 2015.\\n[Online]. Available: http://dx.doi.org/10.1080/2150704X.2015.1062157\\n[26] X. Chen, S. Xiang, C.-L. Liu, and C.-H. Pan, “V ehicle detection in satellite\\nimages by hybrid deep convolutional neural networks,” IEEE Geosci.\\nRemote Sens. Lett. , vol. 11, no. 10, pp. 1797–1801, Oct. 2014.\\n[27] J. Y ue, W. Zhao, S. Mao, and H. Liu, “Spectral–spatial classiﬁcation of\\nhyperspectral images using deep convolutional neural networks,” Remote\\nSens. Lett. , vol. 6, no. 6, pp. 468–477, 2015.\\n[28] W. Zhao and S. Du, “Learning multiscale and deep representations for\\nclassifying remotely sensed imagery,” ISPRS J. Photogrammetry Remote\\nSens. , vol. 113, pp. 155–165, 2016.\\n[29] T. Blaschke, S. Lang, E. Lorup, J. Strobl, and P . Zeil, “Object-oriented\\nimage processing in an integrated gis/remote sensing environment and per-\\nspectives for environmental applications,” Environ. Inf. Planning Politics\\nPublic , vol. 2, pp. 555–570, 2000.[30] G. Duveiller, P . Defourny, B. Descl ´ee, and P . Mayaux, “Deforestation in\\ncentral africa: Estimates at regional, national and landscape levels by ad-\\nvanced processing of systematically-distributed landsat extracts,” Remote\\nSens. Environ. , vol. 112, no. 5, pp. 1969–1981, 2008.\\n[31] Y . Jia et al. , “Caffe: Convolutional architecture for fast feature embedding,”\\ninProc. ACM Int. Conf. Multimedia , 2014, pp. 675–678.\\n[32] F. Zhang, B. Du, and L. Zhang, “Scene classiﬁcation via a gradient boost-\\ning random convolutional network framework,” IEEE Trans. Geosci. Re-\\nmote Sens. , vol. 54, no. 3, pp. 1793–1802, Mar. 2016.\\n[33] M. A. Ranzato, F. J. Huang, Y .-L. Boureau, and Y . LeCun, “Unsupervised\\nlearning of invariant feature hierarchies with applications to object recog-\\nnition,” in Proc. IEEE Conf. Comput. Vis. Pattern Recog. , 2007, pp. 1–8.\\n[34] C. Farabet, C. Couprie, L. Najman, and Y . LeCun, “Learning hierarchical\\nfeatures for scene labeling,” IEEE Trans. Pattern Anal. Mach. Intell. ,\\nvol. 35, no. 8, pp. 1915–1929, Aug. 2013.\\n[35] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hierarchies\\nfor accurate object detection and semantic segmentation,” in Proc. IEEE\\nConf. Comput. Vis. Pattern Recognit. , 2014, pp. 580–587.\\n[36] M. Cramer, “The dgpf-test on digital airborne camera evaluation–overview\\nand test design,” Photogrammetrie Fernerkundung Geoinf. , vol. 2010,\\nno. 2, pp. 73–82, 2010.\\n[37] C. Small, “High spatial resolution spectral mixture analysis of urban re-\\nﬂectance,” Remote Sens. Environ. , vol. 88, nos. 1/2, pp. 170–186, 2003.\\n[38] A. Darwish, K. Leukert, and W. Reinhardt, “Image segmentation for the\\npurpose of object-based classiﬁcation,” in Proc. IEEE Int. Geosci. Remote\\nSens. Symp. , 2003, vol. 3, pp. 2039–2041.\\n[39] S. W. Myint, P . Gober, A. Brazel, S. Grossman-Clarke, and Q. Weng,\\n“Per-pixel vs. object-based classiﬁcation of urban land cover extraction\\nusing high spatial resolution imagery,” Remote Sens. Environ. , vol. 115,\\nno. 5, pp. 1145–1161, 2011.\\nWenzhi Zhao was born in Shandong, China, in 1990.\\nHe is currently working toward the Ph.D. degree in\\nthe Institution of Remote Sensing and Geographic In-\\nformation System, Peking University, Beijing, China.\\nHis research interests include hyperspectral data\\nanalysis, high-resolution image processing, deep\\nlearning techniques, and computational intelligence\\nin remote sensing images.\\nShihong Du received the B.S. and M.S. degrees in\\ncartography and geographic information system from\\nthe Wuhan University, Hubei, China, and the Ph.D.\\ndegree in cartography and geographic information\\nsystem from the Institute of Remote Sensing Ap-\\nplications, Chinese Academy of Sciences, Beijing,\\nChina, in 1998, 2001, and 2004, respectively.\\nHe is currently an Associate Professor with the\\nPeking University, Beijing. His research interests in-\\nclude qualitative knowledge representation, reason-\\ning and its applications, and semantic understanding\\nof spatial data including GIS and remote sensing data.\\nWilliam J. Emery (F’02) received the Ph.D. de-\\ngree in physical oceanography from the University\\nof Hawaii, Honolulu, HI, USA, in 1975.\\nAfter working at Texas A&M University, he\\nmoved to the University of British Columbia, in\\n1978, where he created a Satellite Oceanography\\nfacility/education/research program. He was an Ap-\\npointed Professor in Aerospace Engineering Sciences\\nat the University of Colorado in 1987. He is an Ad-\\njunct Professor of informatics at Tor V ergata Univer-\\nsity, Rome, Italy. He has authored more than 182-\\nrefereed publications and 2 textbooks in addition to having given 91 conference\\npapers. Dr. Emery is the VP for publications of the IEEE Geoscience and Re-\\nmote Sensing Society (GRSS). He received the 2004 GRSS Educational Award\\nand the 2009 GRSS Outstanding Service Award. He is a Fellow of the Ameri-\\ncan Meteorological Society (2010), the American Astronautical Society (2011),\\nand the American Geophysical Union (2012). He is the new Chair of the IEEE\\nPeriodicals Committee of the IEEE Technical Advisory Board.\\nAuthorized licensed use limited to: b-on: ISCTE. Downloaded on December 03,2023 at 19:35:37 UTC from IEEE Xplore.  Restrictions apply. ', '', \"Majlesi Journal of Telecommunication Devices                                                                 Vol. 12, No. 1, March 2023  \\n \\n43 \\nPaper type: Research paper  \\nDOI: 10.30486/mjtd.2023.1979006.1024  \\nHow to cite this paper: Z . Dorrani, “ Road Detection with Deep Learning in Satellite Images ”, Majlesi  Journal of \\nTelecommunication Devices , Vol. 12, No. 1, pp. 43-47, 2023.  \\n  \\nRoad Detection  with Deep Learning  in Satellite Images  \\n \\nZohreh  Dorrani  \\nDepartment of Electrical Engineering, Payame Noor University, Tehran, Iran.  \\nEmail: dorrani.z @pnu.ac.ir   (Corresponding author)  \\n \\nReceived : 29 December 2022   Revised : 27 January 2023   Accepted : 18 February 2023  \\n \\n \\nABSTRACT:  \\nRoad detection from high -resolution satellite images using deep learning is proposed in this article. The VGG19 \\narchitecture, which is one of the deep convolutional neural network architectur es, is used in the proposed method. To \\ndetect the road, two steps are implemented. To achieve high accuracy, image segmentation is done in the first step. At \\nthis stage, based on the semantic division, the objects whose area is small are removed. In the se cond stage, edge \\ndetection of images combines two techniques of segmentation and edge detection to improve road detection. Considering \\nthe good accuracy of the VGG19 architecture and the need for few parameters, the obtained results are favorable. To \\ncheck  the performance of the proposed method, the IoU criterion was used. The values obtained for this criterion show \\nan improvement of more than 80%. While this criterion is less than 80% for the compared methods. The obtained results \\ncan be used for the purpo ses of digital mapping, transportation management and many other applications.  \\n \\nKEYWORDS:  Convolutional Neural Networks, Deep Learning, Edge Detection, Road Segmentation, Satellite Images , \\nVGG19.  \\n  \\n1.  INTRODUCTION  \\nNowadays, due to the expansion of satellites, the \\nanalysis and analysis of satellite images  [1, 2]  is one of \\nthe most widely used fields in image processing  [3,4]. \\nDue to the accuracy and details of the information \\nrequired in this field, ther e are many limitations in the \\ndata collection stage, which include the cost of the \\nprocess and the need to spend a long time  [5]. \\nRoad detection  [6] in aerial images is one of the most \\ncritical applications of satellite image analysis. It plays \\nan essentia l role in transportation applications, because \\nit creates, maintains, and updates the road network \\ndatabase. This information is used in order to perform \\nactivities such as traffic management, automatic \\nnavigation of vehicles, maintaining security and \\nassessing risks and natural disasters, exchanging and \\nsharing location data in the form of location data \\ninfrastructure, and preventing pa rallel work and \\nrepetition p laced  [7]. The main challenge in road \\nextraction is the complex structure of images, which \\nincludes different objects such as roads, houses, trees, \\netc., with differences in shape and texture. Meanwhile, \\nthe ever -increasing increase in deep learning  [8] \\ncapabilities, which has brought many applications to \\nvarious sciences and techniques, has caused  engineers to \\npay attention to it and use it in various fields to carry out \\ntheir projects. Since the introduction of convolutional \\nneural networks, various architectures have been presented that has led to deeper and thus better network \\nand increased accu racy [9]. In this field, VGG \\narchitecture has a good structure, and simple and \\nappropriate precision, which has been used in many \\napplications and has had favorable results  [10]. \\nTherefore, in this article, this architecture is used to \\nextract roads. In the next section, some research in this \\nfield was examined. In the following, the proposed \\nmethod is analyzed. At the end comes the results and \\nconclusions section.  \\n \\n2.  RELATED WORK S \\n     Satellite images contain valuable data that need to be \\nprocessed and extract ed. For this purpose, various \\ntechniques have been presented that can extract useful \\ninformation. The extraction of roads and their \\nsegmentation is important information that can be \\nextracted and used for digital mapping. But the presence \\nof factors such a s complexity, noise, lack of proper \\nclarity, presence of shadows, obstructions, and \\nenvironmental factors requires the use of deep learning \\nmethods to process these types of images  [11]. A method \\nbased on deep learning for road detection based on \\nsemantic segmentation and edge detection is presented, \\nin which the two techniques of segmentation and edge \\ndetection are combined to improve road detection. In \\nthis model, a two -part hybrid encoder includes full -\\nresolution feature extraction and high -resolution fe ature Majlesi  Journal of Telecommunication Devices                                                                 Vol. 12, No. 1, March 20 23 \\n \\n44 \\n encoding. The second part is used to increase the overall \\nreceiving field, which provides backgr ound information \\nto the network  [5]. \\nA deep learning approach using the DenseUNet model \\nis also proposed. This method uses dense connection \\nunits and jump  connections to solve the problem of \\ndecreasing accuracy as the network deepens. On the \\nother hand, the integration of different scales is done in \\nthis model, which leads to the strengthening of the \\nnetwork by connections in different layers [ 12]. \\n  SegNet  architecture [ 13] was presented for semantic \\npixel segmentation. This architecture consists of an \\nencoder network that provides low -resolution encoder \\nfeature maps to a decoder network with a pixel \\nclassification layer corresponding to the feature maps to  \\ngenerate full input resolution for pixel classification. \\nTherefore, in this model, the input feature map is \\nsampled with a lower resolution. Also, the calculated \\nintegration indices are used to perform non -linear \\nsampling to eliminate the need for samplin g learning.  \\nFor segmentation and separation, fully complex \\nnetworks (FCNs) [1 4] can also be used. These networks also use an encoder and a decoder. The first part extracts \\nfeature at different levels to segment objects of different \\nsizes, and then a decode r combines the coded features.  \\n \\n3.  PROPOSED  METHOD  \\nThe VGG network has a good and simple structure \\nwith a small number of layers and is used in many \\ncomputer vision fields. This network was introduced \\nwith two different architectures VGG16, and VGG19  \\n[15 ,16]. First, the VGG16 network was proposed, and \\nlater, with minor changes in the VGG16 network, the \\nVGG19 network was proposed. VGG16 network \\nincludes 16 convolutional layers or 16 parametric layers \\nand VGG19 consists of 19 layers. The VGG19 network \\nis an inn ovative object recognition model and is actually \\na deep CNN that performs very well in many tasks and \\ndatasets other than ImageNet. This network is one of the \\nmost widely used image recognition architectures and its \\narchitecture is shown in Figure 1.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig. 1. VGG19.  \\n \\n \\nThe unique aspect of VGG19 is that instead of \\nhaving a large number of meta -parameters, it is a 3x3 \\nfilter with a stride of 1 and the max pooling layer is a \\n2x2 filter with a stride of 2. This architecture includes \\ntwo convolutional layers with 64 3x3 filt ers that are \\nplaced one behind the other. Then, a 2x2 max pooling \\nlayer with a stride of 2 is set. In addition to sampling, \\nthis max -pooling layer also has the task of reducing the \\ndimension of features by half. Next, two more \\nconvolutional layers with 128  filters of 3x3 and a 2x2 \\nmax pooling layer, and 2 jumps are placed. Similarly, \\nthree convolutional layers with 256 filters of 3x3and, \\none 2x2 max pooling layer with 2 hops are included. 3 \\nconvolutional layers with 512 filters of 3x3 and a max pooling laye r are the continuation of this network, which \\nof course is repeated twice. Finally, the features are \\nconverted into a feature vector for fully connected neural \\nlayers. Two layers of neurons with dimensions of 4096 \\nare placed one behind the other. Finally, a neural layer \\nwith dimensions of 1000, which corresponds to the \\nnumber of application classes, is considered. The \\nactivation function called RELU has been used in all \\nconvolution and neuron layers.  \\nEdge detection using VGG19  [17] is used for road \\ndetection. Edge detection from raw satellite images is \\nvery difficult and does not lead to desirable results. \\nHence, segmentation is done first and its output is given \\nto edge detection.  Convolution+ReLU                           max pooling                fully connected+ReLU  MAX  \\nPOOling  CONV1  \\nCONV 2 CONV 3 CONV 4 CONV 5 FC6           FC7           FC8  \\nMajlesi Journal of Telecommunication Devices                                                                 Vol. 12, No. 1, March 2023  \\n \\n45 \\n Table 1.  Size of layer in VGG19 . \\nLayer  size \\nCONV1  224×224×64 \\nCONV2  112×112×128 \\nCONV3  56×56×256 \\nCONV4  28×28×512 \\nCONV5  14×14×512 \\nMAX  POOling  7×7×512 \\nFC6 1×1×4096  \\nFC7 1×1×4096  \\nFC8 1×1×1000  4.  RESULT  \\nReference images [ 6] have been used to check the \\nproposed method. In this reference, there are 3 satellite \\nimages shown in Figure 2. In order to extract roads from \\nthese input images, image segmentation has been done \\nfirst, and using its output, edge detection has been done. \\nFor a better comparison, the ground truth is shown in \\nFigure 3b. And in part c of this figure, the results of the \\nmethod are presented.  \\n \\n \\n \\n  \\n  \\n \\n(A) \\n   \\n(B) \\n   \\n(C) \\nFig. 2. A: Input Image, B: Ground T ruth C: Edge Detection with proposed method.  \\nMajlesi  Journal of Telecommunication Devices                                                                 Vol. 12, No. 1, March 20 23 \\n \\n46 \\n Comparing the results obtained with ground truth \\nshows that road extraction is done with proper accuracy \\nusing deep learning.  \\nThe use of evaluation criteria is a suitable method to \\ncompare the performance of the proposed method with \\nsome existing methods, therefore, the IoU criterion is \\nused, which is obtained by the following relationship  \\n[5]: \\n \\n𝐼𝑜𝑈=1\\n𝑐∑𝑇𝑃(𝑘)\\n𝑇𝑃(𝑘)+𝐹𝑃(𝑘)+𝐹𝑁(𝑘)𝑐\\n𝑘 (1) \\n \\nC: the number of categories,  \\nTP(k): True Positive,  \\n FP(k):  False Positive,  \\nFN(k): False Negative.  \\n \\n \\nFig. 3. IoU criterion  for proposed method compared \\nto other methods  . \\n \\nThe criterion values show that this criterion is above \\n80% for the proposed method, which is lower than 80% \\nfor the compared methods. In the proposed method, an \\narchitecture is used that does not require many \\nparameters and therefore can be unique in this aspect.  \\n \\n5.  CON CLUSION  \\nSatellite imaging and satellite image proce ssing are \\nused in the mapping. These maps are used in various \\nfields such as agriculture, urban, regional, and forestry \\nplanning, and transportation. Today's digital aerial \\ncameras have good storage capabilities. But to do this, \\nthey also need a suitable s ystem to store a complete set \\nof images, and the ability to work with very large data \\nand analyze them. In some cases, the ability to prepare a \\nlarge amount of image data and transfer it to the desired \\nground reference is required. Computer vision \\ntechniqu es used to process these images can also help to \\nprepare these images. Therefore, in this article, a method \\nbased on deep learning was proposed to analyze these images. The VGG19 method was used to extract the \\nroads, which is implemented in two steps. The first stage \\nis segmentation and the next stage is edge detection so \\nthat roads can be extracted. The proposed method has \\nled to the improvem ent of the Io U criterion and has \\nincreased its value to over 80%.  \\n \\nREFERENCES  \\n[1] M. Alkhelaiwi, W. Boulila, J.  Ahmad, A. Koubaa, \\nM. Driss, “An Efficient Approach Based on \\nPrivacy -Preserving Deep Learning for Satellite \\nImage Classification,”  Remote Sens. Vol.13, pp. \\n2221 -2232, 2021.  \\n[2] E McAllister, A Payo , A Novellino , T Dolphin , and \\nE. Medina -Lopez. “Multispectral satellite imagery \\nand machine learning for the extra ction of \\nshoreline indicators,”  Coastal Eng ineering , vol. 10, \\npp.104102 -104113, 2022 . \\n[3] M. Zarei, and M. Esmaeilpour,  “Enhancing the \\nQuality of Satellite Images Enhancing through \\nCombination of Feature and Pixel Level Image \\nFusion,” Majlesi Journal of Telecommunication \\nDevices vol. 8, no. 4, pp. 141-147, 2019 . \\n[4] A.A. Shahraki, and M. Emadi, “Improving Image \\nQuality based on Feature Extraction and \\nGaussian Model,” Majlesi Journal of \\nTelecommunication Devices , vol.  8, no. 2 , pp. 35 -\\n41, 2019 . \\n[5] H. Ghandorh, W . Boulila, S . Masood, and A. \\nKoubaa. “Semantic segmentation and edge \\ndetection —Approach to road detection in very \\nhigh resolution satellite images,”  Remote Sensing , \\nvol. 14, no. 3, 1 -22, 2022.  \\n[6] J. Wan, Z. Xie, Y. Xu, S. Chen, and Q.  Qiu, \\n“RoadNet: A Dual -Attention Network for Road \\nExtraction from High Resolution  Satellite \\nImagery ,” IEEE J. Sel. Top. Appl. Earth Obs. \\nRemote Sens. Vol. 14, pp. 6302 –6315 , 2021.  \\n[7] A. Abdollahi, B. Pradhan, N. Shukla, S. \\nChakraborty, A. Alamri, “Deep Learning \\nApproaches Applied to Remote Sensing Datasets \\nfor Road Extraction: A State -Of-The-Art \\nReview,” Remote Sens. Vol. 12, pp. 1444 -1452, \\n2020.  \\n[8] M. Ghasemi, and M, Bayati , “Improving the \\nAccuracy of Detecting Cancerous Tumors Based \\non Deep Learning on MRI Images,”  Majlesi \\nJournal of Telecommunication Devices , vol. 11, no. \\n2 ,2022 . \\n[9] Z. Dorrani, H. Farsi, and S. Moh amadzadeh, “Deep \\nLearning in Vehicle Detection Using ResUNet -a \\nArchitecture,”  Jordan Journal of Electrical \\nEngineering , Volume 8, no. 2, pp.  166-178, 2022.  \\n[10] Z. Dorrani, H. Farsi, and S. Moh amadzadeh , “Edge \\nDetection and Identification using Deep  Learning \\nto Identify Vehicles,”  Journal of Information \\nSystems and Telec ommunication, vol. 3, no. 39, pp. \\n201-213, 2022.  \\n[11] S. Chen, Z. Zhang, R.  Zhong, L. Zhang, H. Ma, L.  \\nLiu, “A dense feature pyramid network -based \\ndeep learning model for road marking instance \\nsegmentation using MLS point clouds,”  IEEE \\nMajlesi Journal of Telecommunication Devices                                                                 Vol. 12, No. 1, March 2023  \\n \\n47 \\n Trans. Geosci. Remote Sens , vol. 59, pp. 784–800, \\n2020 . \\n[12] K. Heidler, L. Mou, C. Baumhoer, A. Dietz, X.X. \\nZhu, “HED -UNet: Combined S egmentation and \\nEdge Detection for Monitoring the Antarctic \\nCoastline,”  IEEE Trans. Geosci. Remot e Sens, vol.  \\n60, pp. 1 –14, 2021.  \\n[13] V. Badrinarayanan , A. Kendall , and R. Cipolla, \\n“SegNet: A Deep Convolutional Encoder -\\nDecoder Architecture for Image Segmentation,”  \\nIEEE Trans. Pattern Anal. Mach. Intell, vol. 39, pp. \\n2481 –2495, 2017.  \\n[14] K. Chaiyasarn , A. Buatik , H. Mohamad , M. Zhou , S. \\nKongsilp, and N. Poovarodom,  “Integrated pixel -\\nlevel CNN -FCN crack detection via \\nphotogrammetric 3D texture mapping of concrete structures,”  Automation in Construction, \\nvol. 140, pp. 104388 -104398, 2022 . \\n[15] A. Karacı, “VGGCOV19 -NET: automatic \\ndetection of COVID -19 cases from X -ray images \\nusing modified VGG19 CNN architecture  and \\nYOLO algorithm,”  Neural Computing and \\nApplications, vol. 34, no. 10, pp. 8253 -8274 , 2022 . \\n[16] M. Ghasemzade , “Extracting Image Features \\nThrough Deep Learning,” Majlesi Journal of \\nTelecommunication Devices , vol.  9, no. 3 , pp. 109-\\n114, 2020.  \\n[17] MJ. Ahmed, and P. Nayak, “Detection of \\nLymphoblastic Leukemia Using VGG19 Model,”  \\nIn 2021 Fifth International Conference on I -SMAC \\n(IoT in Social, Mobile, Analytics and Cloud)  (I-\\nSMAC), pp. 716 -723, 2021.\\n \\n \\n \", 'ROBUST ROAD EXTRACTION FOR HIGH RESOLUTION SATELLITE IMAGES\\nEmmanuel Christophe and Jordi Inglada\\nCNES, BPI 1219, 18 avenue Edouard Belin, 31401 Toulouse Cedex 9, France\\nemmanuel.christophe@cnes.fr, jordi.inglada@cnes.fr\\nABSTRACT\\nAutomatic road extraction is a cr itical feature for an efﬁcient\\nuse of remote sensing imagery in most contexts. This paperproposes a robust geometric method to provide a ﬁrst step ex-\\ntraction level. These results can be used as an initialization\\nfor other algorithms or as a starting point for manual road ex-traction. Results of the extraction are vectorized for GIS in-\\ntegration and for a better interaction with human experts that\\ncan reﬁne the results. The algorithm is fast, has very few pa-\\nrameters and is only slightly affected by the image properties\\n(resolution, noise). The algorithm is available in the open-source Orfeo Toolbox.\\nKeywords: Remote sensing, Road extraction\\n1. INTRODUCTION\\nRoad extraction is a critical feature for an efﬁcient use of high\\nresolution satellite images. There are many applications ofroad extraction: update of GIS database, reference for im-\\nage registration, help for identiﬁcation algorithms and rapid\\nmapping for example. Road network can be used to regis-\\nter an optical image with a map or an optical image with a\\nradar image for example. Road network extraction can helpfor other algorithms: isolated building detection, bridge de-\\ntection. In these cases, a rough extraction can be sufﬁcient.\\nIn the context of response to crisis, a fast mapping is neces-sary: within 6 hours, infrastructures for the designated area\\nare required [1]. Within this timeframe, a manual extraction\\nis inconceivable and an automatic help is necessary.\\nA high resolution satellite image typically has a resolution\\nof 0.5 to 1 m and four spectral bands. Using the result of a\\nfusion, a product combining four spectral bands at the higher\\nresolution can be obtained. This is usually the case for theQuickbird satellite from DigitalGlobe and will be the standard\\nproduct for the future Pleiades constellation from CNES.\\nOne of the most important key features of satellites is\\ntheir ability to cover an important area, thus gathering a huge\\namount of data. One Pleiades scene will cover an area of20×20 km. A manual exploitation of these data, even just\\nfor road extraction is not possible. An operator would need\\nto visualize more than 1000 screens to process these data at\\nfull resolution. An efﬁcient automatic algorithm leading to agood (even if not perfect) solution would facilitate the work\\nof remote sensing experts.\\nSince 1990, different methods have been proposed to pro-\\nvide automatic or semi-automatic road extraction in remote\\nsensing images. In [2], an exhaustive bibliography on auto-\\nmated road extraction is provided. These methods includesnakes [3], higher order active contours [4], dynamic pro-\\ngramming [5] or probabilistic approaches [6]. However, to\\nour best knowledge, none of these algorithms satisﬁes the op-erators, mainly because of com putation time. In most situa-\\ntions, road extraction is still manual.\\nThe main problem of these methods is the difﬁculty to\\nprovide the best parameters for a given image. Given a newimage, the user often has to try different combinations be-\\nfore obtaining a satisfactory result. The computation time is\\nalso often a problem. As the resolution of optical sensors in-creases, size of images increases as well, thus increasing the\\ncomputation time necessary to pr ocess a scene. Information\\nprovided by the color is also often misused, most algorithms\\nfocusing on the panchromatic data.\\nThe aim of this paper is to provide a simple, fast, robust\\nand efﬁcient algorithm to extr act roads. The algorithm pro-\\nvides a vectorized result. The only entry parameter is thecolor of the road or the spectrum if more spectral bands are\\navailable. Results are not expected to be perfect but rather to\\nbe a good initialization for more complex algorithms or for\\nhuman reﬁnement.\\n2. ROAD DETECTION\\nThe color of the road provides a rich information which of-\\nten enables to distinguish between roads and vegetation. Thiscolor information, usually in four or more spectral bands can\\nbe interpreted as a vector. However, most algorithms prefer\\nto work with scalar data. One efﬁcient way to convert this\\nspectral information to a scalar data is to use the spectral an-\\ngle with respect to a reference pixel. The spectral angle isdeﬁned as:\\nSA =cos\\n−1/parenleftBiggnb\\n∑\\nb=1r(b).p(b)/slashBig/radicalBigg\\nnb\\n∑\\nb=1r(b)2nb\\n∑\\nb=1p(b)2/parenrightBigg\\n,(1)V - 437 1-4244-1437-7/07/$20.00 ©2007 IEEE ICIP 2007\\nAuthorized licensed use limited to: b-on: ISCTE. Downloaded on November 21,2023 at 13:39:21 UTC from IEEE Xplore.  Restrictions apply. (a)\\n (b)\\n (c)\\n (d)\\n (e)\\nFig. 1 .Original image (a), Distance according to spectral angle (b), Gra dient directions (c), line detector (d) and after vectorization (e).\\nbbeing the spectral band, ris the reference pixel and p\\nthe current pixel.\\nThere are two main advantages to using this representa-\\ntion. The ﬁrst one being that the following algorithm is work-\\ning on the spectral angle image and does not depend on thenumber of bands. Thus the road extraction process can be\\napplied to multispectral images with any number of bands.\\nThe second advantage is that choosing the reference pixel, wewill be able to extract either tarred roads or dirt tracks. The\\nspectral angle is taken between the reference vector pixel and\\nthe current vector pixel. The resulting image contains all the\\nroads in the darker color. Figure 1 (b) shows the distance be-\\ntween each pixel and the reference pixel.\\nOn this spectral angle image we apply a line detection\\nmethod based on a constrained gradient. This method was\\ndescribed in [7]. A gradient ﬁlter is ﬁrst applied to the image.Each pixel contains gradient direction and gradient intensity\\nvalues. An example of gradient directions is illustrated on ﬁg-\\nure 1 (c). Then, knowing that the road is darker than pixels\\naround, we know that the gradient direction will be opposite\\non each side of the road, as illustrated on ﬁgure 2 (a). To usethis property, we compute the scalar product between oppo-\\nsite gradient vectors around a pixel. The higher (in absolute\\nvalue) scalar product which is also negative gives us the di-rection of the road. On ﬁgure 2 (b), the highest scalar value is\\nobtained between pixels a\\n1and b1, leading to the dotted red\\ndirection vector with the computed length. The spectral angle\\nprovides an important property that the original algorithm did\\nnot have: as we know that the road is darker, we can keep onlythe pixel where the neighbor gradient vectors point toward the\\npixel.\\nTo improve the line following, pixels which do not have\\nthe maximum scalar value across d irection are removed (Fig. 3).\\n3. VECTORIZATION\\nVectorization of the extracted ro ads is decisive for an efﬁcient\\nuse in GIS systems. From the previous step, we had obtained\\nan image with pieces of roads, often irregular as illustrated on\\nﬁgure 1 (d).\\nAfter the previous part, paths can be extracted from the(a)a0 a1\\na2\\na3 b0\\nb1b2b3\\n(b)\\nFig. 2 .Gradient representation and the roads: (a) gradient direction\\ntowards the roads; (b) computation of the direction and scalar values\\nfor each pixel.\\nFig. 3 .Removal of non maximum scalar values across direction:\\none of the neighbor pixel across the road direction has a greater value\\nthan the current pixel which is removed for the path construction.\\nimage. To get a better localization, path vertices can be lo-\\ncated at non integer positions (Fig. 4). Their position is ob-\\ntained with the weighted barycenter of few pixels on the given\\ndirection. However, paths obtained with this process containtoo many vertices: about as many as the length of the path.\\nThey are also usually too short due to the noise on the gra-\\ndient image. Few steps are necessary to reﬁne these pathsaccording to general road properties.\\nMany vertices can be removed as they are usually aligned.\\nFigure 5 illustrates this step. dis the distance between one\\nvertex and the proposed new path. If the distance between\\nall old vertices and the new proposed path is lower than one\\npixel, the new path is accepted.\\nOne result of the vectorization process is that paths can\\npresent a sharp angle at their extremity due to the noise on the\\ngradient image. This sharp angle will be damageable for theV - 438\\nAuthorized licensed use limited to: b-on: ISCTE. Downloaded on November 21,2023 at 13:39:21 UTC from IEEE Xplore.  Restrictions apply. Fig. 4 .Vector coordinates are non integer, allowing a better accu-\\nracy.\\nFig. 5 .Simplifying paths.\\nnext step. In general roads contain smooth curves and do not\\nhave sharp turns. To comply with this property we split paths\\nwhich contain a sharp angle between two segments. An angle\\nis considered as sharp if it is above π/8. The exact value does\\nnot inﬂuence much results of the algorithm.\\nTo allow the detected paths to go over tree shadows or\\nvehicles on the road which imp act the gradient, a relation be-\\ntween paths must be considered. The relation is described on\\nﬁgure 6. Within a given search distance d, which depends on\\nthe resolution, a link can be found between two paths. To be\\naccepted, these paths have to comply with some conditions:\\nangles between the ﬁrst path and the link, α1, the second path\\nand the link, α2, and the last segment of both paths, α1−α2\\nhave to be within a certain range. For a 0.7 m resolution im-\\nage, the search distance considered is 40 pixels and the angle\\nwas chosen to be within the [−π/8;π/8]interval.\\nFig. 6 .Linking paths.\\nAfter this step, some very short paths are still present on\\nthe image. They usually correspond to noise and do not pro-\\nvide much information so they are removed.\\nThe last step associates a conﬁdence value to each path\\naccording to the spectral angle v alue along the path. One path\\ncan have a low spectral angle value all along, thus the algo-\\nrithm will be conﬁdent that this path is a road. Some otherpaths can have higher spectral angle value leading the algo-\\nrithm to be less conﬁdent. The conﬁdence value is presented\\nby the color of the path: darker color for low conﬁdence and\\nbrighter color for high conﬁdence.4. RESULTS\\nAll the results are obtained within the Orfeo Toolbox (OTB)\\nframework [8]. The OTB is an open source set of tools for\\nremote sensing data exploitation. This algorithm is included\\nin the latest release of OTB and follows the principles of re-producible research.\\nThe algorithm is applied on two extracts from a 1000 ×\\n1000 Quickbird image. We compare the results with thoseobtained only with the line detector from [7]. The algorithm\\nis fast. On a standard PIV 2.8 GHz, processing a 1000 ×1000\\nimage takes less than 3 s. This short time enables interactive\\nprocessing during visualization at full resolution on a screen\\nsize.\\nOn ﬁgure 8 (c), we can see that the new algorithm pro-\\nvides improved results. Roads are better deﬁned and without\\nstaircase effects. Segments have an information about howconﬁdent the algorithm is: the dark segments are those for\\nwhich the algorithm is not very conﬁdent (the segment on\\ntrees for example). The railroad track in the lower right cor-ner is not mistaken for a road. In two places, the algorithm\\nsuccessfully ﬁnds the road hidden by tree occultations.\\nOn ﬁgure 8 (f), focus was on extracting the main roads\\nwith the reference pixel taken from one of these roads. The\\nalgorithm manages to make the difference with the secondary\\nroads from the top right of the image. Most main roads, or at\\nleast portions of them are extracted. Few false alarms appear\\non some buildings.\\nThe same algorithm can be app lied to radar images. As we\\nhave seen before, the computation of the spectral angle leads\\nto a representation of the road in darker color. In radar images,as roads usually present specu lar reﬂections, they also appear\\nin darker color (Fig. 7 (a)). We applied the same algorithm\\ndirectly on the radar image instead on the spectral angle im-\\nage to obtain the results presented in Fig. 7 (b). These results\\nare good despite of the noise. Radar images contain a multi-plicative noise so the gradient ﬁlter is not the most adapted.\\nA ratio of means, as in [9], will probably give better results.\\n(a)\\n (b)\\nFig. 7 .Road extraction result on a radar image: the same algorithm\\nis applied without the spectral angle computation.V - 439\\nAuthorized licensed use limited to: b-on: ISCTE. Downloaded on November 21,2023 at 13:39:21 UTC from IEEE Xplore.  Restrictions apply. Fig. 8 (a) Original image\\n Fig. 8 (b) Line detector\\n Fig. 8 (c) Final algorithm\\nFig. 8 (d) Original image\\n Fig. 8 (e) Line detector\\n Fig. 8 (f) Final algorithm\\n5. CONCLUSION\\nThe presented algorithm is automatic with very little interac-\\ntion from the users. The main one is the color (or spectrum)\\nof the road in which the user is interested in. The only param-\\neter which has a signiﬁcant imp act on the results is the search\\narea to link paths. This parameter can be easily related to the\\nimage resolution. We have seen that the algorithm, originally\\ndesigned for optical images, can successfully extract roads on\\na radar image. Moreover, this algorithm is fast, allowing in-\\nteractive computation. And ﬁnally, this algorithm uses most\\nof available properties of roads in high resolution satellite im-\\nages: spectral information (with the computation of the spec-\\ntral angle) and the shape (ﬁrst with the gradient and then with\\nthe vectors).\\nOf course, results are not perfect but could be reﬁned by\\nmore complex methods. These methods, usually more costly\\nin terms of computation time, could be applied only in the\\nselected area.\\n6. ACKNOWLEDGMENTS\\nThe authors wish to thank Vinciane Lacroix from the Royal Military\\nAcademy for the fruitful discussions and for providing results with\\nthe original algorithm.7. REFERENCES\\n[1] J. B´ equignon et al., “Report of the working group on space in-\\nfrastructure for the GMES emergency response service,” Tech.\\nRep. 3.2, DDSC, DLR, CNES, ASI and JRC, Dec. 2006.\\n[2] J. Mena, “State of the art on automatic road extraction for\\nGIS update: a novel classiﬁcation,” Pattern Recognition Letters ,\\nvol. 24, pp. 3037–3058, 2003.\\n[3] I. Laptev, H. Mayer, T. Lindeberg, W. Eckstein, C. Steger, and\\nA. Baumgartner, “Automatic extraction of roads from aerial im-\\nages based on scale space and snakes,” Machine Vision and Ap-\\nplications , vol. 12, pp. 23–31, 2000.\\n[4] M. Rochery, I. Jermyn, and J. Zerubia, “Higher order active\\ncontours,” International Journal of Computer Vision , vol. 69,\\npp. 27–42, Aug. 2006.\\n[5] M. Barzohar and D. B. Cooper, “Automatic ﬁnding of main\\nroads in aerial images by using geometric-stochastic models and\\nestimation,” IEEE Transactions on Pattern Analysis and Ma-\\nchine Intelligence , vol. 18, pp. 707–721, July 1996.\\n[6] M. Bicego, S. Dalﬁni, G. Vernazza, and V . Murino, “Auto-\\nmatic road extraction from aerial images by probabilistic con-\\ntour tracking,” in IEEE International Conference on Image Pro-\\ncessing, ICIP’03 , vol. 3, pp. 585–588, 2003.\\n[7] V . Lacroix and M. Acheroy, “Feature extraction using the con-\\nstrained gradient,” ISPRS Journal of Photogrammetry & Remote\\nSensing , vol. 53, pp. 85–94, 1998.\\n[8] “The ORFEO toolbox software guide.” http://otb.cnes.fr, 2007.\\n[9] F. Tupin, H. Maˆ ıtre, J.-F. Mangin, J.-M. Nicolas, and E. Pech-\\nersky, “Detection of linear features in SAR images: applicationto road network extraction,” IEEE Transactions on Geoscience\\nand Remote Sensing , vol. 36, pp. 434–453, Mar. 1998.V - 440\\nAuthorized licensed use limited to: b-on: ISCTE. Downloaded on November 21,2023 at 13:39:21 UTC from IEEE Xplore.  Restrictions apply. ', 'Citation: Yang, K.; Cui, W.; Shi, S.;\\nLiu, Y.; Li, Y.; Ge, M. Semi-Automatic\\nMethod of Extracting Road Networks\\nfrom High-Resolution\\nRemote-Sensing Images. Appl. Sci.\\n2022 ,12, 4705. https://doi.org/\\n10.3390/app12094705\\nAcademic Editor: Feng Guo\\nReceived: 22 February 2022\\nAccepted: 5 May 2022\\nPublished: 7 May 2022\\nPublisher’s Note: MDPI stays neutral\\nwith regard to jurisdictional claims in\\npublished maps and institutional afﬁl-\\niations.\\nCopyright: © 2022 by the authors.\\nLicensee MDPI, Basel, Switzerland.\\nThis article is an open access article\\ndistributed under the terms and\\nconditions of the Creative Commons\\nAttribution (CC BY) license (https://\\ncreativecommons.org/licenses/by/\\n4.0/).\\napplied  \\nsciences \\nArticle\\nSemi-Automatic Method of Extracting Road Networks from\\nHigh-Resolution Remote-Sensing Images\\nKaili Yang1,2\\n, Weihong Cui1,2,*, Shu Shi3, Yu Liu2,4, Yuanjin Li1and Mengyu Ge1\\n1School of Remote Sensing and Information Engineering, Wuhan University, Wuhan 430079, China;\\nkailiyang@whu.edu.cn (K.Y.); liyuanjine@whu.edu.cn (Y.L.); mengyu_ge@whu.edu.cn (M.G.)\\n2Key Laboratory of Aerospace Information Application of CETC, Shijiazhuang 050002, China;\\nliuyu0213@stu.xidian.edu.cn\\n3College of Geomatics, Xi’an University of Science and Technology, Xi’an 710054, China; shiishu@163.com\\n4School of Artiﬁcial Intelligence, Xidian University, Xi’an 710071, China\\n*Correspondence: whcui@whu.edu.cn\\nAbstract: Road network extraction plays a critical role in data updating, urban development, and\\ndecision support. To improve the efﬁciency of labeling road datasets and addressing the problems\\nof traditional methods of manually extracting road networks from high-resolution images, such as\\ntheir slow speed and heavy workload, this paper proposes a semi-automatic method of road network\\nextraction from high-resolution remote-sensing images. The proposed method needs only a few\\npoints to extract a single road in the image. After the roads are extracted one by one, the road network\\nis generated according to the width of each road and the spatial relationships among the roads. For\\nthis purpose, we use regional growth, morphology, vector tracking, vector simpliﬁcation, endpoint\\nmodiﬁcation, road connections, and intersection connections to generate road networks. Experiments\\non four images with different terrains and different resolutions show that this method has high\\nextraction accuracy under different image conditions. The comparisons with the semi-automatic\\nGVF-snake method based on regional growth also showed its advantages and potentiality. The\\nproposed method is a novel form of semi-automatic road network extraction, and it signiﬁcantly\\nincreases the efﬁciency of road network extraction.\\nKeywords: high-resolution image; road extraction; semi-automatic; morphology; vector processing\\n1. Introduction\\nWith the acceleration of urban and rural construction, quickly identifying and extract-\\ning roads and updating road networks has become a crucial issue [ 1–4]. As a signiﬁcant\\ncomponent of urban transportation, roads play an essential role in political [ 5–7], eco-\\nnomic [ 5,8], and military ﬁelds [ 9,10], among others. At present, with the development\\nof high-spatial-resolution remote sensors, the spatial resolution of remote-sensing images\\ncan be as ﬁne as the submeter level [ 11]. Unlike the thin line shape that roads present\\nin low-resolution images, roads in high-resolution images are continuous homogeneous\\nregions, which means that roads can be extracted from these images more accurately [ 10,12].\\nHowever, due to the inﬂuence of ‘different objects with similar spectra’, different image\\nresolutions, different road types, road occlusions, etc., the difﬁculty of designing road\\nextraction algorithms is also increasing [13,14].\\nAutomatic road extraction methods represented by deep learning have been widely\\nreported in previous studies [ 1,15–17]. For example, Gao et al. extracted the roads from\\noptical satellite images using a reﬁned deep residual convolutional neural network with a\\npost-processing stage [ 2]. Yang et al. proposed using recurrent convolution neural network\\nU-Net to extract roads and predict center lines [ 3]. Zhang et al. applied a fully convolu-\\ntional network (FCN) that introduced a weighted loss function to extract roads from aerial\\nimages [ 18]. However, they all need a sufﬁcient number of representative training data and\\nAppl. Sci. 2022 ,12, 4705. https://doi.org/10.3390/app12094705 https://www.mdpi.com/journal/applsciAppl. Sci. 2022 ,12, 4705 2 of 21\\nthe prediction ability is highly related to the training samples fed into the model [3,19,20] .\\nOwing to the complexity of roads themselves, automatic road extraction models cannot\\nachieve good results through the direct application on another dataset [21–24] . Therefore,\\nthe limited remote-sensing datasets with labels are an obstacle to developing and eval-\\nuating new deep learning methods [ 19,25]. In actual road labeling, road networks are\\nstill extracted by traditional manual sketching [ 26]. Although manual operation ensures\\nthat the topological relationship is established accurately, the workload is large and the\\nefﬁciency is low [ 27]. It is necessary to propose a semi-automatic method that can improve\\nthe efﬁciency of road sample labeling [ 3]. Considering the need for deep learning methods\\nand to improve the labeling efﬁciency to build new road datasets, semi-automatic road\\nextraction methods combined with computer visual interpretation are still the focus of\\ncurrent research [9,13,22].\\nIn accordance with the process and focus of the extraction algorithm, existing semi-\\nautomatic road extraction methods are mainly based on regional growth [ 28–30]; dynamic\\nprogramming [ 31–33]; edge detection [ 34], including contour identiﬁcation by ﬁnding\\nthe gradient and potential of the image [ 35] followed by edge thinning and division [ 36];\\nimage segmentation [ 9,10,23]; template matching [ 37,38]; active contour models such as\\nSnake [39–41]; and machine learning and neural networks [ 2,21,37,42]. However, low\\nefﬁciency and poor robustness remain as problems. The approaches of dynamic program-\\nming and Snake have complicated manual settings and are subject to the time-consuming\\nprocess of iterative optimization [ 31,39]. Algorithms based on template matching and\\nneural networks are greatly inﬂuenced by the template and the sample of the label. Hence,\\nthe adaptability of these models to different roads is not reliable [ 37]. Numerous road\\nextraction methods based on regional growth, edges, and image segmentation have been\\ndeveloped, but they may confuse roads with surrounding objects due to the complexity of\\nroad edges [ 43]. At the same time, most of these semi-automatic methods focus on image\\nraster processing, and there is little consideration of centerline extraction, vectorization,\\nand road network generation. However, road vectors play a vital role in the construction\\nof geographic information systems and the comprehensive analysis of information, and\\nthey need to be taken seriously [ 35,44]. Moreover, the methods mentioned above cannot\\ncompletely overcome road extraction difﬁculties, such as occlusions, shadows, and varying\\nresolutions and widths.\\nConsidering the problems described above, to improve the labeling efﬁciency of the\\nroad dataset, we propose a new method of interactive road extraction and automatic road\\nnetwork generation. The main contributions of this paper follow:\\n1. A complete road network extraction framework with high accuracy and availability\\nis proposed. With only a few seed points, the whole road can be obtained quickly.\\nFirst, the width and seed points of a road are set interactively, and the skeleton of the\\nroad is extracted by using regional growth and morphological algorithms. Then, a\\nsingle-road vector is obtained after vector tracking, vector simpliﬁcation, endpoint\\nmodiﬁcation, and road connection. Finally, the road network is generated by using\\nintersection connection and buffer algorithms.\\n2. To further improve the effectiveness of the proposed method, we adopt the road\\nsegment modiﬁcation and road network construction strategy using the combination\\nof grid image level and vector level. At the raster level, morphological algorithms are\\nused to acquire the initial road segment, and at the vector level, further corrections\\nand connections are completed based on road geometric features. For example,\\nconsidering the ‘T’, ‘Y’, and ‘+’ shape of the intersection, an intersection connection\\nalgorithm is proposed.\\n3. The strategy proposed in this paper can be successfully applied to the extraction of\\nrural areas, suburbs, and urban areas. At the same time, it also has a certain degree of\\ncorrection effect on occlusion and shadow problems. The algorithm for extracting a\\nsingle road can extract roads with a length of more than 4000 pixels at a time, whichAppl. Sci. 2022 ,12, 4705 3 of 21\\nis fast and convenient, and has great potential for the application of labeling images\\nfor deep learning.\\n2. Materials and Methods\\n2.1. Experimental Data\\nFour different types of remote-sensing images were selected for road extraction ex-\\nperiments. One type is aerial images with a spatial resolution of 0.2 m of a certain rural\\narea in Huizhou, Guangdong Province, as shown in Figure 1a, an image with a size of\\n5000 pixels\\x025000 pixels is selected and recorded as Data 1. The second type is GF-2\\nsatellite images with a spatial resolution of 1 m of a certain suburban district in Wuhan,\\nHubei Province, as shown in Figure 1b, an image with the same size as Data 1 is selected\\nand recorded as Data 2. The third type is a partial image of the Massachusetts Road Data\\nSet [ 45], with a size of 630 pixels \\x02600 pixels and spatial resolution of 1 m, which is\\nrecorded as Data 3 and shown in Figure 1c. For this image, a part of the image containing\\nthe road was left, and more details of the road can be seen by zooming in. The last image\\nis one image in DeepGlobe Datasets [ 46], with a size of 1024 pixels \\x021024 pixels and a\\nspatial resolution of 0.5 m, which is recorded as Data 4 and shown in Figure 1d. Data 1 has\\nonly 6 long roads and 4 simple intersections but at least 8 obvious occlusions on the roads.\\nData 2 includes 21 roads and 26 different intersections, together with 14 areas of buildings.\\nAdditionally, there are more than 15 roads and 20 intersections in Data 3 and Data 4, with\\nmany buildings on the roadside.\\nAppl.\\xa0Sci.\\xa02022,\\xa012,\\xa0x\\xa0FOR\\xa0PEER\\xa0REVIEW \\xa0 3\\xa0of\\xa022\\xa0\\n\\xa0\\nof\\xa0correction \\xa0effect\\xa0on\\xa0occlusion \\xa0and\\xa0shadow\\xa0problems. \\xa0The\\xa0algorithm \\xa0for\\xa0extracting \\xa0\\na\\xa0single\\xa0road\\xa0can\\xa0extract\\xa0roads\\xa0with\\xa0a\\xa0length\\xa0of\\xa0more\\xa0than\\xa04000\\xa0pixels\\xa0at\\xa0a\\xa0time,\\xa0which\\xa0\\nis\\xa0fast\\xa0and\\xa0convenient, \\xa0and\\xa0has\\xa0great\\xa0potential \\xa0for\\xa0the\\xa0application \\xa0of\\xa0labeling\\xa0images\\xa0\\nfor\\xa0deep\\xa0learning. \\xa0\\n2.\\xa0Materials \\xa0and\\xa0Methods \\xa0\\n2.1.\\xa0Experimental \\xa0Data\\xa0\\nFour\\xa0different \\xa0types\\xa0of\\xa0remote‐sensing\\xa0images\\xa0were\\xa0selected\\xa0for\\xa0road\\xa0extraction \\xa0ex‐\\nperiments. \\xa0One\\xa0type\\xa0is\\xa0aerial\\xa0images\\xa0with\\xa0a\\xa0spatial\\xa0resolution \\xa0of\\xa00.2\\xa0m\\xa0of\\xa0a\\xa0certain\\xa0rural\\xa0\\narea\\xa0in\\xa0Huizhou, \\xa0Guangdong \\xa0Province, \\xa0as\\xa0shown\\xa0in\\xa0Figure\\xa01a,\\xa0an\\xa0image\\xa0with\\xa0a\\xa0size\\xa0of\\xa0\\n5000\\xa0pixels\\xa0×\\xa05000\\xa0pixels\\xa0is\\xa0selected\\xa0and\\xa0recorded \\xa0as\\xa0Data\\xa01.\\xa0The\\xa0second\\xa0type\\xa0is\\xa0GF‐2\\xa0sat‐\\nellite\\xa0images\\xa0with\\xa0a\\xa0spatial\\xa0resolution \\xa0of\\xa01\\xa0m\\xa0of\\xa0a\\xa0certain\\xa0suburban \\xa0district\\xa0in\\xa0Wuhan,\\xa0Hu‐\\nbei\\xa0Province, \\xa0as\\xa0shown\\xa0in\\xa0Figure\\xa01b,\\xa0an\\xa0image\\xa0with\\xa0the\\xa0same\\xa0size\\xa0as\\xa0Data\\xa01\\xa0is\\xa0selected\\xa0and\\xa0\\nrecorded \\xa0as\\xa0Data\\xa02.\\xa0The\\xa0third\\xa0type\\xa0is\\xa0a\\xa0partial\\xa0image\\xa0of\\xa0the\\xa0Massachusetts \\xa0Road\\xa0Data\\xa0Set\\xa0\\n[45],\\xa0with\\xa0a\\xa0size\\xa0of\\xa0630\\xa0pixels\\xa0×\\xa0600\\xa0pixels\\xa0and\\xa0spatial\\xa0resolution \\xa0of\\xa01\\xa0m,\\xa0which\\xa0is\\xa0recorded \\xa0\\nas\\xa0Data\\xa03\\xa0and\\xa0shown\\xa0in\\xa0Figure\\xa01c.\\xa0For\\xa0this\\xa0image,\\xa0a\\xa0part\\xa0of\\xa0the\\xa0image\\xa0containing \\xa0the\\xa0road\\xa0\\nwas\\xa0left,\\xa0and\\xa0more\\xa0details\\xa0of\\xa0the\\xa0road\\xa0can\\xa0be\\xa0seen\\xa0by\\xa0zooming \\xa0in.\\xa0The\\xa0last\\xa0image\\xa0is\\xa0one\\xa0\\nimage\\xa0in\\xa0DeepGlobe \\xa0Datasets\\xa0[46],\\xa0with\\xa0a\\xa0size\\xa0of\\xa01024\\xa0pixels\\xa0×\\xa01024\\xa0pixels\\xa0and\\xa0a\\xa0spatial\\xa0\\nresolution \\xa0of\\xa00.5\\xa0m,\\xa0which\\xa0is\\xa0recorded \\xa0as\\xa0Data\\xa04\\xa0and\\xa0shown\\xa0in\\xa0Figure\\xa01d.\\xa0Data\\xa01\\xa0has\\xa0only\\xa0\\n6\\xa0long\\xa0roads\\xa0and\\xa04\\xa0simple\\xa0intersections \\xa0but\\xa0at\\xa0least\\xa08\\xa0obvious\\xa0occlusions \\xa0on\\xa0the\\xa0roads.\\xa0Data\\xa0\\n2\\xa0includes\\xa021\\xa0roads\\xa0and\\xa026\\xa0different \\xa0intersections, \\xa0together\\xa0with\\xa014\\xa0areas\\xa0of\\xa0buildings. \\xa0Ad‐\\nditionally, \\xa0there\\xa0are\\xa0more\\xa0than\\xa015\\xa0roads\\xa0and\\xa020\\xa0intersections \\xa0in\\xa0Data\\xa03\\xa0and\\xa0Data\\xa04,\\xa0with\\xa0\\nmany\\xa0buildings \\xa0on\\xa0the\\xa0roadside. \\xa0\\n\\xa0\\xa0\\n(a)\\xa0 (b)\\xa0\\n\\xa0 \\xa0\\n(c)\\xa0 (d)\\xa0\\nFigure\\xa01.\\xa0Experimental \\xa0data:\\xa0(a)\\xa0Data\\xa01\\xa0in\\xa0a\\xa0rural\\xa0area,\\xa0with\\xa0a\\xa0size\\xa0of\\xa05000\\xa0pixels\\xa0×\\xa05000\\xa0pixels\\xa0and\\xa0a\\xa0\\nspatial\\xa0resolution \\xa0of\\xa00.2\\xa0m;\\xa0(b)\\xa0Data\\xa02\\xa0in\\xa0a\\xa0suburban \\xa0area,\\xa0with\\xa0a\\xa0size\\xa0of\\xa05000\\xa0pixels\\xa0×\\xa05000\\xa0pixels\\xa0and\\xa0\\na\\xa0spatial\\xa0resolution \\xa0of\\xa01.0\\xa0m;\\xa0(c)\\xa0Data\\xa03\\xa0in\\xa0an\\xa0urban\\xa0area\\xa0with\\xa0a\\xa0size\\xa0of\\xa0630\\xa0pixels\\xa0×\\xa0600\\xa0pixels\\xa0and\\xa0\\nFigure 1. Experimental data: ( a) Data 1 in a rural area, with a size of 5000 pixels \\x025000 pixels and a\\nspatial resolution of 0.2 m; ( b) Data 2 in a suburban area, with a size of 5000 pixels \\x025000 pixels and a\\nspatial resolution of 1.0 m; ( c) Data 3 in an urban area with a size of 630 pixels \\x02600 pixels and also a\\nspatial resolution of 1 m; ( d) Data 4 in a rural residential area with a size of 1024 pixels\\x021024 pixels\\nand a spatial resolution of 0.5 m.Appl. Sci. 2022 ,12, 4705 4 of 21\\n2.2. Methodology\\nIn remote-sensing images, roads have obvious shape characteristics, including high\\naspect ratio, small width changes, small curvatures, and ‘T’-, ‘Y’-, and ‘+’-shaped intersec-\\ntions, etc. The radiation characteristics of the roads are also obvious, such as consistent\\ngray values, obvious edges, and uniform textures, etc. As shown in Figure 2, based on the\\ncharacteristics of the road, the road network extraction method is composed of two parts:\\ninteractive single-road extraction and road network generation.\\nAppl.\\xa0Sci.\\xa02022,\\xa012,\\xa0x\\xa0FOR\\xa0PEER\\xa0REVIEW \\xa0 4\\xa0of\\xa022\\xa0\\n\\xa0\\nalso\\xa0a\\xa0spatial\\xa0resolution \\xa0of\\xa01\\xa0m;\\xa0(d)\\xa0Data\\xa04\\xa0in\\xa0a\\xa0rural\\xa0residential \\xa0area\\xa0with\\xa0a\\xa0size\\xa0of\\xa01024\\xa0pixels\\xa0×\\xa01024\\xa0\\npixels\\xa0and\\xa0a\\xa0spatial\\xa0resolution \\xa0of\\xa00.5\\xa0m.\\xa0\\n2.2.\\xa0Methodology \\xa0\\nIn\\xa0remote‐sensing\\xa0images,\\xa0roads\\xa0have\\xa0obvious\\xa0shape\\xa0characteristics, \\xa0including \\xa0high\\xa0\\naspect\\xa0ratio,\\xa0small\\xa0width\\xa0changes, \\xa0small\\xa0curvatures, \\xa0and\\xa0‘T’‐,\\xa0‘Y’‐,\\xa0and\\xa0‘+’‐shaped\\xa0intersec‐\\ntions,\\xa0etc.\\xa0The\\xa0radiation \\xa0characteristics \\xa0of\\xa0the\\xa0roads\\xa0are\\xa0also\\xa0obvious, \\xa0such\\xa0as\\xa0consistent \\xa0\\ngray\\xa0values,\\xa0obvious\\xa0edges,\\xa0and\\xa0uniform\\xa0textures, \\xa0etc.\\xa0As\\xa0shown\\xa0in\\xa0Figure\\xa02,\\xa0based\\xa0on\\xa0the\\xa0\\ncharacteristics \\xa0of\\xa0the\\xa0road,\\xa0the\\xa0road\\xa0network\\xa0extraction \\xa0method\\xa0is\\xa0composed \\xa0of\\xa0two\\xa0parts:\\xa0\\ninteractive \\xa0single‐road\\xa0extraction \\xa0and\\xa0road\\xa0network\\xa0generation. \\xa0\\n\\xa0\\nFigure\\xa02.\\xa0Road\\xa0network\\xa0extraction \\xa0flow\\xa0chart.\\xa0\\nSingle‐road\\xa0extraction \\xa0is\\xa0performed \\xa0in\\xa0four\\xa0steps.\\xa0First,\\xa0a\\xa0single\\xa0growing \\xa0point\\xa0on\\xa0the\\xa0\\nroad\\xa0is\\xa0selected\\xa0manually \\xa0to\\xa0acquire\\xa0the\\xa0road\\xa0width\\xa0automatically. \\xa0Several\\xa0seed\\xa0points\\xa0are\\xa0\\nfurther\\xa0selected\\xa0and\\xa0a\\xa0working \\xa0area\\xa0of\\xa0the\\xa0algorithm \\xa0is\\xa0generated \\xa0at\\xa0the\\xa0same\\xa0time\\xa0accord‐\\ning\\xa0to\\xa0the\\xa0road\\xa0width.\\xa0Second,\\xa0the\\xa0skeleton\\xa0of\\xa0the\\xa0road\\xa0is\\xa0obtained \\xa0by\\xa0region\\xa0growth,\\xa0mor‐\\nphological \\xa0closing\\xa0operations, \\xa0and\\xa0image\\xa0thinning. \\xa0Third,\\xa0a\\xa0tracking\\xa0algorithm \\xa0is\\xa0used\\xa0to\\xa0\\nvectorize \\xa0the\\xa0skeleton. \\xa0Then,\\xa0the\\xa0optimized \\xa0road\\xa0segments \\xa0are\\xa0obtained \\xa0through\\xa0simplifi‐\\ncation\\xa0and\\xa0deletion\\xa0of\\xa0overly\\xa0short\\xa0segments. \\xa0Finally,\\xa0the\\xa0endpoint \\xa0modifications \\xa0and\\xa0road\\xa0\\nconnections \\xa0are\\xa0processed \\xa0to\\xa0optimize \\xa0the\\xa0road\\xa0segments \\xa0further,\\xa0and\\xa0the\\xa0broken\\xa0road\\xa0\\nsegments \\xa0are\\xa0connected \\xa0into\\xa0a\\xa0complete \\xa0road\\xa0vector.\\xa0\\nAfter\\xa0obtaining \\xa0a\\xa0series\\xa0of\\xa0road\\xa0vectors\\xa0one\\xa0by\\xa0one,\\xa0the\\xa0road\\xa0network\\xa0generation \\xa0al‐\\ngorithm\\xa0is\\xa0performed. \\xa0First,\\xa0complete \\xa0vectorized \\xa0centerlines \\xa0are\\xa0generated \\xa0by\\xa0intersection \\xa0\\nconnection. \\xa0After\\xa0that,\\xa0the\\xa0road\\xa0network\\xa0is\\xa0formed\\xa0through\\xa0the\\xa0buffer\\xa0algorithm. \\xa0\\n2.2.1.\\xa0Interactive \\xa0Single‐Road\\xa0Extraction \\xa0\\nImage\\xa0Road\\xa0Skeleton \\xa0Extraction \\xa0\\nTo\\xa0obtain\\xa0a\\xa0complete \\xa0road\\xa0skeleton\\xa0from\\xa0high‐resolution \\xa0images,\\xa0considering \\xa0the\\xa0\\nglitches,\\xa0holes,\\xa0and\\xa0interruptions \\xa0that\\xa0may\\xa0be\\xa0encountered \\xa0during\\xa0the\\xa0extraction \\xa0process,\\xa0\\nFigure 2. Road network extraction ﬂow chart.\\nSingle-road extraction is performed in four steps. First, a single growing point on the\\nroad is selected manually to acquire the road width automatically. Several seed points\\nare further selected and a working area of the algorithm is generated at the same time\\naccording to the road width. Second, the skeleton of the road is obtained by region growth,\\nmorphological closing operations, and image thinning. Third, a tracking algorithm is\\nused to vectorize the skeleton. Then, the optimized road segments are obtained through\\nsimpliﬁcation and deletion of overly short segments. Finally, the endpoint modiﬁcations\\nand road connections are processed to optimize the road segments further, and the broken\\nroad segments are connected into a complete road vector.\\nAfter obtaining a series of road vectors one by one, the road network generation\\nalgorithm is performed. First, complete vectorized centerlines are generated by intersection\\nconnection. After that, the road network is formed through the buffer algorithm.\\n2.2.1. Interactive Single-Road Extraction\\nImage Road Skeleton Extraction\\nTo obtain a complete road skeleton from high-resolution images, considering the\\nglitches, holes, and interruptions that may be encountered during the extraction process, we\\nuse regional growth, morphological closing operations, and thinning algorithms to extract\\nroads. To improve the accuracy and efﬁciency of road extraction, the spectral and geometric\\ncharacteristics of roads are considered. We determine the implementation area of the\\nalgorithm according to the road width, thereby avoiding the problem of excessive growth.Appl. Sci. 2022 ,12, 4705 5 of 21\\nFor the purpose of obtaining the road width, we ﬁrst perform regional growth in a\\nlocal area to obtain one segment with parallel edges. The operation of regional growth\\ncan be described as follows: ﬁrst, select a certain pixel on the road as the growing point.\\nEspecially, for road width determination, the growing point should be located on a road\\nsegment without glitches, holes, or interruptions. Next, compare the grayscale of the\\ngrowing point with its eight neighborhoods. Finally, combine the neighbor pixels meeting\\nthe merge threshold requirements with growing point and set it as a new growing point\\nuntil no new pixels are combined. Regional growth has a simple rule, high calculating\\nspeed and interactivity, which is suitable for road extraction requirements. The road width\\nL is deﬁned in meters by calculating the minimum distance between the edges of the local\\nroad segment.\\nThen, several seed points are selected manually on the road from one end to the other,\\nand a working area of the regional growth algorithm is limited by a buffer algorithm with\\nthe line of seed points as the central axis. When selecting seed points, we only need to\\nensure that the working area can cover the whole road. As shown in Figure 3a, the seed\\npoints are in red and the green border delineates the working area. Then, the operations of\\nregional growth and morphological processing are performed in the working area.\\nAppl.\\xa0Sci.\\xa02022,\\xa012,\\xa0x\\xa0FOR\\xa0PEER\\xa0REVIEW \\xa0 5\\xa0of\\xa022\\xa0\\n\\xa0\\nwe\\xa0use\\xa0regional\\xa0growth,\\xa0morphological \\xa0closing\\xa0operations, \\xa0and\\xa0thinning \\xa0algorithms \\xa0to\\xa0\\nextract\\xa0roads.\\xa0To\\xa0improve \\xa0the\\xa0accuracy \\xa0and\\xa0efficiency \\xa0of\\xa0road\\xa0extraction, \\xa0the\\xa0spectral\\xa0and\\xa0\\ngeometric \\xa0characteristics \\xa0of\\xa0roads\\xa0are\\xa0considered. \\xa0We\\xa0determine \\xa0the\\xa0implementation \\xa0area\\xa0\\nof\\xa0the\\xa0algorithm \\xa0according \\xa0to\\xa0the\\xa0road\\xa0width,\\xa0thereby\\xa0avoiding \\xa0the\\xa0problem\\xa0of\\xa0excessive \\xa0\\ngrowth.\\xa0\\nFor\\xa0the\\xa0purpose\\xa0of\\xa0obtaining \\xa0the\\xa0road\\xa0width,\\xa0we\\xa0first\\xa0perform\\xa0regional\\xa0growth\\xa0in\\xa0a\\xa0\\nlocal\\xa0area\\xa0to\\xa0obtain\\xa0one\\xa0segment\\xa0with\\xa0parallel\\xa0edges.\\xa0The\\xa0operation \\xa0of\\xa0regional\\xa0growth\\xa0can\\xa0\\nbe\\xa0described \\xa0as\\xa0follows:\\xa0first,\\xa0select\\xa0a\\xa0certain\\xa0pixel\\xa0on\\xa0the\\xa0road\\xa0as\\xa0the\\xa0growing \\xa0point.\\xa0Espe‐\\ncially,\\xa0for\\xa0road\\xa0width\\xa0determination, \\xa0the\\xa0growing \\xa0point\\xa0should\\xa0be\\xa0located\\xa0on\\xa0a\\xa0road\\xa0seg‐\\nment\\xa0without\\xa0glitches,\\xa0holes,\\xa0or\\xa0interruptions. \\xa0Next,\\xa0compare \\xa0the\\xa0grayscale \\xa0of\\xa0the\\xa0growing\\xa0\\npoint\\xa0with\\xa0its\\xa0eight\\xa0neighborhoods. \\xa0Finally,\\xa0combine \\xa0the\\xa0neighbor \\xa0pixels\\xa0meeting\\xa0the\\xa0\\nmerge\\xa0threshold \\xa0requirements \\xa0with\\xa0growing \\xa0point\\xa0and\\xa0set\\xa0it\\xa0as\\xa0a\\xa0new\\xa0growing \\xa0point\\xa0until\\xa0\\nno\\xa0new\\xa0pixels\\xa0are\\xa0combined. \\xa0Regional \\xa0growth\\xa0has\\xa0a\\xa0simple\\xa0rule,\\xa0high\\xa0calculating \\xa0speed\\xa0\\nand\\xa0interactivity, \\xa0which\\xa0is\\xa0suitable\\xa0for\\xa0road\\xa0extraction \\xa0requirements. \\xa0The\\xa0road\\xa0width\\xa0L\\xa0is\\xa0\\ndefined\\xa0in\\xa0meters\\xa0by\\xa0calculating \\xa0the\\xa0minimum \\xa0distance\\xa0between\\xa0the\\xa0edges\\xa0of\\xa0the\\xa0local\\xa0\\nroad\\xa0segment. \\xa0\\nThen,\\xa0several\\xa0seed\\xa0points\\xa0are\\xa0selected\\xa0manually \\xa0on\\xa0the\\xa0road\\xa0from\\xa0one\\xa0end\\xa0to\\xa0the\\xa0\\nother,\\xa0and\\xa0a\\xa0working \\xa0area\\xa0of\\xa0the\\xa0regional\\xa0growth\\xa0algorithm \\xa0is\\xa0limited\\xa0by\\xa0a\\xa0buffer\\xa0algo‐\\nrithm\\xa0with\\xa0the\\xa0line\\xa0of\\xa0seed\\xa0points\\xa0as\\xa0the\\xa0central\\xa0axis.\\xa0When\\xa0selecting \\xa0seed\\xa0points,\\xa0we\\xa0only\\xa0\\nneed\\xa0to\\xa0ensure\\xa0that\\xa0the\\xa0working \\xa0area\\xa0can\\xa0cover\\xa0the\\xa0whole\\xa0road.\\xa0As\\xa0shown\\xa0in\\xa0Figure\\xa03a,\\xa0\\nthe\\xa0seed\\xa0points\\xa0are\\xa0in\\xa0red\\xa0and\\xa0the\\xa0green\\xa0border\\xa0delineates \\xa0the\\xa0working \\xa0area.\\xa0Then,\\xa0the\\xa0\\noperations \\xa0of\\xa0regional\\xa0growth\\xa0and\\xa0morphological \\xa0processing \\xa0are\\xa0performed \\xa0in\\xa0the\\xa0work‐\\ning\\xa0area.\\xa0\\n\\xa0\\xa0\\xa0\\xa0\\n(a)\\xa0 (b)\\xa0 (c)\\xa0 (d)\\xa0\\nFigure\\xa03.\\xa0Road\\xa0skeleton\\xa0extraction: \\xa0(a)\\xa0test\\xa0image,\\xa0where\\xa0seed\\xa0points\\xa0are\\xa0shown\\xa0in\\xa0red;\\xa0(b)\\xa0regional\\xa0\\ngrowth\\xa0result;\\xa0(c)\\xa0closing\\xa0operation \\xa0result;\\xa0(d)\\xa0thinning \\xa0result.\\xa0\\nTo\\xa0extract\\xa0one\\xa0long\\xa0road\\xa0at\\xa0a\\xa0time,\\xa0we\\xa0use\\xa0the\\xa0strategy\\xa0of\\xa0growing \\xa0multiple \\xa0seed\\xa0\\npoints\\xa0simultaneously. \\xa0This\\xa0method\\xa0has\\xa0the\\xa0advantages \\xa0of\\xa0simple\\xa0rules\\xa0and\\xa0a\\xa0high\\xa0calcu‐\\nlation\\xa0speed.\\xa0The\\xa0regional\\xa0growth\\xa0results\\xa0are\\xa0shown\\xa0in\\xa0Figure\\xa03b,\\xa0and\\xa0a\\xa0relatively \\xa0com‐\\nplete\\xa0road\\xa0can\\xa0be\\xa0extracted. \\xa0However, \\xa0there\\xa0are\\xa0burrs\\xa0and\\xa0rough\\xa0areas\\xa0on\\xa0the\\xa0edges\\xa0of\\xa0the\\xa0\\ngenerated \\xa0roads\\xa0due\\xa0to\\xa0the\\xa0pixel‐by‐pixel\\xa0processing \\xa0of\\xa0the\\xa0regional\\xa0growth\\xa0algorithm; \\xa0\\nthere\\xa0are\\xa0also\\xa0many\\xa0holes\\xa0inside\\xa0the\\xa0road,\\xa0which\\xa0are\\xa0caused\\xa0by\\xa0incomplete \\xa0growth\\xa0due\\xa0to\\xa0\\nshadows, \\xa0occlusions, \\xa0etc.\\xa0Therefore, \\xa0further\\xa0optimization \\xa0of\\xa0the\\xa0regional\\xa0growth\\xa0results\\xa0is\\xa0\\nneeded.\\xa0To\\xa0minimize \\xa0the\\xa0errors\\xa0of\\xa0holes\\xa0and\\xa0fragmented \\xa0growth,\\xa0this\\xa0method\\xa0adopts\\xa0the\\xa0\\nmorphological \\xa0closing\\xa0operation, \\xa0which\\xa0is\\xa0most\\xa0commonly \\xa0used\\xa0for\\xa0trimming \\xa0edges\\xa0and\\xa0\\nfilling\\xa0gaps,\\xa0to\\xa0optimize \\xa0the\\xa0initial\\xa0extraction \\xa0results.\\xa0The\\xa0morphological \\xa0closing\\xa0operation \\xa0\\n[12]\\xa0is\\xa0defined\\xa0as\\xa0follows:\\xa0\\nA∙Bൌ ሺA⊕B ሻ⊖B,\\xa0 (1)\\xa0\\nFigure 3. Road skeleton extraction: ( a) test image, where seed points are shown in red; ( b) regional\\ngrowth result; ( c) closing operation result; ( d) thinning result.\\nTo extract one long road at a time, we use the strategy of growing multiple seed points\\nsimultaneously. This method has the advantages of simple rules and a high calculation\\nspeed. The regional growth results are shown in Figure 3b, and a relatively complete road\\ncan be extracted. However, there are burrs and rough areas on the edges of the generated\\nroads due to the pixel-by-pixel processing of the regional growth algorithm; there are\\nalso many holes inside the road, which are caused by incomplete growth due to shadows,\\nocclusions, etc. Therefore, further optimization of the regional growth results is needed. To\\nminimize the errors of holes and fragmented growth, this method adopts the morphological\\nclosing operation, which is most commonly used for trimming edges and ﬁlling gaps, to\\noptimize the initial extraction results. The morphological closing operation [ 12] is deﬁned\\nas follows:\\nA\\x01B=(A\\x08B)\\tB, (1)\\nIn the formula, the operator \\x01denotes the closing operation, A represents the binary\\nimage, and B represents the structural elements. The symbols \\x08and\\tdenote the expansion\\nand corrosion operators, respectively. Figure 3c shows the result of the morphological\\nclosing operation.\\nThe road skeleton is the basis for generating the road network. Therefore, we use\\na morphological thinning algorithm to skeletonize the previously extracted roads. By\\nprogressively stripping the road pixels, only a single-pixel-width skeleton is left to maintain\\nconnectivity. The Zhang–Suen parallel algorithm [ 47] is used for reﬁnement. Because of its\\nlow number of iterations and high speed, it has superior thinning ability at intersectionsAppl. Sci. 2022 ,12, 4705 6 of 21\\nand is widely used in the ﬁeld of image thinning [ 3,12,48]. By searching the points of roads\\none by one in the binary image obtained by the morphological closing operation, where\\na road point has a gray value of 255 and a background point has a value of 0, boundary\\npoints that do not affect road connectivity are deleted. The operation has two steps:\\nStep 1: Find the boundary points that meet the following requirements:\\n2\\x14N1\\x146, (2)\\nT(p1)=1, (3)\\nIn the formulas, N1means the number of road points among eight neighborhoods, and\\nT(p1)is the number of transitions of gray values from 0 to 255 among eight neighborhoods.\\nStep 2: if a boundary point found meets requirements of either group A or group B, its\\ngray value will be changed to 0, which means it is deleted. In the formulas, pimeans the\\ngray value of the i-th pixel. The arrangement of pixels is shown in Figure 4.\\nAppl.\\xa0Sci.\\xa02022,\\xa012,\\xa0x\\xa0FOR\\xa0PEER\\xa0REVIEW \\xa0 6\\xa0of\\xa022\\xa0\\n\\xa0\\nIn\\xa0the\\xa0formula, \\xa0the\\xa0operator \\xa0∙\\xa0denotes\\xa0the\\xa0closing\\xa0operation, \\xa0A\\xa0represents \\xa0the\\xa0binary\\xa0\\nimage,\\xa0and\\xa0B\\xa0represents \\xa0the\\xa0structural \\xa0elements. \\xa0The\\xa0symbols\\xa0⊕\\xa0and\\xa0⊖\\xa0denote\\xa0the\\xa0ex‐\\npansion\\xa0and\\xa0corrosion \\xa0operators, \\xa0respectively. \\xa0Figure\\xa03c\\xa0shows\\xa0the\\xa0result\\xa0of\\xa0the\\xa0morpho‐\\nlogical\\xa0closing\\xa0operation. \\xa0\\nThe\\xa0road\\xa0skeleton\\xa0is\\xa0the\\xa0basis\\xa0for\\xa0generating \\xa0the\\xa0road\\xa0network. \\xa0Therefore, \\xa0we\\xa0use\\xa0a\\xa0\\nmorphological \\xa0thinning \\xa0algorithm \\xa0to\\xa0skeletonize \\xa0the\\xa0previously \\xa0extracted \\xa0roads.\\xa0By\\xa0pro‐\\ngressively \\xa0stripping \\xa0the\\xa0road\\xa0pixels,\\xa0only\\xa0a\\xa0single‐pixel‐width\\xa0skeleton\\xa0is\\xa0left\\xa0to\\xa0maintain \\xa0\\nconnectivity. \\xa0The\\xa0Zhang–Suen \\xa0parallel\\xa0algorithm \\xa0[47]\\xa0is\\xa0used\\xa0for\\xa0refinement. \\xa0Because\\xa0of\\xa0\\nits\\xa0low\\xa0number\\xa0of\\xa0iterations \\xa0and\\xa0high\\xa0speed,\\xa0it\\xa0has\\xa0superior\\xa0thinning \\xa0ability\\xa0at\\xa0intersec‐\\ntions\\xa0and\\xa0is\\xa0widely\\xa0used\\xa0in\\xa0the\\xa0field\\xa0of\\xa0image\\xa0thinning \\xa0[3,12,48]. \\xa0By\\xa0searching \\xa0the\\xa0points\\xa0\\nof\\xa0roads\\xa0one\\xa0by\\xa0one\\xa0in\\xa0the\\xa0binary\\xa0image\\xa0obtained \\xa0by\\xa0the\\xa0morphological \\xa0closing\\xa0operation, \\xa0\\nwhere\\xa0a\\xa0road\\xa0point\\xa0has\\xa0a\\xa0gray\\xa0value\\xa0of\\xa0255\\xa0and\\xa0a\\xa0background \\xa0point\\xa0has\\xa0a\\xa0value\\xa0of\\xa00,\\xa0\\nboundary \\xa0points\\xa0that\\xa0do\\xa0not\\xa0affect\\xa0road\\xa0connectivity \\xa0are\\xa0deleted.\\xa0The\\xa0operation \\xa0has\\xa0two\\xa0\\nsteps:\\xa0\\nStep\\xa01:\\xa0Find\\xa0the\\xa0boundary \\xa0points\\xa0that\\xa0meet\\xa0the\\xa0following \\xa0requirements: \\xa0\\n2\\u0d51N ଵ\\u0d516,\\xa0 (2)\\xa0\\nTሺpଵሻൌ1,\\xa0 (3)\\xa0\\nIn\\xa0the\\xa0formulas, \\xa0Nଵmeans\\xa0the\\xa0number\\xa0of\\xa0road\\xa0points\\xa0among\\xa0eight\\xa0neighborhoods, \\xa0\\nand\\xa0Tሺpଵሻ\\xa0is\\xa0the\\xa0number\\xa0of\\xa0transitions \\xa0of\\xa0gray\\xa0values\\xa0from\\xa00\\xa0to\\xa0255\\xa0among\\xa0eight\\xa0neigh‐\\nborhoods. \\xa0\\nStep\\xa02:\\xa0if\\xa0a\\xa0boundary \\xa0point\\xa0found\\xa0meets\\xa0requirements \\xa0of\\xa0either\\xa0group\\xa0A\\xa0or\\xa0group\\xa0B,\\xa0\\nits\\xa0gray\\xa0value\\xa0will\\xa0be\\xa0changed\\xa0to\\xa00,\\xa0which\\xa0means\\xa0it\\xa0is\\xa0deleted.\\xa0In\\xa0the\\xa0formulas, \\xa0p୧\\xa0means\\xa0\\nthe\\xa0gray\\xa0value\\xa0of\\xa0the\\xa0i‐th\\xa0pixel.\\xa0The\\xa0arrangement \\xa0of\\xa0pixels\\xa0is\\xa0shown\\xa0in\\xa0Figure\\xa04.\\xa0\\nGroup\\xa0A:\\xa0\\npଶ∙pସ∙p\\u0b3aൌ0,\\xa0 (4)\\xa0\\npସ∙p\\u0b3a∙p଼ൌ0,\\xa0 (5)\\xa0\\nGroup\\xa0B:\\xa0\\npଶ∙pସ∙p଼ൌ0,\\xa0 (6)\\xa0\\npଶ∙p\\u0b3a∙p଼ൌ0,\\xa0 (7)\\xa0\\n\\xa0\\nFigure\\xa04.\\xa0Arrangement \\xa0of\\xa0pixels\\xa0in\\xa0the\\xa0operation \\xa0of\\xa0morphological \\xa0thinning. \\xa0\\nFigure 4. Arrangement of pixels in the operation of morphological thinning.\\nGroup A:\\np2\\x01p4\\x01p6=0, (4)\\np4\\x01p6\\x01p8=0, (5)\\nGroup B:\\np2\\x01p4\\x01p8=0, (6)\\np2\\x01p6\\x01p8=0, (7)\\nRepeat the two steps until no more points should be deleted. The result is shown in\\nFigure 3d.\\nRoad Skeleton Vectorization and Optimization\\nAfter the processing described above, a road skeleton with a single-pixel width is\\nacquired. Compared with the raster map, the vector map is more convenient to save and\\nmodify and easier to update and use in data analysis. Therefore, it is necessary to perform\\nvectorization processing on the road skeleton obtained. We use a tracking algorithm to\\nvectorize the road skeleton. To obtain a relatively accurate centerline vector of the road and\\naddress the problems of overly short segments and incomplete roads, various operations\\nare adopted, which include simplifying road segments, deleting overly short segments,\\nmodifying endpoints, and connecting roads. The strategies and algorithms used in each\\npart are introduced below.\\n(1) Road vectorization to generate road segmentsAppl. Sci. 2022 ,12, 4705 7 of 21\\nThis method uses a road tracking algorithm to vectorize the raster road; the results\\nof this algorithm are accurate, and its calculation speed is high. This operation can be\\ndescribed as the following steps:\\nStep 1: Search for a starting road point (with a gray value of 255, pixel p1in Figure 4)\\nin the whole image from top left to bottom right and record its coordinate, then record this\\npoint as the ﬁrst point of a road and change its gray value to 0, that is, delete it.\\nStep 2: search for another road point in a certain order (see Figure 5) in the eight\\nneighborhoods of the ﬁrst point and set it as a new starting point. Then record this point as\\nthe latter point of the road and also change its gray value to 0. Repeat this step until no\\nmore road points are to be recorded.\\nAppl.\\xa0Sci.\\xa02022,\\xa012,\\xa0x\\xa0FOR\\xa0PEER\\xa0REVIEW \\xa0 7\\xa0of\\xa022\\xa0\\n\\xa0\\nRepeat\\xa0the\\xa0two\\xa0steps\\xa0until\\xa0no\\xa0more\\xa0points\\xa0should\\xa0be\\xa0deleted.\\xa0The\\xa0result\\xa0is\\xa0shown\\xa0in\\xa0\\nFigure\\xa03d.\\xa0\\nRoad\\xa0Skeleton \\xa0Vectorization \\xa0and\\xa0Optimization \\xa0\\nAfter\\xa0the\\xa0processing \\xa0described \\xa0above,\\xa0a\\xa0road\\xa0skeleton\\xa0with\\xa0a\\xa0single‐pixel\\xa0width\\xa0is\\xa0\\nacquired. \\xa0Compared \\xa0with\\xa0the\\xa0raster\\xa0map,\\xa0the\\xa0vector\\xa0map\\xa0is\\xa0more\\xa0convenient \\xa0to\\xa0save\\xa0and\\xa0\\nmodify\\xa0and\\xa0easier\\xa0to\\xa0update\\xa0and\\xa0use\\xa0in\\xa0data\\xa0analysis. \\xa0Therefore, \\xa0it\\xa0is\\xa0necessary \\xa0to\\xa0perform\\xa0\\nvectorization \\xa0processing \\xa0on\\xa0the\\xa0road\\xa0skeleton\\xa0obtained. \\xa0We\\xa0use\\xa0a\\xa0tracking\\xa0algorithm \\xa0to\\xa0\\nvectorize \\xa0the\\xa0road\\xa0skeleton. \\xa0To\\xa0obtain\\xa0a\\xa0relatively \\xa0accurate\\xa0centerline \\xa0vector\\xa0of\\xa0the\\xa0road\\xa0\\nand\\xa0address\\xa0the\\xa0problems \\xa0of\\xa0overly\\xa0short\\xa0segments \\xa0and\\xa0incomplete \\xa0roads,\\xa0various\\xa0opera‐\\ntions\\xa0are\\xa0adopted, \\xa0which\\xa0include\\xa0simplifying \\xa0road\\xa0segments, \\xa0deleting\\xa0overly\\xa0short\\xa0seg‐\\nments,\\xa0modifying \\xa0endpoints, \\xa0and\\xa0connecting \\xa0roads.\\xa0The\\xa0strategies \\xa0and\\xa0algorithms \\xa0used\\xa0in\\xa0\\neach\\xa0part\\xa0are\\xa0introduced \\xa0below.\\xa0\\n(1) Road\\xa0vectorization \\xa0to\\xa0generate \\xa0road\\xa0segments \\xa0\\nThis\\xa0method\\xa0uses\\xa0a\\xa0road\\xa0tracking\\xa0algorithm \\xa0to\\xa0vectorize \\xa0the\\xa0raster\\xa0road;\\xa0the\\xa0results\\xa0\\nof\\xa0this\\xa0algorithm \\xa0are\\xa0accurate, \\xa0and\\xa0its\\xa0calculation \\xa0speed\\xa0is\\xa0high.\\xa0This\\xa0operation \\xa0can\\xa0be\\xa0de‐\\nscribed\\xa0as\\xa0the\\xa0following \\xa0steps:\\xa0\\nStep\\xa01:\\xa0Search\\xa0for\\xa0a\\xa0starting\\xa0road\\xa0point\\xa0(with\\xa0a\\xa0gray\\xa0value\\xa0of\\xa0255,\\xa0pixel\\xa0pଵ\\xa0in\\xa0Figure\\xa0\\n4)\\xa0in\\xa0the\\xa0whole\\xa0image\\xa0from\\xa0top\\xa0left\\xa0to\\xa0bottom\\xa0right\\xa0and\\xa0record\\xa0its\\xa0coordinate, \\xa0then\\xa0record\\xa0\\nthis\\xa0point\\xa0as\\xa0the\\xa0first\\xa0point\\xa0of\\xa0a\\xa0road\\xa0and\\xa0change\\xa0its\\xa0gray\\xa0value\\xa0to\\xa00,\\xa0that\\xa0is,\\xa0delete\\xa0it.\\xa0\\nStep\\xa02:\\xa0search\\xa0for\\xa0another\\xa0road\\xa0point\\xa0in\\xa0a\\xa0certain\\xa0order\\xa0(see\\xa0Figure\\xa05)\\xa0in\\xa0the\\xa0eight\\xa0\\nneighborhoods \\xa0of\\xa0the\\xa0first\\xa0point\\xa0and\\xa0set\\xa0it\\xa0as\\xa0a\\xa0new\\xa0starting\\xa0point.\\xa0Then\\xa0record\\xa0this\\xa0point\\xa0\\nas\\xa0the\\xa0latter\\xa0point\\xa0of\\xa0the\\xa0road\\xa0and\\xa0also\\xa0change\\xa0its\\xa0gray\\xa0value\\xa0to\\xa00.\\xa0Repeat\\xa0this\\xa0step\\xa0until\\xa0no\\xa0\\nmore\\xa0road\\xa0points\\xa0are\\xa0to\\xa0be\\xa0recorded. \\xa0\\n\\xa0\\nFigure\\xa05.\\xa0Arrangement \\xa0order\\xa0of\\xa0pixels\\xa0in\\xa0the\\xa0road\\xa0tracking\\xa0algorithm. \\xa0\\nStep\\xa03:\\xa0As\\xa0the\\xa0first\\xa0point\\xa0may\\xa0not\\xa0be\\xa0the\\xa0actual\\xa0starting\\xa0point\\xa0of\\xa0a\\xa0road,\\xa0set\\xa0the\\xa0first\\xa0\\npoint\\xa0as\\xa0the\\xa0starting\\xa0point\\xa0again\\xa0and\\xa0continue \\xa0searching \\xa0in\\xa0the\\xa0same\\xa0order\\xa0as\\xa0step\\xa02,\\xa0but\\xa0\\nrecord\\xa0the\\xa0new\\xa0point\\xa0found\\xa0as\\xa0a\\xa0previous \\xa0point\\xa0instead,\\xa0indicating \\xa0that\\xa0the\\xa0road\\xa0being\\xa0\\ntracked\\xa0has\\xa0a\\xa0certain\\xa0direction. \\xa0\\nStep\\xa04:\\xa0When\\xa0no\\xa0new\\xa0points\\xa0needs\\xa0to\\xa0be\\xa0recorded \\xa0in\\xa0step\\xa03,\\xa0an\\xa0entire\\xa0road\\xa0has\\xa0been\\xa0\\nrecorded \\xa0and\\xa0all\\xa0the\\xa0points\\xa0of\\xa0it\\xa0have\\xa0been\\xa0deleted.\\xa0Start\\xa0again\\xa0from\\xa0step\\xa01\\xa0to\\xa0track\\xa0a\\xa0new\\xa0\\nroad\\xa0and\\xa0stop\\xa0tracking\\xa0when\\xa0no\\xa0road\\xa0points\\xa0can\\xa0be\\xa0found\\xa0in\\xa0the\\xa0entire\\xa0image.\\xa0\\nThe\\xa0simplifying \\xa0algorithm \\xa0and\\xa0the\\xa0operation \\xa0of\\xa0deleting\\xa0overly\\xa0short\\xa0segments \\xa0are\\xa0\\napplied\\xa0after\\xa0the\\xa0tracking\\xa0algorithm. \\xa0We\\xa0use\\xa0the\\xa0Douglas–Peucker \\xa0algorithm \\xa0[49]\\xa0for\\xa0sim‐\\nplification; \\xa0it\\xa0has\\xa0translation \\xa0and\\xa0rotation\\xa0invariance \\xa0and\\xa0removes \\xa0unnecessary \\xa0points\\xa0by\\xa0\\nsetting\\xa0a\\xa0distance\\xa0threshold \\xa0to\\xa0perform\\xa0data\\xa0compression. \\xa0This\\xa0algorithm \\xa0mainly\\xa0includes\\xa0\\nthese\\xa0following \\xa0steps:\\xa0\\nFigure 5. Arrangement order of pixels in the road tracking algorithm.\\nStep 3: As the ﬁrst point may not be the actual starting point of a road, set the ﬁrst\\npoint as the starting point again and continue searching in the same order as step 2, but\\nrecord the new point found as a previous point instead, indicating that the road being\\ntracked has a certain direction.\\nStep 4: When no new points needs to be recorded in step 3, an entire road has been\\nrecorded and all the points of it have been deleted. Start again from step 1 to track a new\\nroad and stop tracking when no road points can be found in the entire image.\\nThe simplifying algorithm and the operation of deleting overly short segments are\\napplied after the tracking algorithm. We use the Douglas–Peucker algorithm [ 49] for\\nsimpliﬁcation; it has translation and rotation invariance and removes unnecessary points\\nby setting a distance threshold to perform data compression. This algorithm mainly\\nincludes these following steps:\\nStep 1: Connect the two endpoints A and B and record them as reserved points.\\nCalculate the Euclidean distance from each point between the two ends to line AB, and\\nﬁnd the point P with the greatest distance. If there is no point between the two ends, all\\npoints have been simpliﬁed, and the simplifying operation exits.\\nStep 2: Compare the distance from point P to line ABand the simplifying threshold T,\\ndelete point P when the distance is less than T, otherwise, keep it.\\nStep 3: Repeat the steps above with points A and P as endpoints and with B and P as\\nendpoints, respectively.\\nThe simplifying threshold T is deﬁned as:\\nT=k\\x02resolution, (8)\\nwhere resolution is the image resolution in meters; k is a simplifying parameter, which is\\nset to 3.0.\\nFigure 6 shows the principle of the Douglas–Peucker algorithm: the endpoints are\\ndetermined and deleted in turn, and ﬁnally a ﬁve-point polyline is used to describe theAppl. Sci. 2022 ,12, 4705 8 of 21\\ninitial seven-point polyline. Blue points are the ones being judged and then kept, and the\\nred points are the ones judged and then deleted. Red dashed lines show the distances\\nbetween the judged point P and the connecting line of endpoints A and B (blue line) and\\nblack dashed lines are the segments to be deleted in each step.\\nAppl.\\xa0Sci.\\xa02022,\\xa012,\\xa0x\\xa0FOR\\xa0PEER\\xa0REVIEW \\xa0 8\\xa0of\\xa022\\xa0\\n\\xa0\\nStep\\xa01:\\xa0Connect\\xa0the\\xa0two\\xa0endpoints \\xa0A\\xa0and\\xa0B\\xa0and\\xa0record\\xa0them\\xa0as\\xa0reserved \\xa0points.\\xa0Cal‐\\nculate\\xa0the\\xa0Euclidean \\xa0distance\\xa0from\\xa0each\\xa0point\\xa0between\\xa0the\\xa0two\\xa0ends\\xa0to\\xa0line\\xa0ABതതതത,\\xa0and\\xa0find\\xa0\\nthe\\xa0point\\xa0P\\xa0with\\xa0the\\xa0greatest\\xa0distance. \\xa0If\\xa0there\\xa0is\\xa0no\\xa0point\\xa0between\\xa0the\\xa0two\\xa0ends,\\xa0all\\xa0points\\xa0\\nhave\\xa0been\\xa0simplified, \\xa0and\\xa0the\\xa0simplifying \\xa0operation \\xa0exits.\\xa0\\nStep\\xa02:\\xa0Compare \\xa0the\\xa0distance\\xa0from\\xa0point\\xa0P\\xa0to\\xa0line\\xa0ABതതതത\\xa0and\\xa0the\\xa0simplifying \\xa0threshold \\xa0\\nT,\\xa0delete\\xa0point\\xa0P\\xa0when\\xa0the\\xa0distance\\xa0is\\xa0less\\xa0than\\xa0T,\\xa0otherwise, \\xa0keep\\xa0it.\\xa0\\nStep\\xa03:\\xa0Repeat\\xa0the\\xa0steps\\xa0above\\xa0with\\xa0points\\xa0A\\xa0and\\xa0P\\xa0as\\xa0endpoints \\xa0and\\xa0with\\xa0B\\xa0and\\xa0P\\xa0as\\xa0\\nendpoints, \\xa0respectively. \\xa0\\nThe\\xa0simplifying \\xa0threshold \\xa0T\\xa0is\\xa0defined\\xa0as:\\xa0\\nTൌ kൈr e s o l u t i o n ,\\xa0 (8)\\xa0\\nwhere\\xa0resolution \\xa0is\\xa0the\\xa0image\\xa0resolution \\xa0in\\xa0meters;\\xa0k\\xa0is\\xa0a\\xa0simplifying \\xa0parameter, \\xa0which\\xa0is\\xa0\\nset\\xa0to\\xa03.0.\\xa0\\nFigure\\xa06\\xa0shows\\xa0the\\xa0principle \\xa0of\\xa0the\\xa0Douglas–Peucker \\xa0algorithm: \\xa0the\\xa0endpoints \\xa0are\\xa0\\ndetermined \\xa0and\\xa0deleted\\xa0in\\xa0turn,\\xa0and\\xa0finally\\xa0a\\xa0five‐point\\xa0polyline\\xa0is\\xa0used\\xa0to\\xa0describe\\xa0the\\xa0\\ninitial\\xa0seven‐point\\xa0polyline. \\xa0Blue\\xa0points\\xa0are\\xa0the\\xa0ones\\xa0being\\xa0judged\\xa0and\\xa0then\\xa0kept,\\xa0and\\xa0the\\xa0\\nred\\xa0points\\xa0are\\xa0the\\xa0ones\\xa0judged\\xa0and\\xa0then\\xa0deleted.\\xa0Red\\xa0dashed\\xa0lines\\xa0show\\xa0the\\xa0distances \\xa0\\nbetween\\xa0the\\xa0judged\\xa0point\\xa0P\\xa0and\\xa0the\\xa0connecting \\xa0line\\xa0of\\xa0endpoints \\xa0A\\xa0and\\xa0B\\xa0(blue\\xa0line)\\xa0and\\xa0\\nblack\\xa0dashed\\xa0lines\\xa0are\\xa0the\\xa0segments \\xa0to\\xa0be\\xa0deleted\\xa0in\\xa0each\\xa0step.\\xa0\\n\\xa0\\xa0\\xa0\\n(a)\\xa0 (b)\\xa0 (c)\\xa0\\n\\xa0\\xa0\\xa0\\n(d)\\xa0 (e)\\xa0\\xa0\\nFigure\\xa06.\\xa0Principle \\xa0of\\xa0the\\xa0Douglas–Peucker \\xa0algorithm: \\xa0(a)\\xa0initial\\xa0polyline; \\xa0(b–d)\\xa0judgment \\xa0and\\xa0pro‐\\ncessing\\xa0step\\xa0by\\xa0step;\\xa0(e)\\xa0simplifed \\xa0result.\\xa0\\nDuring\\xa0the\\xa0road\\xa0tracking\\xa0process,\\xa0due\\xa0to\\xa0the\\xa0influence \\xa0of\\xa0intersections, \\xa0shadows, \\xa0etc.,\\xa0\\non\\xa0the\\xa0road,\\xa0some\\xa0overly\\xa0short\\xa0segments \\xa0may\\xa0be\\xa0generated, \\xa0as\\xa0shown\\xa0in\\xa0Figure\\xa07a,b.\\xa0These\\xa0\\noverly\\xa0short\\xa0segments \\xa0are\\xa0not\\xa0part\\xa0of\\xa0the\\xa0road\\xa0and\\xa0are\\xa0likely\\xa0to\\xa0cause\\xa0interference \\xa0in\\xa0the\\xa0\\nsubsequent \\xa0connection \\xa0algorithms. \\xa0Therefore, \\xa0we\\xa0delete\\xa0them\\xa0by\\xa0setting\\xa0a\\xa0length\\xa0thresh‐\\nold.\\xa0The\\xa0length\\xa0threshold \\xa0d୲\\xa0is\\xa0defined\\xa0as:\\xa0\\nd୲ൌ1 0ൈL ,\\xa0 (9)\\xa0\\nThis\\xa0definition \\xa0is\\xa0based\\xa0on\\xa0the\\xa0characteristics \\xa0of\\xa0long\\xa0and\\xa0thin\\xa0roads;\\xa0according \\xa0to\\xa0\\nexperience, \\xa0the\\xa0aspect\\xa0ratio\\xa0of\\xa0roads\\xa0is\\xa0generally \\xa0greater\\xa0than\\xa010.\\xa0\\nThe\\xa0road\\xa0vectorization \\xa0algorithm \\xa0is\\xa0shown\\xa0in\\xa0Algorithm \\xa01:\\xa0\\nAlgorithm \\xa01:\\xa0Road\\xa0Vectorization \\xa0\\nInput:\\xa0\\xa0image\\xa0Input\\xa0Image\\xa0\\xa0\\nneighbor \\xa0Neighborhood \\xa0Search\\xa0Order\\xa0\\xa0\\ndt\\xa0Length\\xa0Threshold \\xa0\\nOutput:\\xa0Lines\\xa0Segments \\xa0\\nFigure 6. Principle of the Douglas–Peucker algorithm: ( a) initial polyline; ( b–d) judgment and\\nprocessing step by step; ( e) simplifed result.\\nDuring the road tracking process, due to the inﬂuence of intersections, shadows, etc.,\\non the road, some overly short segments may be generated, as shown in Figure 7a,b. These\\noverly short segments are not part of the road and are likely to cause interference in the\\nsubsequent connection algorithms. Therefore, we delete them by setting a length threshold.\\nThe length threshold d tis deﬁned as:\\ndt=10\\x02L, (9)\\nAppl.\\xa0Sci.\\xa02022,\\xa012,\\xa0x\\xa0FOR\\xa0PEER\\xa0REVIEW \\xa0 9\\xa0of\\xa022\\xa0\\n\\xa0\\n1:\\xa0\\xa0function \\xa0FindLines(image, \\xa0Lines,\\xa0neighbor ,\\xa0dt)\\xa0\\n2:\\xa0\\xa0BEGIN\\xa0\\n3:\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 while\\xa0(findFirstPoint(image, \\xa0firstPt))\\xa0\\xa0\\n4:\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 BEGIN\\xa0\\n5:\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 line.push_back(firstPt); \\xa0\\xa0\\n6:\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 currPt\\xa0=\\xa0firstPt;\\xa0\\xa0\\n7:\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 while\\xa0(findNextPoint(neighbor, \\xa0image,\\xa0currPt,\\xa0nextPt))\\xa0\\xa0\\n8:\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 BEGIN\\xa0\\n9:\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 line.push_back(nextPt); \\xa0\\xa0\\n10:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 currPt\\xa0=\\xa0nextPt;\\xa0\\n11:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 END\\xa0\\n12:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 currPt\\xa0=\\xa0firstPt;\\xa0\\xa0\\n13:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 while\\xa0(findNextPoint(neighbor, \\xa0image,\\xa0currPt,\\xa0nextPt))\\xa0\\n14:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 BEGIN\\xa0\\n15:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 line.push_front(nextPt); \\xa0\\xa0\\n16:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 currPt\\xa0=\\xa0nextPt;\\xa0\\xa0\\n17:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 END\\xa0\\n18:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\xa0(line.length() \\xa0>\\xa0dt)\\xa0\\n19:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 BEGIN\\xa0\\n20:\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 line.simplify(T); \\xa0\\n21:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 Lines.push_back(line); \\xa0\\n22:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 END\\xa0\\n23:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 END\\xa0\\n24:\\xa0\\xa0END\\xa0\\xa0\\nAmong\\xa0the\\xa0functions \\xa0above,\\xa0findFirstPoint \\xa0(image,\\xa0firstPt)\\xa0is\\xa0used\\xa0to\\xa0globally\\xa0search\\xa0\\nfor\\xa0the\\xa0first\\xa0point\\xa0firstPt,\\xa0findNextPoint \\xa0(neighbor, \\xa0image,\\xa0currPt,\\xa0nextPt)\\xa0is\\xa0used\\xa0to\\xa0search\\xa0\\nfor\\xa0the\\xa0next\\xa0point\\xa0nextPt,\\xa0and\\xa0line.simplify \\xa0(T)\\xa0is\\xa0used\\xa0to\\xa0simplify\\xa0the\\xa0line\\xa0segment\\xa0accord‐\\ning\\xa0to\\xa0the\\xa0threshold \\xa0T.\\xa0\\nFigure\\xa07\\xa0shows\\xa0the\\xa0result\\xa0of\\xa0vectorization \\xa0processing: \\xa0(a)\\xa0is\\xa0a\\xa0local\\xa0road\\xa0thinning \\xa0re‐\\nsult,\\xa0(b)\\xa0shows\\xa0the\\xa0road\\xa0segments \\xa0obtained \\xa0by\\xa0the\\xa0tracking\\xa0algorithm, \\xa0(c)\\xa0is\\xa0the\\xa0result\\xa0of\\xa0\\ndeleting\\xa0overly\\xa0short\\xa0segments, \\xa0and\\xa0(d)\\xa0is\\xa0the\\xa0result\\xa0of\\xa0the\\xa0simplifying \\xa0algorithm. \\xa0\\n\\xa0\\xa0\\xa0\\xa0\\n(a)\\xa0 (b)\\xa0 (c)\\xa0 (d)\\xa0\\nFigure\\xa07.\\xa0Vectorization \\xa0to\\xa0obtain\\xa0segments: \\xa0(a)\\xa0result\\xa0after\\xa0thinning; \\xa0(b)\\xa0result\\xa0after\\xa0applying \\xa0the\\xa0\\ntracking\\xa0algorithm; \\xa0(c)\\xa0result\\xa0after\\xa0deleting\\xa0overly\\xa0short\\xa0segments; \\xa0(d)\\xa0result\\xa0after\\xa0simplification. \\xa0\\n(2) Road\\xa0segment\\xa0optimization \\xa0and\\xa0connection \\xa0\\nThe\\xa0vectorization \\xa0operation \\xa0mentioned \\xa0above\\xa0generates \\xa0a\\xa0series\\xa0of\\xa0separate\\xa0road\\xa0seg‐\\nments,\\xa0and\\xa0the\\xa0offset\\xa0problem\\xa0occurring \\xa0at\\xa0the\\xa0head\\xa0and\\xa0tail\\xa0segments \\xa0during\\xa0the\\xa0thinning \\xa0\\nprocess\\xa0cannot\\xa0be\\xa0solved,\\xa0as\\xa0shown\\xa0in\\xa0Figure\\xa08a,c.\\xa0Therefore, \\xa0the\\xa0road\\xa0segments \\xa0generated \\xa0\\nneed\\xa0to\\xa0be\\xa0further\\xa0optimized \\xa0and\\xa0connected \\xa0to\\xa0form\\xa0the\\xa0entire\\xa0road.\\xa0\\nFigure 7. Vectorization to obtain segments: ( a) result after thinning; ( b) result after applying the\\ntracking algorithm; ( c) result after deleting overly short segments; ( d) result after simpliﬁcation.\\nThis deﬁnition is based on the characteristics of long and thin roads; according to\\nexperience, the aspect ratio of roads is generally greater than 10.\\nThe road vectorization algorithm is shown in Algorithm 1:Appl. Sci. 2022 ,12, 4705 9 of 21\\nAlgorithm 1: Road Vectorization\\nInput: image Input Image\\nneighbor Neighborhood Search Order\\ndtLength Threshold\\nOutput: Lines Segments\\n1: function FindLines(image, Lines, neighbor, dt)\\n2: BEGIN\\n3: while (ﬁndFirstPoint(image, ﬁrstPt))\\n4: BEGIN\\n5: line.push_back(ﬁrstPt);\\n6: currPt = ﬁrstPt;\\n7: while (ﬁndNextPoint(neighbor, image, currPt, nextPt))\\n8: BEGIN\\n9: line.push_back(nextPt);\\n10: currPt = nextPt;\\n11: END\\n12: currPt = ﬁrstPt;\\n13: while (ﬁndNextPoint(neighbor, image, currPt, nextPt))\\n14: BEGIN\\n15: line.push_front(nextPt);\\n16: currPt = nextPt;\\n17: END\\n18: if(line.length() > dt)\\n19: BEGIN\\n20: line.simplify(T);\\n21: Lines.push_back(line);\\n22: END\\n23: END\\n24: END\\nAmong the functions above, ﬁndFirstPoint (image, ﬁrstPt) is used to globally search\\nfor the ﬁrst point ﬁrstPt, ﬁndNextPoint (neighbor, image, currPt, nextPt) is used to search\\nfor the next point nextPt, and line.simplify (T) is used to simplify the line segment according\\nto the threshold T.\\nFigure 7 shows the result of vectorization processing: (a) is a local road thinning result,\\n(b) shows the road segments obtained by the tracking algorithm, (c) is the result of deleting\\noverly short segments, and (d) is the result of the simplifying algorithm.\\n(2) Road segment optimization and connection\\nThe vectorization operation mentioned above generates a series of separate road\\nsegments, and the offset problem occurring at the head and tail segments during the\\nthinning process cannot be solved, as shown in Figure 8a,c. Therefore, the road segments\\ngenerated need to be further optimized and connected to form the entire road.\\nConsidering the smoothness of roads, branches and offsets are eliminated by judging\\nthe direction of the endpoints and the neighboring nodes. The principle of endpoint\\nmodiﬁcation is shown in Figure 9, where the dotted line indicates the direction of vector\\nBA. For each segment, two consecutive nodes Aand Badjacent to endpoint Care selected,\\nand\\x12is the angle between the vectors BAand AC. Endpoint Cis retained when \\x12<10\\x0e.\\nOtherwise, it is deleted along with the overly short segment, shown in red in Figure 9.\\nFigure 8b,d show the results of road segment optimization.Appl. Sci. 2022 ,12, 4705 10 of 21\\nAppl.\\xa0Sci.\\xa02022,\\xa012,\\xa0x\\xa0FOR\\xa0PEER\\xa0REVIEW \\xa0 10\\xa0of\\xa022\\xa0\\n\\xa0\\nConsidering \\xa0the\\xa0smoothness \\xa0of\\xa0roads,\\xa0branches \\xa0and\\xa0offsets\\xa0are\\xa0eliminated \\xa0by\\xa0judging\\xa0\\nthe\\xa0direction \\xa0of\\xa0the\\xa0endpoints \\xa0and\\xa0the\\xa0neighboring \\xa0nodes.\\xa0The\\xa0principle \\xa0of\\xa0endpoint \\xa0mod‐\\nification\\xa0is\\xa0shown\\xa0in\\xa0Figure\\xa09,\\xa0where\\xa0the\\xa0dotted\\xa0line\\xa0indicates \\xa0the\\xa0direction \\xa0of\\xa0vector\\xa0BAതതതത.\\xa0\\nFor\\xa0each\\xa0segment, \\xa0two\\xa0consecutive \\xa0nodes\\xa0A\\xa0and\\xa0B\\xa0adjacent\\xa0to\\xa0endpoint \\xa0C\\xa0are\\xa0selected, \\xa0\\nand\\xa0θ\\xa0is\\xa0the\\xa0angle\\xa0between\\xa0the\\xa0vectors\\xa0BAതതതത\\xa0and\\xa0ACതതതത.\\xa0Endpoint \\xa0C\\xa0is\\xa0retained\\xa0when\\xa0θ൏\\n10°.\\xa0Otherwise, \\xa0it\\xa0is\\xa0deleted\\xa0along\\xa0with\\xa0the\\xa0overly\\xa0short\\xa0segment, \\xa0shown\\xa0in\\xa0red\\xa0in\\xa0Figure\\xa0\\n9.\\xa0Figure\\xa08b,d\\xa0show\\xa0the\\xa0results\\xa0of\\xa0road\\xa0segment\\xa0optimization. \\xa0\\n\\xa0\\xa0\\xa0\\xa0\\n(a)\\xa0 (b)\\xa0 (c)\\xa0 (d)\\xa0\\nFigure\\xa08.\\xa0Optimizing \\xa0segments: \\xa0(a,c)\\xa0show\\xa0vectors\\xa0with\\xa0offset\\xa0problems; \\xa0(b,d)\\xa0are\\xa0the\\xa0respective \\xa0\\nresults\\xa0of\\xa0endpoint \\xa0modification. \\xa0\\n\\xa0\\nFigure\\xa09.\\xa0Endpoint \\xa0modification. \\xa0\\nTo\\xa0connect\\xa0the\\xa0optimized \\xa0segments \\xa0into\\xa0an\\xa0entire\\xa0road,\\xa0a\\xa0road\\xa0connection \\xa0algorithm \\xa0\\nis\\xa0designed. \\xa0The\\xa0connection \\xa0rules\\xa0include\\xa0rules\\xa0governing \\xa0the\\xa0distance\\xa0between\\xa0endpoints \\xa0\\nand\\xa0the\\xa0angle\\xa0between\\xa0vectors.\\xa0If\\xa0two\\xa0segments \\xa0meet\\xa0the\\xa0connection \\xa0rules,\\xa0the\\xa0endpoints \\xa0\\nare\\xa0connected. \\xa0\\nThe\\xa0rule\\xa0governing \\xa0the\\xa0distance\\xa0between\\xa0endpoints \\xa0is\\xa0defined\\xa0as\\xa0follows:\\xa0first,\\xa0the\\xa0\\ndistance\\xa0between\\xa0two\\xa0endpoints \\xa0must\\xa0be\\xa0the\\xa0shortest\\xa0globally. \\xa0Second,\\xa0considering \\xa0the\\xa0\\ncontinuous \\xa0and\\xa0uninterrupted \\xa0characteristics \\xa0of\\xa0a\\xa0road,\\xa0we\\xa0again\\xa0choose\\xa0d୲\\xa0as\\xa0the\\xa0dis‐\\ntance\\xa0threshold \\xa0for\\xa0connection. \\xa0The\\xa0distance\\xa0between\\xa0two\\xa0connected \\xa0endpoints \\xa0should\\xa0be\\xa0\\nless\\xa0than\\xa0d୲.\\xa0\\nFurthermore, \\xa0considering \\xa0the\\xa0smoothness \\xa0of\\xa0a\\xa0specific\\xa0road,\\xa0the\\xa0rule\\xa0governing \\xa0the\\xa0\\nangle\\xa0between\\xa0vectors\\xa0is\\xa0defined\\xa0as\\xa0shown\\xa0in\\xa0Figure\\xa010.\\xa0Black\\xa0solid\\xa0lines\\xa0show\\xa0the\\xa0seg‐\\nments,\\xa0black\\xa0dotted\\xa0lines\\xa0show\\xa0their\\xa0extensions, \\xa0and\\xa0blue\\xa0dashed\\xa0lines\\xa0indicate\\xa0the\\xa0hori‐\\nzontal‐right\\xa0direction. \\xa0If\\xa0the\\xa0endpoints \\xa0A\\xa0and\\xa0B\\xa0of\\xa0the\\xa0road\\xa0segment\\xa0meet\\xa0the\\xa0distance\\xa0rule,\\xa0\\nthen\\xa0we\\xa0find\\xa0the\\xa0nodes\\xa0A′\\xa0and\\xa0B′\\xa0adjacent\\xa0to\\xa0the\\xa0endpoints \\xa0A\\xa0and\\xa0B.\\xa0θଵ\\xa0and\\xa0θଶ ሺθଵ,θଶ∈\\nሺ0°, 180° ሿሻ\\xa0are\\xa0the\\xa0angles\\xa0between\\xa0the\\xa0vectors\\xa0A’Aሬሬሬሬሬሬ⃗\\xa0and\\xa0B’Bሬሬሬሬሬሬ⃗\\xa0and\\xa0the\\xa0horizontal ‐right\\xa0di‐\\nrection,\\xa0respectively, \\xa0and\\xa0θൌ |θଵെθ ଶ|.\\xa0If\\xa0θ \\u0d50 90° ,\\xa0the\\xa0endpoints \\xa0are\\xa0connected. \\xa0The\\xa0con‐\\nnection\\xa0line\\xa0is\\xa0shown\\xa0in\\xa0red.\\xa0\\nFigure 8. Optimizing segments: ( a,c) show vectors with offset problems; ( b,d) are the respective\\nresults of endpoint modiﬁcation.\\nAppl.\\xa0Sci.\\xa02022,\\xa012,\\xa0x\\xa0FOR\\xa0PEER\\xa0REVIEW \\xa0 10\\xa0of\\xa022\\xa0\\n\\xa0\\nConsidering \\xa0the\\xa0smoothness \\xa0of\\xa0roads,\\xa0branches \\xa0and\\xa0offsets\\xa0are\\xa0eliminated \\xa0by\\xa0judging\\xa0\\nthe\\xa0direction \\xa0of\\xa0the\\xa0endpoints \\xa0and\\xa0the\\xa0neighboring \\xa0nodes.\\xa0The\\xa0principle \\xa0of\\xa0endpoint \\xa0mod‐\\nification\\xa0is\\xa0shown\\xa0in\\xa0Figure\\xa09,\\xa0where\\xa0the\\xa0dotted\\xa0line\\xa0indicates \\xa0the\\xa0direction \\xa0of\\xa0vector\\xa0BAതതതത.\\xa0\\nFor\\xa0each\\xa0segment, \\xa0two\\xa0consecutive \\xa0nodes\\xa0A\\xa0and\\xa0B\\xa0adjacent\\xa0to\\xa0endpoint \\xa0C\\xa0are\\xa0selected, \\xa0\\nand\\xa0θ\\xa0is\\xa0the\\xa0angle\\xa0between\\xa0the\\xa0vectors\\xa0BAതതതത\\xa0and\\xa0ACതതതത.\\xa0Endpoint \\xa0C\\xa0is\\xa0retained\\xa0when\\xa0θ൏\\n10°.\\xa0Otherwise, \\xa0it\\xa0is\\xa0deleted\\xa0along\\xa0with\\xa0the\\xa0overly\\xa0short\\xa0segment, \\xa0shown\\xa0in\\xa0red\\xa0in\\xa0Figure\\xa0\\n9.\\xa0Figure\\xa08b,d\\xa0show\\xa0the\\xa0results\\xa0of\\xa0road\\xa0segment\\xa0optimization. \\xa0\\n\\xa0\\xa0\\xa0\\xa0\\n(a)\\xa0 (b)\\xa0 (c)\\xa0 (d)\\xa0\\nFigure\\xa08.\\xa0Optimizing \\xa0segments: \\xa0(a,c)\\xa0show\\xa0vectors\\xa0with\\xa0offset\\xa0problems; \\xa0(b,d)\\xa0are\\xa0the\\xa0respective \\xa0\\nresults\\xa0of\\xa0endpoint \\xa0modification. \\xa0\\n\\xa0\\nFigure\\xa09.\\xa0Endpoint \\xa0modification. \\xa0\\nTo\\xa0connect\\xa0the\\xa0optimized \\xa0segments \\xa0into\\xa0an\\xa0entire\\xa0road,\\xa0a\\xa0road\\xa0connection \\xa0algorithm \\xa0\\nis\\xa0designed. \\xa0The\\xa0connection \\xa0rules\\xa0include\\xa0rules\\xa0governing \\xa0the\\xa0distance\\xa0between\\xa0endpoints \\xa0\\nand\\xa0the\\xa0angle\\xa0between\\xa0vectors.\\xa0If\\xa0two\\xa0segments \\xa0meet\\xa0the\\xa0connection \\xa0rules,\\xa0the\\xa0endpoints \\xa0\\nare\\xa0connected. \\xa0\\nThe\\xa0rule\\xa0governing \\xa0the\\xa0distance\\xa0between\\xa0endpoints \\xa0is\\xa0defined\\xa0as\\xa0follows:\\xa0first,\\xa0the\\xa0\\ndistance\\xa0between\\xa0two\\xa0endpoints \\xa0must\\xa0be\\xa0the\\xa0shortest\\xa0globally. \\xa0Second,\\xa0considering \\xa0the\\xa0\\ncontinuous \\xa0and\\xa0uninterrupted \\xa0characteristics \\xa0of\\xa0a\\xa0road,\\xa0we\\xa0again\\xa0choose\\xa0d୲\\xa0as\\xa0the\\xa0dis‐\\ntance\\xa0threshold \\xa0for\\xa0connection. \\xa0The\\xa0distance\\xa0between\\xa0two\\xa0connected \\xa0endpoints \\xa0should\\xa0be\\xa0\\nless\\xa0than\\xa0d୲.\\xa0\\nFurthermore, \\xa0considering \\xa0the\\xa0smoothness \\xa0of\\xa0a\\xa0specific\\xa0road,\\xa0the\\xa0rule\\xa0governing \\xa0the\\xa0\\nangle\\xa0between\\xa0vectors\\xa0is\\xa0defined\\xa0as\\xa0shown\\xa0in\\xa0Figure\\xa010.\\xa0Black\\xa0solid\\xa0lines\\xa0show\\xa0the\\xa0seg‐\\nments,\\xa0black\\xa0dotted\\xa0lines\\xa0show\\xa0their\\xa0extensions, \\xa0and\\xa0blue\\xa0dashed\\xa0lines\\xa0indicate\\xa0the\\xa0hori‐\\nzontal‐right\\xa0direction. \\xa0If\\xa0the\\xa0endpoints \\xa0A\\xa0and\\xa0B\\xa0of\\xa0the\\xa0road\\xa0segment\\xa0meet\\xa0the\\xa0distance\\xa0rule,\\xa0\\nthen\\xa0we\\xa0find\\xa0the\\xa0nodes\\xa0A′\\xa0and\\xa0B′\\xa0adjacent\\xa0to\\xa0the\\xa0endpoints \\xa0A\\xa0and\\xa0B.\\xa0θଵ\\xa0and\\xa0θଶ ሺθଵ,θଶ∈\\nሺ0°, 180° ሿሻ\\xa0are\\xa0the\\xa0angles\\xa0between\\xa0the\\xa0vectors\\xa0A’Aሬሬሬሬሬሬ⃗\\xa0and\\xa0B’Bሬሬሬሬሬሬ⃗\\xa0and\\xa0the\\xa0horizontal ‐right\\xa0di‐\\nrection,\\xa0respectively, \\xa0and\\xa0θൌ |θଵെθ ଶ|.\\xa0If\\xa0θ \\u0d50 90° ,\\xa0the\\xa0endpoints \\xa0are\\xa0connected. \\xa0The\\xa0con‐\\nnection\\xa0line\\xa0is\\xa0shown\\xa0in\\xa0red.\\xa0\\nFigure 9. Endpoint modiﬁcation.\\nTo connect the optimized segments into an entire road, a road connection algorithm is\\ndesigned. The connection rules include rules governing the distance between endpoints\\nand the angle between vectors. If two segments meet the connection rules, the endpoints\\nare connected.\\nThe rule governing the distance between endpoints is deﬁned as follows: ﬁrst, the\\ndistance between two endpoints must be the shortest globally. Second, considering the\\ncontinuous and uninterrupted characteristics of a road, we again choose dtas the distance\\nthreshold for connection. The distance between two connected endpoints should be less\\nthan d t.\\nFurthermore, considering the smoothness of a specific road, the rule governing the angle\\nbetween vectors is defined as shown in Figure 10. Black solid lines show the segments,\\nblack dotted lines show their extensions, and blue dashed lines indicate the horizontal-right\\ndirection. If the endpoints A and B of the road segment meet the distance rule, then we find\\nthe nodes A0and B0adjacent to the endpoints A and B. \\x121and\\x122(\\x121,\\x1222(0\\x0e, 180\\x0e])are the\\nangles between the vectors!\\nA0Aand!\\nB0Band the horizontal-right direction, respectively , and\\n\\x12=j\\x121\\x00\\x122j. If\\x12>90\\x0e, the endpoints are connected. The connection line is shown in red.\\nAppl.\\xa0Sci.\\xa02022,\\xa012,\\xa0x\\xa0FOR\\xa0PEER\\xa0REVIEW \\xa0 11\\xa0of\\xa022\\xa0\\n\\xa0\\n\\xa0\\xa0\\n(a)\\xa0 (b)\\xa0\\nFigure\\xa010.\\xa0Angle\\xa0rule\\xa0between\\xa0vectors:\\xa0(a)\\xa0the\\xa0two\\xa0vectors\\xa0are\\xa0on\\xa0the\\xa0same\\xa0side\\xa0of\\xa0the\\xa0horizontal \\xa0\\nline;\\xa0(b)\\xa0the\\xa0two\\xa0vectors\\xa0are\\xa0on\\xa0opposite \\xa0sides\\xa0of\\xa0the\\xa0horizontal \\xa0line.\\xa0\\nThe\\xa0pseudocode \\xa0of\\xa0the\\xa0road\\xa0connection \\xa0algorithm \\xa0in\\xa0Algorithm \\xa02:\\xa0\\nAlgorithm \\xa02:\\xa0Road\\xa0Connection \\xa0\\nInput:\\xa0\\xa0Lines\\xa0Segments \\xa0after\\xa0Tracking \\xa0Algorithm \\xa0\\nOutput:\\xa0Lines\\xa0Roads\\xa0after\\xa0Connection \\xa0Algorithm \\xa0\\n1:\\xa0\\xa0 function \\xa0LinkLines(&Lines) \\xa0\\n2:\\xa0\\xa0 BEGIN\\xa0\\n3:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 Ver\\xa0=\\xa0FindVertex(Lines); \\xa0\\n4:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 Ver_theta \\xa0=\\xa0FindVertexAzimuth(Lines); \\xa0\\n5:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for\\xa0i\\xa0←\\xa00\\xa0to\\xa0Ver.size() \\xa0\\n6:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 BEGIN\\xa0\\xa0\\n7:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 min\\xa0=\\xa09999.0;\\xa0\\n8:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\xa0(isinConnectedPt(i, \\xa0ConnectedPt)) \\xa0continue; \\xa0\\n9:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for\\xa0j\\xa0←\\xa0(i\\xa0+\\xa01)\\xa0to\\xa0Ver.size() \\xa0\\n10:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 BEGIN\\xa0\\n11:\\xa0\\xa0 if\\xa0(isinSameLine(Ver(i), \\xa0Ver(j)))\\xa0 continue; \\xa0\\n12:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 temp_distance \\xa0=\\xa0cal_Distance(Ver(i),Ver(j)); \\xa0\\n13:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\xa0(temp_distance \\xa0<\\xa0min)\\xa0\\n14:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 BEGIN\\xa0\\n15:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 min\\xa0=\\xa0temp_distance; \\xa0\\n16:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 flag\\xa0=\\xa0j;\\xa0\\n17:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 END\\xa0\\n18:\\xa0\\xa0 END\\xa0\\n19:\\xa0\\xa0 if\\xa0((abs(Ver_theta(i) ‐Ver_theta(flag)) \\xa0>\\xa090)\\xa0AND\\xa0(min\\xa0<\\xa0dt))\\xa0\\xa0\\n20:\\xa0\\xa0 BEGIN\\xa0\\xa0\\n21:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 Temp_line.pushback(Ver(i)); \\xa0\\n22:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 Temp_line.pushback(Ver(flag)); \\xa0\\n23:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 L1\\xa0=\\xa0findLinefromVer(Ver(i)); \\xa0\\n24:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 L2\\xa0=\\xa0findLinefromVer(Ver(flag)); \\xa0\\n25:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 JudgeConnectOrder(L1,L2,&Line1,Temp_line,&Line2); \\xa0\\n26:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 Line1\\xa0=\\xa0Line1.combine(Temp_line); \\xa0\\n27:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 Line1\\xa0=\\xa0Line1.combine(Line2); \\xa0\\nFigure 10. Angle rule between vectors: ( a) the two vectors are on the same side of the horizontal line;\\n(b) the two vectors are on opposite sides of the horizontal line.Appl. Sci. 2022 ,12, 4705 11 of 21\\nThe pseudocode of the road connection algorithm in Algorithm 2:\\nAlgorithm 2: Road Connection\\nInput: Lines Segments after Tracking Algorithm\\nOutput: Lines Roads after Connection Algorithm\\n1: function LinkLines(&Lines)\\n2: BEGIN\\n3: Ver = FindVertex(Lines);\\n4: Ver_theta = FindVertexAzimuth(Lines);\\n5: for i  0 to Ver.size()\\n6: BEGIN\\n7: min = 9999.0;\\n8: if (isinConnectedPt(i, ConnectedPt)) continue;\\n9: for j  (i + 1) to Ver.size()\\n10: BEGIN\\n11: if (isinSameLine(Ver(i), Ver(j))) continue;\\n12: temp_distance = cal_Distance(Ver(i),Ver(j));\\n13: if (temp_distance < min)\\n14: BEGIN\\n15: min = temp_distance;\\n16: ﬂag = j;\\n17: END\\n18: END\\n19: if ((abs(Ver_theta(i)-Ver_theta(ﬂag)) > 90) AND (min < dt))\\n20: BEGIN\\n21: Temp_line.pushback(Ver(i));\\n22: Temp_line.pushback(Ver(ﬂag));\\n23: L1 = ﬁndLinefromVer(Ver(i));\\n24: L2 = ﬁndLinefromVer(Ver(ﬂag));\\n25: JudgeConnectOrder(L1,L2,&Line1,Temp_line,&Line2);\\n26: Line1 = Line1.combine(Temp_line);\\n27: Line1 = Line1.combine(Line2);\\n28: ChangeLine(L1,Line1, &Lines);\\n29: deleteLine(L2,&Lines);\\n30: ConnectedPt.pushback(ﬂag);\\n31: END\\n32: END\\n33: END\\nFindVertex (Lines) is used to ﬁnd the endpoint of the segment. FindVertexAzimuth\\n(Lines) is used to calculate the angle between the end vector and the horizontal-right\\ndirection. ConnectedPt stores the number of connected vertices. The isinConnectedPt(i,\\nconnectedpt) function is used to determine whether the currently numbered vertex i has\\nbeen connected; isinSameLine(Ver(i), Ver(j)) is used to judge whether the two vertices are\\non the same road segment; cal_Distance(Ver(i), Ver(j)) is used to calculate the Euclidean\\ndistance between two vertices; ﬁndLinefromVer(Ver(i)) is used to ﬁnd the corresponding\\nroad segment through vertex Ver(i); and JudgeConnectOrder(L1, L2, & Line1, Temp_line, &\\nLine2) is used to determine the connection order. The combine function connects segments.\\nChangeLine(L1, Line1, & Lines) is used to reset lines and deleteLine(L2, & Lines) is used to\\ndelete redundant segments.\\nFigure 11 shows the connection results of single roads, in which (a) and (c) depict two\\nroads to be connected. The middle part of each road is divided into two sections due to\\ntree occlusion, and this cannot be repaired by the morphological closing algorithm; (b) and\\n(d) show the road connection results.Appl. Sci. 2022 ,12, 4705 12 of 21\\nAppl.\\xa0Sci.\\xa02022,\\xa012,\\xa0x\\xa0FOR\\xa0PEER\\xa0REVIEW \\xa0 12\\xa0of\\xa022\\xa0\\n\\xa0\\n28:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ChangeLine(L1,Line1, \\xa0&Lines);\\xa0\\n29:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 deleteLine(L2,&Lines); \\xa0\\n30:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ConnectedPt.pushback(flag); \\xa0\\n31:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 END\\xa0\\n32:\\xa0\\xa0 END\\xa0\\n33:\\xa0\\xa0 END\\xa0\\nFindVertex \\xa0(Lines)\\xa0is\\xa0used\\xa0to\\xa0find\\xa0the\\xa0endpoint \\xa0of\\xa0the\\xa0segment. \\xa0FindVertexAzimuth \\xa0\\n(Lines)\\xa0is\\xa0used\\xa0to\\xa0calculate \\xa0the\\xa0angle\\xa0between\\xa0the\\xa0end\\xa0vector\\xa0and\\xa0the\\xa0horizontal ‐right\\xa0di‐\\nrection.\\xa0ConnectedPt \\xa0stores\\xa0the\\xa0number\\xa0of\\xa0connected \\xa0vertices.\\xa0The\\xa0isinConnectedPt(i, \\xa0con‐\\nnectedpt) \\xa0function\\xa0is\\xa0used\\xa0to\\xa0determine \\xa0whether\\xa0the\\xa0currently \\xa0numbered \\xa0vertex\\xa0i\\xa0has\\xa0been\\xa0\\nconnected; \\xa0isinSameLine(Ver(i), \\xa0Ver(j))\\xa0is\\xa0used\\xa0to\\xa0judge\\xa0whether\\xa0the\\xa0two\\xa0vertices\\xa0are\\xa0on\\xa0\\nthe\\xa0same\\xa0road\\xa0segment; \\xa0cal_Distance(Ver(i), \\xa0Ver(j))\\xa0is\\xa0used\\xa0to\\xa0calculate \\xa0the\\xa0Euclidean \\xa0dis‐\\ntance\\xa0between\\xa0two\\xa0vertices;\\xa0findLinefromVer(Ver(i)) \\xa0is\\xa0used\\xa0to\\xa0find\\xa0the\\xa0corresponding \\xa0\\nroad\\xa0segment\\xa0through\\xa0vertex\\xa0Ver(i);\\xa0and\\xa0JudgeConnectOrder(L1, \\xa0L2,\\xa0&\\xa0Line1,\\xa0Temp_line, \\xa0\\n&\\xa0Line2)\\xa0is\\xa0used\\xa0to\\xa0determine \\xa0the\\xa0connection \\xa0order.\\xa0The\\xa0combine \\xa0function\\xa0connects \\xa0seg‐\\nments.\\xa0ChangeLine(L1, \\xa0Line1,\\xa0&\\xa0Lines)\\xa0is\\xa0used\\xa0to\\xa0reset\\xa0lines\\xa0and\\xa0deleteLine(L2, \\xa0&\\xa0Lines)\\xa0\\nis\\xa0used\\xa0to\\xa0delete\\xa0redundant \\xa0segments. \\xa0\\nFigure\\xa011\\xa0shows\\xa0the\\xa0connection \\xa0results\\xa0of\\xa0single\\xa0roads,\\xa0in\\xa0which\\xa0(a)\\xa0and\\xa0(c)\\xa0depict\\xa0\\ntwo\\xa0roads\\xa0to\\xa0be\\xa0connected. \\xa0The\\xa0middle\\xa0part\\xa0of\\xa0each\\xa0road\\xa0is\\xa0divided\\xa0into\\xa0two\\xa0sections\\xa0due\\xa0\\nto\\xa0tree\\xa0occlusion, \\xa0and\\xa0this\\xa0cannot\\xa0be\\xa0repaired\\xa0by\\xa0the\\xa0morphological \\xa0closing\\xa0algorithm; \\xa0(b)\\xa0\\nand\\xa0(d)\\xa0show\\xa0the\\xa0road\\xa0connection \\xa0results.\\xa0\\n\\xa0\\xa0 \\xa0 \\xa0\\n(a)\\xa0 (b)\\xa0 (c)\\xa0 (d)\\xa0\\nFigure\\xa011.\\xa0The\\xa0road\\xa0connection \\xa0results:\\xa0(a,c)\\xa0depict\\xa0the\\xa0roads\\xa0to\\xa0be\\xa0connected; \\xa0(b,d)\\xa0are\\xa0the\\xa0respective \\xa0\\nroad\\xa0connection \\xa0results.\\xa0\\n2.2.2.\\xa0\\xa0Road\\xa0Network \\xa0Generation \\xa0\\nThe\\xa0road\\xa0network\\xa0is\\xa0acquired \\xa0through\\xa0road\\xa0intersection \\xa0connections \\xa0and\\xa0buffer\\xa0gen‐\\neration.\\xa0Road\\xa0intersections \\xa0have\\xa0three\\xa0main\\xa0shape\\xa0types:\\xa0‘T’,\\xa0‘Y’,\\xa0and\\xa0‘+’.\\xa0‘T’‐\\xa0and\\xa0‘Y’‐\\nshaped\\xa0intersections \\xa0are\\xa0similar,\\xa0as\\xa0shown\\xa0in\\xa0Figure\\xa012a,\\xa0in\\xa0which\\xa0directions \\xa01\\xa0and\\xa02\\xa0with\\xa0\\nlarger\\xa0included \\xa0angles\\xa0can\\xa0be\\xa0regarded \\xa0as\\xa0the\\xa0same\\xa0single\\xa0road,\\xa0recorded \\xa0as\\xa0road\\xa0A.\\xa0Di‐\\nrection\\xa03\\xa0is\\xa0regarded \\xa0as\\xa0a\\xa0single\\xa0road\\xa0and\\xa0is\\xa0recorded \\xa0as\\xa0road\\xa0B.\\xa0At\\xa0this\\xa0time,\\xa0the\\xa0extension \\xa0\\nline\\xa0(dashed\\xa0line)\\xa0of\\xa0road\\xa0B\\xa0is\\xa0constructed \\xa0to\\xa0a\\xa0certain\\xa0length,\\xa0which\\xa0is\\xa0equal\\xa0to\\xa0d୲,\\xa0to\\xa0de‐\\ntermine\\xa0whether\\xa0there\\xa0is\\xa0an\\xa0intersection \\xa0with\\xa0road\\xa0A.\\xa0If\\xa0an\\xa0intersection \\xa0exists\\xa0(blue\\xa0point),\\xa0\\nit\\xa0is\\xa0connected \\xa0to\\xa0road\\xa0B\\xa0(blue\\xa0line).\\xa0\\nThere\\xa0are\\xa0two\\xa0situations \\xa0for\\xa0an\\xa0intersection \\xa0with\\xa0a\\xa0‘+’\\xa0shape.\\xa0One\\xa0is\\xa0shown\\xa0in\\xa0Figure\\xa0\\n12b.\\xa0The\\xa0four\\xa0directions \\xa0intersect\\xa0at\\xa0the\\xa0same\\xa0point,\\xa0and\\xa0the\\xa0angles\\xa0between\\xa0directions \\xa01\\xa0\\nand\\xa03\\xa0and\\xa0between\\xa02\\xa0and\\xa04\\xa0are\\xa0both\\xa0close\\xa0to\\xa0180°;\\xa0this\\xa0can\\xa0be\\xa0taken\\xa0to\\xa0indicate\\xa0two\\xa0single\\xa0\\nroads\\xa0that\\xa0should\\xa0be\\xa0extracted \\xa0separately, \\xa0and\\xa0the\\xa0roads\\xa0A\\xa0and\\xa0B\\xa0obtained \\xa0have\\xa0a\\xa0unique\\xa0\\nintersection \\xa0(blue\\xa0point).\\xa0No\\xa0additional \\xa0processing \\xa0is\\xa0required. \\xa0The\\xa0other\\xa0situation \\xa0is\\xa0rela‐\\ntively\\xa0complicated, \\xa0as\\xa0shown\\xa0in\\xa0Figure\\xa012c.\\xa0Directions \\xa01\\xa0and\\xa03\\xa0can\\xa0be\\xa0regarded \\xa0as\\xa0the\\xa0same\\xa0\\nroad\\xa0for\\xa0extraction \\xa0to\\xa0obtain\\xa0road\\xa0A,\\xa0while\\xa0directions \\xa02\\xa0and\\xa04\\xa0can\\xa0only\\xa0be\\xa0extracted \\xa0as\\xa0\\nFigure 11. The road connection results: ( a,c) depict the roads to be connected; ( b,d) are the respective\\nroad connection results.\\n2.2.2. Road Network Generation\\nThe road network is acquired through road intersection connections and buffer gener-\\nation. Road intersections have three main shape types: ‘T’, ‘Y’, and ‘+’. ‘T’- and ‘Y’-shaped\\nintersections are similar, as shown in Figure 12a, in which directions 1 and 2 with larger\\nincluded angles can be regarded as the same single road, recorded as road A. Direction 3 is\\nregarded as a single road and is recorded as road B. At this time, the extension line (dashed\\nline) of road B is constructed to a certain length, which is equal to dt, to determine whether\\nthere is an intersection with road A. If an intersection exists (blue point), it is connected to\\nroad B (blue line).\\nAppl.\\xa0Sci.\\xa02022,\\xa012,\\xa0x\\xa0FOR\\xa0PEER\\xa0REVIEW \\xa0 13\\xa0of\\xa022\\xa0\\n\\xa0\\nindividual \\xa0roads\\xa0that\\xa0generate \\xa0roads\\xa0B\\xa0and\\xa0C.\\xa0In\\xa0this\\xa0case,\\xa0the\\xa0intersection \\xa0can\\xa0be\\xa0regarded \\xa0\\nas\\xa0the\\xa0superposition \\xa0of\\xa0two\\xa0‘T’‐shaped\\xa0roads\\xa0formed\\xa0by\\xa0road\\xa0A\\xa0with\\xa0either\\xa0B\\xa0or\\xa0C.\\xa0Then,\\xa0\\nprocessing \\xa0can\\xa0be\\xa0performed \\xa0according \\xa0to\\xa0the\\xa0‘T’‐shaped\\xa0intersection \\xa0connection \\xa0strategy. \\xa0\\nFigure\\xa013\\xa0shows\\xa0the\\xa0connection \\xa0results\\xa0of\\xa0the\\xa0intersection \\xa0connection \\xa0algorithm. \\xa0It\\xa0\\ncan\\xa0be\\xa0seen\\xa0that\\xa0the\\xa0strategy\\xa0described \\xa0above\\xa0has\\xa0a\\xa0good\\xa0connection \\xa0effect.\\xa0\\n\\xa0\\xa0 \\xa0\\n(a)\\xa0 (b)\\xa0 (c)\\xa0\\nFigure\\xa012.\\xa0Description \\xa0of\\xa0road\\xa0intersections: \\xa0(a)\\xa0‘T’‐\\xa0or\\xa0‘Y’‐shaped\\xa0intersection; \\xa0(b)\\xa0simple\\xa0situation \\xa0\\nof\\xa0‘+’‐shaped\\xa0intersection; \\xa0(c)\\xa0multiple \\xa0situations \\xa0of\\xa0‘+’‐shaped\\xa0intersections. \\xa0\\n\\xa0\\xa0 \\xa0\\n(a)\\xa0 (c)\\xa0 (e)\\xa0\\n\\xa0\\xa0 \\xa0\\n(b)\\xa0 (d)\\xa0 (f)\\xa0\\nFigure\\xa013.\\xa0Intersection \\xa0connection \\xa0results:\\xa0(a,c,e)\\xa0depict\\xa0the\\xa0road\\xa0intersections \\xa0before\\xa0the\\xa0connection \\xa0\\nis\\xa0performed \\xa0and\\xa0(b,d,f)\\xa0are\\xa0the\\xa0results\\xa0of\\xa0the\\xa0intersection \\xa0connection \\xa0algorithm. \\xa0\\nThe\\xa0intersection \\xa0connection \\xa0algorithm \\xa0is\\xa0shown\\xa0in\\xa0Algorithm \\xa03:\\xa0\\nAlgorithm \\xa03:\\xa0Intersection \\xa0Connection \\xa0\\nInput:\\xa0\\xa0Lines\\xa0Roads\\xa0\\nOutput:\\xa0addLines \\xa0Newly\\xa0added\\xa0connection \\xa0roads\\xa0\\n1:\\xa0\\xa0 function \\xa0addJunctionLines(Lines, \\xa0addLines) \\xa0\\n2:\\xa0\\xa0 BEGIN\\xa0\\n3:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 Ver\\xa0=\\xa0FindVertex(Lines); \\xa0\\n4:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 Ver_theta \\xa0=\\xa0FindVertexAzimuth(Lines); \\xa0\\n5:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for\\xa0i\\xa0←\\xa00\\xa0to\\xa0Ver.size() \\xa0\\nFigure 12. Description of road intersections: ( a) ‘T’- or ‘Y’-shaped intersection; ( b) simple situation of\\n‘+’-shaped intersection; ( c) multiple situations of ‘+’-shaped intersections.\\nThere are two situations for an intersection with a ‘+’ shape. One is shown in Figure 12b.\\nThe four directions intersect at the same point, and the angles between directions 1 and\\n3 and between 2 and 4 are both close to 180\\x0e; this can be taken to indicate two single\\nroads that should be extracted separately, and the roads A and B obtained have a unique\\nintersection (blue point). No additional processing is required. The other situation is\\nrelatively complicated, as shown in Figure 12c. Directions 1 and 3 can be regarded as the\\nsame road for extraction to obtain road A, while directions 2 and 4 can only be extracted as\\nindividual roads that generate roads B and C. In this case, the intersection can be regarded\\nas the superposition of two ‘T’-shaped roads formed by road A with either B or C. Then,\\nprocessing can be performed according to the ‘T’-shaped intersection connection strategy.\\nFigure 13 shows the connection results of the intersection connection algorithm. It can\\nbe seen that the strategy described above has a good connection effect.Appl. Sci. 2022 ,12, 4705 13 of 21\\nAppl.\\xa0Sci.\\xa02022,\\xa012,\\xa0x\\xa0FOR\\xa0PEER\\xa0REVIEW \\xa0 13\\xa0of\\xa022\\xa0\\n\\xa0\\nindividual \\xa0roads\\xa0that\\xa0generate \\xa0roads\\xa0B\\xa0and\\xa0C.\\xa0In\\xa0this\\xa0case,\\xa0the\\xa0intersection \\xa0can\\xa0be\\xa0regarded \\xa0\\nas\\xa0the\\xa0superposition \\xa0of\\xa0two\\xa0‘T’‐shaped\\xa0roads\\xa0formed\\xa0by\\xa0road\\xa0A\\xa0with\\xa0either\\xa0B\\xa0or\\xa0C.\\xa0Then,\\xa0\\nprocessing \\xa0can\\xa0be\\xa0performed \\xa0according \\xa0to\\xa0the\\xa0‘T’‐shaped\\xa0intersection \\xa0connection \\xa0strategy. \\xa0\\nFigure\\xa013\\xa0shows\\xa0the\\xa0connection \\xa0results\\xa0of\\xa0the\\xa0intersection \\xa0connection \\xa0algorithm. \\xa0It\\xa0\\ncan\\xa0be\\xa0seen\\xa0that\\xa0the\\xa0strategy\\xa0described \\xa0above\\xa0has\\xa0a\\xa0good\\xa0connection \\xa0effect.\\xa0\\n\\xa0\\xa0 \\xa0\\n(a)\\xa0 (b)\\xa0 (c)\\xa0\\nFigure\\xa012.\\xa0Description \\xa0of\\xa0road\\xa0intersections: \\xa0(a)\\xa0‘T’‐\\xa0or\\xa0‘Y’‐shaped\\xa0intersection; \\xa0(b)\\xa0simple\\xa0situation \\xa0\\nof\\xa0‘+’‐shaped\\xa0intersection; \\xa0(c)\\xa0multiple \\xa0situations \\xa0of\\xa0‘+’‐shaped\\xa0intersections. \\xa0\\n\\xa0\\xa0 \\xa0\\n(a)\\xa0 (c)\\xa0 (e)\\xa0\\n\\xa0\\xa0 \\xa0\\n(b)\\xa0 (d)\\xa0 (f)\\xa0\\nFigure\\xa013.\\xa0Intersection \\xa0connection \\xa0results:\\xa0(a,c,e)\\xa0depict\\xa0the\\xa0road\\xa0intersections \\xa0before\\xa0the\\xa0connection \\xa0\\nis\\xa0performed \\xa0and\\xa0(b,d,f)\\xa0are\\xa0the\\xa0results\\xa0of\\xa0the\\xa0intersection \\xa0connection \\xa0algorithm. \\xa0\\nThe\\xa0intersection \\xa0connection \\xa0algorithm \\xa0is\\xa0shown\\xa0in\\xa0Algorithm \\xa03:\\xa0\\nAlgorithm \\xa03:\\xa0Intersection \\xa0Connection \\xa0\\nInput:\\xa0\\xa0Lines\\xa0Roads\\xa0\\nOutput:\\xa0addLines \\xa0Newly\\xa0added\\xa0connection \\xa0roads\\xa0\\n1:\\xa0\\xa0 function \\xa0addJunctionLines(Lines, \\xa0addLines) \\xa0\\n2:\\xa0\\xa0 BEGIN\\xa0\\n3:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 Ver\\xa0=\\xa0FindVertex(Lines); \\xa0\\n4:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 Ver_theta \\xa0=\\xa0FindVertexAzimuth(Lines); \\xa0\\n5:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for\\xa0i\\xa0←\\xa00\\xa0to\\xa0Ver.size() \\xa0\\nFigure 13. Intersection connection results: ( a,c,e) depict the road intersections before the connection\\nis performed and ( b,d,f) are the results of the intersection connection algorithm.\\nThe intersection connection algorithm is shown in Algorithm 3:\\nAlgorithm 3: Intersection Connection\\nInput: Lines Roads\\nOutput: addLines Newly added connection roads\\n1: function addJunctionLines(Lines, addLines)\\n2: BEGIN\\n3: Ver = FindVertex(Lines);\\n4: Ver_theta = FindVertexAzimuth(Lines);\\n5: for i  0 to Ver.size()\\n6: BEGIN\\n7: extendline.push_back(Ver(i));\\n8: extendpoint = cal_coordinate(Ver(i), Ver_theta(i), dt);\\n9: extendline.push_back(extendpoint);\\n10: for j  0 to Lines.size()\\n11: BEGIN\\n12: if (intersects(Lines(j), extendline))\\n13: BEGIN\\n14: intersect_geo = intersection(Lines(j), extendline);\\n15: intersect = intersect_geo- > asPoint();\\n16: if (Ver(i) 6=intersect)\\n17: BEGIN\\n18: temp_polyline.push_back(Ver(i));\\n19: temp_polyline.push_back(intersect);\\n20: addLines.pushback(temp_polyline);\\n21: END\\n22: END\\n23: END\\n24: END\\n25: ENDAppl. Sci. 2022 ,12, 4705 14 of 21\\nThe function cal_coordinate(Ver(i), Ver_theta(i), dt) is used to calculate the vertex of\\nthe extension line of limited length; intersects(Lines (j), extendline) is used to determine\\nwhether there is an intersection; and intersection(Lines (j), extendline) is used to calculate\\nthe intersection.\\nAfter multiple roads are extracted separately and all road endpoints are judged and\\nconnected, a buffer zone with a radius that is one-half the width of the road is generated to\\nobtain the road network (see Figure 14).\\nAppl.\\xa0Sci.\\xa02022,\\xa012,\\xa0x\\xa0FOR\\xa0PEER\\xa0REVIEW \\xa0 14\\xa0of\\xa022\\xa0\\n\\xa0\\n6:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 BEGIN\\xa0\\xa0\\n7:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 extendline.push_back(Ver(i)); \\xa0\\n8:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 extendpoint \\xa0=\\xa0cal_coordinate(Ver(i), \\xa0Ver_theta(i), \\xa0dt);\\xa0\\n9:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 extendline.push_back(extendpoint); \\xa0\\n10:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for\\xa0j\\xa0←\\xa00\\xa0to\\xa0Lines.size() \\xa0\\n11:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 BEGIN\\xa0\\n12:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\xa0(intersects(Lines(j), \\xa0extendline)) \\xa0\\n13:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 BEGIN\\xa0\\n14:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 intersect_geo \\xa0=\\xa0intersection(Lines(j), \\xa0extendline); \\xa0\\n15:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 intersect\\xa0=\\xa0intersect_geo ‐\\xa0>\\xa0asPoint(); \\xa0\\n16:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\xa0(Ver(i)≠intersect) \\xa0\\n17:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 BEGIN\\xa0\\n18:\\xa0\\xa0 temp_polyline.push_back(Ver(i)); \\xa0\\xa0\\n19:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 temp_polyline.push_back(intersect); \\xa0\\n20:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 addLines.pushback(temp_polyline); \\xa0\\n21:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 END\\xa0\\n22:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 END\\xa0\\n23:\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 END\\xa0\\n24:\\xa0\\xa0 END\\xa0\\n25:\\xa0\\xa0 END\\xa0\\nThe\\xa0function\\xa0cal_coordinate(Ver(i), \\xa0Ver_theta(i), \\xa0dt)\\xa0is\\xa0used\\xa0to\\xa0calculate \\xa0the\\xa0vertex\\xa0of\\xa0\\nthe\\xa0extension \\xa0line\\xa0of\\xa0limited\\xa0length;\\xa0intersects(Lines \\xa0(j),\\xa0extendline) \\xa0is\\xa0used\\xa0to\\xa0determine \\xa0\\nwhether\\xa0there\\xa0is\\xa0an\\xa0intersection; \\xa0and\\xa0intersection(Lines \\xa0(j),\\xa0extendline) \\xa0is\\xa0used\\xa0to\\xa0calculate \\xa0\\nthe\\xa0intersection. \\xa0\\nAfter\\xa0multiple \\xa0roads\\xa0are\\xa0extracted \\xa0separately \\xa0and\\xa0all\\xa0road\\xa0endpoints \\xa0are\\xa0judged\\xa0and\\xa0\\nconnected, \\xa0a\\xa0buffer\\xa0zone\\xa0with\\xa0a\\xa0radius\\xa0that\\xa0is\\xa0one‐half\\xa0the\\xa0width\\xa0of\\xa0the\\xa0road\\xa0is\\xa0generated \\xa0\\nto\\xa0obtain\\xa0the\\xa0road\\xa0network\\xa0(see\\xa0Figure\\xa014).\\xa0\\n\\xa0\\nFigure\\xa014.\\xa0A\\xa0road\\xa0network\\xa0obtained \\xa0through\\xa0a\\xa0buffer\\xa0zone.\\xa0\\n\\xa0 \\xa0\\nFigure 14. A road network obtained through a buffer zone.\\n2.2.3. Evaluation of the Extraction Results\\nIn this method, the precision, accuracy, recall, and intersection over union (IoU) [ 50,51]\\nare used to evaluate extraction performance, see Equations (10)–(14). In the formulas, TP ,\\nFP , FN, and TN are the number of true positives, false positives, false negatives, and true\\nnegatives for the road predictions.\\nPrecision =TP\\nTP+FP, (10)\\nAccuracy =TP+TN\\nTP+TN+FP+FN, (11)\\nRecall =TP\\nTP+FN, (12)\\nThe IoU is the ratio of the intersection and the union of two bounding boxes. If A is\\nthe resulting extracted road and B is the corresponding ground truth, then the IoU is:\\nIoU=A\\\\B\\nA[B, (13)\\nTP , FP , and FN are used to re-state the formula as follows:\\nIf the extraction result is identical to the ground truth, then IoU is equal to 1.\\nIoU=TP\\nTP+FN+FP, (14)\\n3. Results\\n3.1. Parameter Settings\\nIn this study, only three parameters need to be set in the workﬂow of road network\\nextraction: the image resolution, regional growth threshold, and simplifying threshold.\\nThe image resolution can be obtained directly from the data source. Since the difference\\nin gray value between urban buildings and roads is small, the regional growth threshold\\nneeds to be determined by the user according to the number of buildings shown in theAppl. Sci. 2022 ,12, 4705 15 of 21\\nimage. After a large number of experiments, the regional growth threshold was set to\\n40 in suburbs with fewer buildings, and 20 in cities with more buildings. As shown in\\nEquation (8), the simplifying parameter khas a signiﬁcant impact on the simpliﬁed result.\\nBased on experiments, the simplifying parameter k is set to 3. Figure 15 shows the results\\nof different simplifying thresholds for an image with the same type as Data 2; (a) is the\\ntracking result, and (b–d) are the simpliﬁed results when the simplifying parameter k is 1,\\n3, and 5, respectively. When k = 1, redundant points remain in the circle. When k = 5, some\\nkey points are missing, and the centerline in the circle deviates from the road. When k = 3,\\nthe result ﬁts the road centerline well. Hence, the simplifying parameter k is set to 3 here.\\nAppl.\\xa0Sci.\\xa02022,\\xa012,\\xa0x\\xa0FOR\\xa0PEER\\xa0REVIEW \\xa0 15\\xa0of\\xa022\\xa0\\n\\xa0\\n2.2.3.\\xa0Evaluation \\xa0of\\xa0the\\xa0Extraction \\xa0Results\\xa0\\nIn\\xa0this\\xa0method,\\xa0the\\xa0precision, \\xa0accuracy, \\xa0recall,\\xa0and\\xa0intersection \\xa0over\\xa0union\\xa0(IoU)\\xa0\\n[50,51]\\xa0are\\xa0used\\xa0to\\xa0evaluate\\xa0extraction \\xa0performance, \\xa0see\\xa0Equations \\xa0(10)–(14). \\xa0In\\xa0the\\xa0formu‐\\nlas,\\xa0TP,\\xa0FP,\\xa0FN,\\xa0and\\xa0TN\\xa0are\\xa0the\\xa0number\\xa0of\\xa0true\\xa0positives, \\xa0false\\xa0positives, \\xa0false\\xa0negatives, \\xa0\\nand\\xa0true\\xa0negatives \\xa0for\\xa0the\\xa0road\\xa0predictions. \\xa0\\nPrecision ൌ\\u0b58\\u0b54\\n\\u0b58\\u0b54ା\\u0b4a\\u0b54\\xa0,\\xa0 (10)\\xa0\\nAccurac yൌ\\u0b58\\u0b54ା\\u0b58\\u0b52\\n\\u0b58\\u0b54ା\\u0b58\\u0b52ା\\u0b4a\\u0b54ା\\u0b4a\\u0b52\\xa0,\\xa0 (11)\\xa0\\nRecall ൌ\\u0b58\\u0b54\\n\\u0b58\\u0b54ା\\u0b4a\\u0b52\\xa0,\\xa0 (12)\\xa0\\nThe\\xa0IoU\\xa0is\\xa0the\\xa0ratio\\xa0of\\xa0the\\xa0intersection \\xa0and\\xa0the\\xa0union\\xa0of\\xa0two\\xa0bounding \\xa0boxes.\\xa0If\\xa0A\\xa0is\\xa0\\nthe\\xa0resulting \\xa0extracted \\xa0road\\xa0and\\xa0B\\xa0is\\xa0the\\xa0corresponding \\xa0ground\\xa0truth,\\xa0then\\xa0the\\xa0IoU\\xa0is:\\xa0\\nIoU ൌ\\u0b45∩\\u0b46\\n\\u0b45∪\\u0b46\\xa0,\\xa0 (13)\\xa0\\nTP,\\xa0FP,\\xa0and\\xa0FN\\xa0are\\xa0used\\xa0to\\xa0re‐state\\xa0the\\xa0formula\\xa0as\\xa0follows:\\xa0\\nIf\\xa0the\\xa0extraction \\xa0result\\xa0is\\xa0identical \\xa0to\\xa0the\\xa0ground\\xa0truth,\\xa0then\\xa0IoU\\xa0is\\xa0equal\\xa0to\\xa01.\\xa0\\nIoU ൌ\\u0b58\\u0b54\\n\\u0b58\\u0b54ା\\u0b4a\\u0b52ା\\u0b4a\\u0b54\\xa0,\\xa0 (14)\\xa0\\n3.\\xa0Results\\xa0\\n3.1.\\xa0Parameter \\xa0Settings\\xa0\\nIn\\xa0this\\xa0study,\\xa0only\\xa0three\\xa0parameters \\xa0need\\xa0to\\xa0be\\xa0set\\xa0in\\xa0the\\xa0workflow \\xa0of\\xa0road\\xa0network\\xa0\\nextraction: \\xa0the\\xa0image\\xa0resolution, \\xa0regional\\xa0growth\\xa0threshold, \\xa0and\\xa0simplifying \\xa0threshold. \\xa0\\nThe\\xa0image\\xa0resolution \\xa0can\\xa0be\\xa0obtained \\xa0directly\\xa0from\\xa0the\\xa0data\\xa0source.\\xa0Since\\xa0the\\xa0difference \\xa0\\nin\\xa0gray\\xa0value\\xa0between\\xa0urban\\xa0buildings \\xa0and\\xa0roads\\xa0is\\xa0small,\\xa0the\\xa0regional\\xa0growth\\xa0threshold \\xa0\\nneeds\\xa0to\\xa0be\\xa0determined \\xa0by\\xa0the\\xa0user\\xa0according \\xa0to\\xa0the\\xa0number\\xa0of\\xa0buildings \\xa0shown\\xa0in\\xa0the\\xa0\\nimage.\\xa0After\\xa0a\\xa0large\\xa0number\\xa0of\\xa0experiments, \\xa0the\\xa0regional\\xa0growth\\xa0threshold \\xa0was\\xa0set\\xa0to\\xa040\\xa0\\nin\\xa0suburbs\\xa0with\\xa0fewer\\xa0buildings, \\xa0and\\xa020\\xa0in\\xa0cities\\xa0with\\xa0more\\xa0buildings. \\xa0As\\xa0shown\\xa0in\\xa0Equa‐\\ntion\\xa0(8),\\xa0the\\xa0simplifying \\xa0parameter \\xa0k\\xa0has\\xa0a\\xa0significant \\xa0impact\\xa0on\\xa0the\\xa0simplified \\xa0result.\\xa0\\nBased\\xa0on\\xa0experiments, \\xa0the\\xa0simplifying \\xa0parameter \\xa0k\\xa0is\\xa0set\\xa0to\\xa03.\\xa0Figure\\xa015\\xa0shows\\xa0the\\xa0results\\xa0\\nof\\xa0different \\xa0simplifying \\xa0thresholds \\xa0for\\xa0an\\xa0image\\xa0with\\xa0the\\xa0same\\xa0type\\xa0as\\xa0Data\\xa02;\\xa0(a)\\xa0is\\xa0the\\xa0\\ntracking\\xa0result,\\xa0and\\xa0(b–d)\\xa0are\\xa0the\\xa0simplified \\xa0results\\xa0when\\xa0the\\xa0simplifying \\xa0parameter \\xa0k\\xa0is\\xa0\\n1,\\xa03,\\xa0and\\xa05,\\xa0respectively. \\xa0When\\xa0k\\xa0=\\xa01,\\xa0redundant \\xa0points\\xa0remain\\xa0in\\xa0the\\xa0circle.\\xa0When\\xa0k\\xa0=\\xa05,\\xa0\\nsome\\xa0key\\xa0points\\xa0are\\xa0missing,\\xa0and\\xa0the\\xa0centerline \\xa0in\\xa0the\\xa0circle\\xa0deviates\\xa0from\\xa0the\\xa0road.\\xa0When\\xa0\\nk\\xa0=\\xa03,\\xa0the\\xa0result\\xa0fits\\xa0the\\xa0road\\xa0centerline \\xa0well.\\xa0Hence,\\xa0the\\xa0simplifying \\xa0parameter \\xa0k\\xa0is\\xa0set\\xa0to\\xa03\\xa0\\nhere.\\xa0\\n\\xa0\\xa0\\xa0\\xa0\\n(a)\\xa0 (b)\\xa0 (c)\\xa0 (d)\\xa0\\nFigure\\xa015.\\xa0Results\\xa0of\\xa0different \\xa0simplifying \\xa0thresholds: \\xa0(a)\\xa0tracking\\xa0result;\\xa0(b)\\xa0result\\xa0when\\xa0k\\xa0=\\xa01;\\xa0(c)\\xa0\\nresult\\xa0when\\xa0k\\xa0=\\xa03;\\xa0(d)\\xa0result\\xa0when\\xa0k\\xa0=\\xa05.\\xa0\\nFigure 15. Results of different simplifying thresholds: ( a) tracking result; ( b) result when k = 1;\\n(c) result when k = 3; ( d) result when k = 5.\\n3.2. Road Extraction Results for the Four Datasets\\nFigure 16 shows a comparison between the road extraction result and the ground truth,\\nwhere (a,e) correspond to Data 1, (b,f) correspond to Data 2, (c,g) correspond to Data 3, and\\n(d,h) correspond to Data 4; (a–d) are extracted by the developed method to generate road\\nnetworks, and (e–h) are ground truths that are manually labeled. The evaluation results\\nare shown in Table 1. These extraction results have high accuracy and precision of Data\\n1–3, showing that the method applies well in extracting roads from images at different\\nresolutions. Among the four data, Data 1 was located in a rural area, with the simplest road\\nconditions and the highest resolution. Therefore, the extraction result is very similar to\\nthe ground truth (Figure 16a,e). Data 2 is more complex than Data 1, but each road is also\\nextracted relatively completely (Figure 16b,f). Because of many details in Data 2, the recall\\nrate and IoU in the road extraction results are relatively low (Table 1). Data 3 is located in\\nthe city, and the road situation is the most complex. The results of Data 3 have the highest\\nprecision but the lowest accuracy (Table 1), meaning that most parts of roads are identiﬁed\\naccurately but the differences between roads and surroundings are not distinguished well.\\nData 4 is located in a rural residential area with multiple roads of varying lengths and\\nwidths. The proposed method can extract the roads in the image very well by extracting the\\nroads by grading, that is, ﬁrstly extracting the wide roads, and then extracting the narrow\\nroads (Figure 16e,h). However, the width of the road at the same level in the image is also\\ndifferent, which makes the proposed method obtain limited extraction precision and IoU\\n(Table 1). Moreover, all road intersections in the four data are correctly connected.Appl. Sci. 2022 ,12, 4705 16 of 21\\nAppl.\\xa0Sci.\\xa02022,\\xa012,\\xa0x\\xa0FOR\\xa0PEER\\xa0REVIEW \\xa0 16\\xa0of\\xa022\\xa0\\n\\xa0\\n3.2.\\xa0Road\\xa0Extraction \\xa0Results\\xa0for\\xa0the\\xa0Four\\xa0Datasets\\xa0\\nFigure\\xa016\\xa0shows\\xa0a\\xa0comparison \\xa0between\\xa0the\\xa0road\\xa0extraction \\xa0result\\xa0and\\xa0the\\xa0ground\\xa0\\ntruth,\\xa0where\\xa0(a,e)\\xa0correspond \\xa0to\\xa0Data\\xa01,\\xa0(b,f)\\xa0correspond \\xa0to\\xa0Data\\xa02,\\xa0(c,g)\\xa0correspond \\xa0to\\xa0Data\\xa0\\n3,\\xa0and\\xa0(d,h)\\xa0correspond \\xa0to\\xa0Data\\xa04;\\xa0(a–d)\\xa0are\\xa0extracted \\xa0by\\xa0the\\xa0developed \\xa0method\\xa0to\\xa0generate \\xa0\\nroad\\xa0networks, \\xa0and\\xa0(e–h)\\xa0are\\xa0ground\\xa0truths\\xa0that\\xa0are\\xa0manually \\xa0labeled.\\xa0The\\xa0evaluation \\xa0re‐\\nsults\\xa0are\\xa0shown\\xa0in\\xa0Table\\xa01.\\xa0These\\xa0extraction \\xa0results\\xa0have\\xa0high\\xa0accuracy \\xa0and\\xa0precision \\xa0of\\xa0\\nData\\xa01–3,\\xa0showing \\xa0that\\xa0the\\xa0method\\xa0applies\\xa0well\\xa0in\\xa0extracting \\xa0roads\\xa0from\\xa0images\\xa0at\\xa0differ‐\\nent\\xa0resolutions. \\xa0Among\\xa0the\\xa0four\\xa0data,\\xa0Data\\xa01\\xa0was\\xa0located\\xa0in\\xa0a\\xa0rural\\xa0area,\\xa0with\\xa0the\\xa0simplest\\xa0\\nroad\\xa0conditions \\xa0and\\xa0the\\xa0highest\\xa0resolution. \\xa0Therefore, \\xa0the\\xa0extraction \\xa0result\\xa0is\\xa0very\\xa0similar\\xa0\\nto\\xa0the\\xa0ground\\xa0truth\\xa0(Figure\\xa016a,e).\\xa0Data\\xa02\\xa0is\\xa0more\\xa0complex\\xa0than\\xa0Data\\xa01,\\xa0but\\xa0each\\xa0road\\xa0is\\xa0\\nalso\\xa0extracted \\xa0relatively \\xa0completely \\xa0(Figure\\xa016b,f).\\xa0Because\\xa0of\\xa0many\\xa0details\\xa0in\\xa0Data\\xa02,\\xa0the\\xa0\\nrecall\\xa0rate\\xa0and\\xa0IoU\\xa0in\\xa0the\\xa0road\\xa0extraction \\xa0results\\xa0are\\xa0relatively \\xa0low\\xa0(Table\\xa01).\\xa0Data\\xa03\\xa0is\\xa0\\nlocated\\xa0in\\xa0the\\xa0city,\\xa0and\\xa0the\\xa0road\\xa0situation \\xa0is\\xa0the\\xa0most\\xa0complex. \\xa0The\\xa0results\\xa0of\\xa0Data\\xa03\\xa0have\\xa0\\nthe\\xa0highest\\xa0precision \\xa0but\\xa0the\\xa0lowest\\xa0accuracy \\xa0(Table\\xa01),\\xa0meaning \\xa0that\\xa0most\\xa0parts\\xa0of\\xa0roads\\xa0\\nare\\xa0identified \\xa0accurately \\xa0but\\xa0the\\xa0differences \\xa0between\\xa0roads\\xa0and\\xa0surroundings \\xa0are\\xa0not\\xa0dis‐\\ntinguished \\xa0well.\\xa0Data\\xa04\\xa0is\\xa0located\\xa0in\\xa0a\\xa0rural\\xa0residential \\xa0area\\xa0with\\xa0multiple\\xa0roads\\xa0of\\xa0varying\\xa0\\nlengths\\xa0and\\xa0widths.\\xa0The\\xa0proposed \\xa0method\\xa0can\\xa0extract\\xa0the\\xa0roads\\xa0in\\xa0the\\xa0image\\xa0very\\xa0well\\xa0by\\xa0\\nextracting \\xa0the\\xa0roads\\xa0by\\xa0grading,\\xa0that\\xa0is,\\xa0firstly\\xa0extracting \\xa0the\\xa0wide\\xa0roads,\\xa0and\\xa0then\\xa0extract‐\\ning\\xa0the\\xa0narrow\\xa0roads\\xa0(Figure\\xa016e,h).\\xa0However, \\xa0the\\xa0width\\xa0of\\xa0the\\xa0road\\xa0at\\xa0the\\xa0same\\xa0level\\xa0in\\xa0\\nthe\\xa0image\\xa0is\\xa0also\\xa0different, \\xa0which\\xa0makes\\xa0the\\xa0proposed \\xa0method\\xa0obtain\\xa0limited\\xa0extraction \\xa0\\nprecision \\xa0and\\xa0IoU\\xa0(Table\\xa01).\\xa0Moreover, \\xa0all\\xa0road\\xa0intersections \\xa0in\\xa0the\\xa0four\\xa0data\\xa0are\\xa0correctly \\xa0\\nconnected. \\xa0\\n\\xa0\\xa0 \\xa0\\xa0\\n(a)\\xa0 (b)\\xa0 (c)\\xa0 (d)\\xa0\\n\\xa0\\xa0 \\xa0 \\xa0\\n(e)\\xa0 (f)\\xa0 (g)\\xa0 (h)\\xa0\\nFigure\\xa016.\\xa0Comparison \\xa0between\\xa0the\\xa0road\\xa0extraction \\xa0results\\xa0and\\xa0the\\xa0ground\\xa0truth:\\xa0(a–d)\\xa0are\\xa0the\\xa0road\\xa0\\nextraction \\xa0results\\xa0and\\xa0(e–h)\\xa0are\\xa0the\\xa0ground\\xa0truth.\\xa0\\nTable\\xa01.\\xa0Evaluation \\xa0table\\xa0of\\xa0road\\xa0extraction. \\xa0\\nData\\xa0 Precision \\xa0 Accuracy \\xa0 Recall\\xa0 IoU\\xa0\\nData\\xa01\\xa0 88.54%\\xa0 99.70%\\xa0 88.97%\\xa0 0.80\\xa0\\nData\\xa02\\xa0 87.08%\\xa0 98.13%\\xa0 77.06%\\xa0 0.69\\xa0\\nData\\xa03\\xa0 98.10%\\xa0 88.31%\\xa0 88.68%\\xa0 0.87\\xa0\\nData\\xa04\\xa0 80.70%\\xa0 96.80%\\xa0 81.56%\\xa0 0.68\\xa0\\nFigure 16. Comparison between the road extraction results and the ground truth: ( a–d) are the road\\nextraction results and ( e–h) are the ground truth.\\nTable 1. Evaluation table of road extraction.\\nData Precision Accuracy Recall IoU\\nData 1 88.54% 99.70% 88.97% 0.80\\nData 2 87.08% 98.13% 77.06% 0.69\\nData 3 98.10% 88.31% 88.68% 0.87\\nData 4 80.70% 96.80% 81.56% 0.68\\nTen roads in Data 2 were randomly selected for statistical analysis of extraction time\\n(Table 2). Roads with a length of more than 4000 pixels can be extracted at one time in 7 s.\\nIn terms of extraction efﬁciency, the longer the road, the shorter the road extraction time\\nper 1000 pixels. The average extraction time of roads per 1000 pixels is 1.81 s.\\nTable 2. Evaluation table of road extraction efﬁciency.\\nNo. Road Length (pixel) Time (s) Time per 1000 pixels (s)\\n1 2040 4.125 2.02\\n2 2126 4.739 2.23\\n3 2378 4.64 1.95\\n4 2415 5.688 2.36\\n5 2770 6.776 2.45\\n6 4056 4.536 1.12\\n7 4064 6.43 1.58\\n8 4115 5.83 1.42\\n9 4121 6.095 1.48\\n10 4188 6.486 1.55\\nMean - - 1.81\\n3.3. Comparison with Other Existing Methods\\nWe make comparisons with Gu’s road extraction method [ 41] in this study. Gu’s\\nmethod is implemented in the following way: ﬁrst determine a seed point to obtain the\\ninitial contour through region growth (use the same threshold as the developed method),\\nand then use the GVF–Snake method to perform iterative optimization to obtain the road\\nboundary, where the number of iterations is 40. Gu’s method has many iterations, so itAppl. Sci. 2022 ,12, 4705 17 of 21\\nis only suitable for road extraction in local areas. Three local typical roads were selected\\nfor comparison. For each road segment about the length of 300 pixels, the method takes\\naround 2 s. This is less efﬁcient than the developed method (Table 2). Figure 16 shows the\\nextraction results of the two methods.\\nFor Sample 1 and Sample 2, the recall of Gu’s method is higher than ours. However,\\nthe precision and IoU of the developed method are all better than those of Gu’s method\\n(Table 3). As shown in Figure 17a,d, Gu’s method could distinguish wild country and\\nartiﬁcial structures exposed to the sun, including roads, which led to high recall. However,\\nit was easily inﬂuenced by the effect of ‘different objects with similar spectra’. Objects such\\nas concrete ground and bare soil were extracted as roads, leading to low precision, accuracy,\\nand IoU (Table 3). Because of the limited working area, the developed method could avoid\\nover-extraction (Figure 17b,e) and thus reach better precision, accuracy, and IoU (Table 3).\\nWhen shadows occlude some roads, such as those in Sample 3, Gu’s method could hardly\\nidentify these roads (Figure 16g), signiﬁcantly reducing its recall and IoU (Table 3). In\\nthe developed method, roads were thinned to centerlines and regenerated with a certain\\nwidth, which weakened the inﬂuence of shadows (Figure 17h) and achieved an acceptable\\nresult (Table 3). Furthermore, the series of operations including morphological operation\\nand vector simpliﬁcation in the developed method eliminated the burs and made the road\\nedge smoother.\\nTable 3. Performance of local road extraction by Gu’s method and the proposed method.\\nSample 1 Sample 2 Sample 3\\nGu’s Method Proposed Method Gu’s MethodProposed\\nMethodGu’s MethodProposed\\nMethod\\nPrecision 0.64 0.96 0.57 0.79 0.76 0.84\\nAccuracy 0.94 0.97 0.96 0.98 0.94 0.98\\nRecall 0.88 0.70 0.99 0.94 0.25 0.86\\nIoU 0.59 0.67 0.57 0.75 0.23 0.73\\nAppl.\\xa0Sci.\\xa02022,\\xa012,\\xa0x\\xa0FOR\\xa0PEER\\xa0REVIEW \\xa0 18\\xa0of\\xa022\\xa0\\n\\xa0\\n\\xa0\\nFigure\\xa017.\\xa0Comparison \\xa0with\\xa0Gu’s\\xa0method:\\xa0(a,d,g)\\xa0are\\xa0the\\xa0results\\xa0of\\xa0Gu’s\\xa0method;\\xa0(b,e,h)\\xa0are\\xa0the\\xa0\\nresults\\xa0of\\xa0the\\xa0proposed \\xa0method;\\xa0(c,f,i)\\xa0are\\xa0the\\xa0ground\\xa0truth.\\xa0\\nTable\\xa03.\\xa0Performance \\xa0of\\xa0local\\xa0road\\xa0extraction \\xa0by\\xa0Gu’s\\xa0method\\xa0and\\xa0the\\xa0proposed \\xa0method.\\xa0\\n\\xa0 Sample\\xa01\\xa0 Sample\\xa02\\xa0 Sample\\xa03\\xa0\\n\\xa0 Gu’s\\xa0Method\\xa0Proposed \\xa0Method\\xa0Gu’s\\xa0Method\\xa0Proposed \\xa0Method\\xa0Gu’s\\xa0Method\\xa0Proposed \\xa0Method\\xa0\\nPrecision \\xa00.64\\xa0 0.96\\xa0 0.57\\xa0 0.79\\xa0 0.76\\xa0 0.84\\xa0\\nAccuracy \\xa00.94\\xa0 0.97\\xa0 0.96\\xa0 0.98\\xa0 0.94\\xa0 0.98\\xa0\\nRecall\\xa0 0.88\\xa0 0.70\\xa0 0.99\\xa0 0.94\\xa0 0.25\\xa0 0.86\\xa0\\nIoU\\xa0 0.59\\xa0 0.67\\xa0 0.57\\xa0 0.75\\xa0 0.23\\xa0 0.73\\xa0\\n4.\\xa0Discussion \\xa0\\nAfter\\xa0applying \\xa0the\\xa0method\\xa0in\\xa0four\\xa0different \\xa0tested\\xa0images,\\xa0the\\xa0results\\xa0all\\xa0have\\xa0high\\xa0\\naccuracy \\xa0(Table\\xa01).\\xa0For\\xa0simple\\xa0roads\\xa0in\\xa0rural\\xa0areas,\\xa0such\\xa0as\\xa0Data\\xa01,\\xa0the\\xa0proposed \\xa0method\\xa0\\nhas\\xa0high\\xa0precision, \\xa0accuracy, \\xa0recall,\\xa0and\\xa0IoU\\xa0(Table\\xa01).\\xa0For\\xa0suburban \\xa0roads\\xa0such\\xa0as\\xa0Data\\xa02,\\xa0\\nthe\\xa0proposed \\xa0method\\xa0can\\xa0accurately \\xa0extract\\xa0the\\xa0main\\xa0roads\\xa0and\\xa0obtain\\xa0high\\xa0precision \\xa0and\\xa0\\naccuracy \\xa0(Table\\xa01).\\xa0For\\xa0urban\\xa0areas\\xa0such\\xa0as\\xa0Data\\xa03\\xa0and\\xa0rural\\xa0residential \\xa0areas\\xa0in\\xa0Data\\xa04,\\xa0\\nthe\\xa0method\\xa0is\\xa0robust\\xa0to\\xa0shadows \\xa0and\\xa0still\\xa0achieves \\xa0good\\xa0extraction \\xa0results\\xa0in\\xa0areas\\xa0with\\xa0\\nbuildings \\xa0(Table\\xa01,\\xa0Figure\\xa016).\\xa0In\\xa0terms\\xa0of\\xa0extraction \\xa0efficiency, \\xa0the\\xa0longer\\xa0the\\xa0road,\\xa0the\\xa0\\nshorter\\xa0the\\xa0road\\xa0extraction \\xa0time\\xa0per\\xa01000\\xa0pixels\\xa0(Table\\xa02).\\xa0This\\xa0is\\xa0mainly\\xa0because\\xa0most\\xa0of\\xa0\\nthe\\xa0time\\xa0consumption \\xa0in\\xa0this\\xa0method\\xa0focuses\\xa0on\\xa0image\\xa0processing \\xa0at\\xa0the\\xa0grid\\xa0level,\\xa0while\\xa0\\nthe\\xa0algorithm \\xa0at\\xa0the\\xa0vector\\xa0level\\xa0is\\xa0very\\xa0fast.\\xa0The\\xa0longer\\xa0the\\xa0road,\\xa0the\\xa0higher\\xa0the\\xa0extraction \\xa0\\nefficiency—therefore, \\xa0it\\xa0is\\xa0very\\xa0suitable\\xa0for\\xa0large‐area\\xa0road\\xa0extraction, \\xa0especially \\xa0for\\xa0large\\xa0\\nlabeled\\xa0data\\xa0required \\xa0for\\xa0deep\\xa0learning. \\xa0\\nThe\\xa0recall\\xa0rate\\xa0and\\xa0IoU\\xa0in\\xa0the\\xa0road\\xa0extraction \\xa0results\\xa0for\\xa0Data\\xa02\\xa0and\\xa0Data\\xa04\\xa0are\\xa0rela‐\\ntively\\xa0low\\xa0(Table\\xa01).\\xa0For\\xa0Data\\xa02,\\xa0the\\xa0reason\\xa0is\\xa0mainly\\xa0that\\xa0shorter\\xa0roads\\xa0were\\xa0mistakenly \\xa0\\ndeleted\\xa0during\\xa0vectorization \\xa0and\\xa0subsequent \\xa0optimization. \\xa0Moreover, \\xa0this\\xa0method\\xa0mainly\\xa0\\nFigure 17. Comparison with Gu’s method: ( a,d,g) are the results of Gu’s method; ( b,e,h) are the\\nresults of the proposed method; ( c,f,i) are the ground truth.Appl. Sci. 2022 ,12, 4705 18 of 21\\n4. Discussion\\nAfter applying the method in four different tested images, the results all have high\\naccuracy (Table 1). For simple roads in rural areas, such as Data 1, the proposed method\\nhas high precision, accuracy, recall, and IoU (Table 1). For suburban roads such as Data 2,\\nthe proposed method can accurately extract the main roads and obtain high precision and\\naccuracy (Table 1). For urban areas such as Data 3 and rural residential areas in Data 4,\\nthe method is robust to shadows and still achieves good extraction results in areas with\\nbuildings (Table 1, Figure 16). In terms of extraction efﬁciency, the longer the road, the\\nshorter the road extraction time per 1000 pixels (Table 2). This is mainly because most of\\nthe time consumption in this method focuses on image processing at the grid level, while\\nthe algorithm at the vector level is very fast. The longer the road, the higher the extraction\\nefﬁciency—therefore, it is very suitable for large-area road extraction, especially for large\\nlabeled data required for deep learning.\\nThe recall rate and IoU in the road extraction results for Data 2 and Data 4 are relatively\\nlow (Table 1). For Data 2, the reason is mainly that shorter roads were mistakenly deleted\\nduring vectorization and subsequent optimization. Moreover, this method mainly focuses\\non the optimization of road centerlines and the generation of road networks. Hence, as\\nshown in Figure 18, auxiliary roads are not considered here. This is why the recall and IoU\\nare relatively low. For Data 4, the width of the same level road is different, which is the\\nmain reason for the limited precision and IoU.\\nAppl.\\xa0Sci.\\xa02022,\\xa012,\\xa0x\\xa0FOR\\xa0PEER\\xa0REVIEW \\xa0 19\\xa0of\\xa022\\xa0\\n\\xa0\\nfocuses\\xa0on\\xa0the\\xa0optimization \\xa0of\\xa0road\\xa0centerlines \\xa0and\\xa0the\\xa0generation \\xa0of\\xa0road\\xa0networks. \\xa0\\nHence,\\xa0as\\xa0shown\\xa0in\\xa0Figure\\xa018,\\xa0auxiliary \\xa0roads\\xa0are\\xa0not\\xa0considered \\xa0here.\\xa0This\\xa0is\\xa0why\\xa0the\\xa0\\nrecall\\xa0and\\xa0IoU\\xa0are\\xa0relatively \\xa0low.\\xa0For\\xa0Data\\xa04,\\xa0the\\xa0width\\xa0of\\xa0the\\xa0same\\xa0level\\xa0road\\xa0is\\xa0different, \\xa0\\nwhich\\xa0is\\xa0the\\xa0main\\xa0reason\\xa0for\\xa0the\\xa0limited\\xa0precision \\xa0and\\xa0IoU.\\xa0\\n\\xa0\\xa0\\xa0\\n(a)\\xa0 (b)\\xa0 (c)\\xa0\\nFigure\\xa018.\\xa0Comparison \\xa0of\\xa0intersections \\xa0with\\xa0auxiliary \\xa0roads:\\xa0(a)\\xa0original\\xa0image;\\xa0(b)\\xa0extraction \\xa0re‐\\nsult;\\xa0(c)\\xa0ground\\xa0truth.\\xa0\\nWe\\xa0selected\\xa0four\\xa0tested\\xa0images\\xa0with\\xa0different \\xa0resolutions \\xa0and\\xa0obtained \\xa0them\\xa0from\\xa0\\ndifferent \\xa0remote‐sensing\\xa0platforms. \\xa0The\\xa0experiments \\xa0on\\xa0rural,\\xa0suburban, \\xa0urban,\\xa0and\\xa0rural\\xa0\\nresidential \\xa0area\\xa0images\\xa0proved\\xa0the\\xa0universality \\xa0of\\xa0the\\xa0proposed \\xa0method.\\xa0Compared \\xa0with\\xa0\\nthe\\xa0existing\\xa0Gu’s\\xa0methods, \\xa0the\\xa0proposed \\xa0method\\xa0also\\xa0showed\\xa0better\\xa0performance \\xa0(Figure\\xa0\\n17,\\xa0Table\\xa03).\\xa0This\\xa0can\\xa0provide\\xa0an\\xa0accurate\\xa0and\\xa0complete \\xa0way\\xa0to\\xa0extract\\xa0roads\\xa0at\\xa0different \\xa0\\nscales,\\xa0especially \\xa0beneficial \\xa0for\\xa0the\\xa0remote‐sensing\\xa0images\\xa0of\\xa0some\\xa0areas\\xa0with\\xa0shadows \\xa0\\nand\\xa0intersections. \\xa0In\\xa0addition, \\xa0because\\xa0of\\xa0the\\xa0wide\\xa0universality, \\xa0the\\xa0proposed \\xa0method\\xa0has\\xa0\\ngreat\\xa0potential \\xa0in\\xa0data\\xa0labeling. \\xa0We\\xa0realize\\xa0that\\xa0the\\xa0proposed \\xa0method\\xa0reduces\\xa0the\\xa0shadow\\xa0\\neffect;\\xa0however, \\xa0this\\xa0proposed \\xa0method\\xa0still\\xa0does\\xa0not\\xa0eliminate \\xa0it\\xa0(Figure\\xa017b,e,h).\\xa0The\\xa0\\nmethod\\xa0also\\xa0ignores\\xa0auxiliary \\xa0roads\\xa0and\\xa0is\\xa0less\\xa0effective\\xa0when\\xa0road\\xa0widths\\xa0are\\xa0incon‐\\nsistent,\\xa0so\\xa0further\\xa0research\\xa0efforts\\xa0will\\xa0be\\xa0focused\\xa0on\\xa0refining\\xa0the\\xa0developed \\xa0method.\\xa0\\n5.\\xa0Conclusions \\xa0\\nIn\\xa0this\\xa0study,\\xa0a\\xa0full‐flow\\xa0processing \\xa0strategy\\xa0including \\xa0all\\xa0steps\\xa0from\\xa0road\\xa0extraction \\xa0\\nto\\xa0road\\xa0network\\xa0generation \\xa0is\\xa0proposed, \\xa0aiming\\xa0at\\xa0improving \\xa0the\\xa0efficiency \\xa0of\\xa0road\\xa0net‐\\nwork\\xa0extraction \\xa0and\\xa0data\\xa0labeling. \\xa0To\\xa0this\\xa0end,\\xa0a\\xa0new\\xa0framework \\xa0with\\xa0two\\xa0main\\xa0steps—\\nsingle‐road\\xa0extraction \\xa0and\\xa0road\\xa0network\\xa0generation—is \\xa0constructed \\xa0by\\xa0integrating \\xa0vari‐\\nous\\xa0algorithms. \\xa0Among\\xa0these\\xa0algorithms, \\xa0the\\xa0implementation \\xa0of\\xa0new\\xa0algorithms \\xa0such\\xa0as\\xa0\\nendpoint \\xa0modifications, \\xa0road\\xa0connections, \\xa0and\\xa0road\\xa0network\\xa0generation \\xa0algorithms \\xa0was\\xa0\\ncrucial\\xa0for\\xa0establishing \\xa0the\\xa0whole\\xa0road‐extraction \\xa0workflow. \\xa0Four\\xa0high‐resolution \\xa0images\\xa0\\nwith\\xa0different \\xa0terrains\\xa0and\\xa0resolutions \\xa0are\\xa0used\\xa0to\\xa0validate\\xa0the\\xa0proposed \\xa0framework, \\xa0and\\xa0\\nthe\\xa0results\\xa0show\\xa0that\\xa0the\\xa0strategy\\xa0greatly\\xa0improves \\xa0the\\xa0road\\xa0network\\xa0extraction \\xa0effect.\\xa0It\\xa0\\nhas\\xa0good\\xa0accuracy \\xa0and\\xa0universality \\xa0and\\xa0can\\xa0be\\xa0used\\xa0to\\xa0perform\\xa0road\\xa0extraction \\xa0and\\xa0road\\xa0\\nnetwork\\xa0update\\xa0with\\xa0high‐resolution \\xa0remote‐sensing\\xa0images.\\xa0The\\xa0evaluations \\xa0of\\xa0the\\xa0ex‐\\ntraction\\xa0results\\xa0of\\xa0the\\xa0four\\xa0images\\xa0show\\xa0that\\xa0the\\xa0road\\xa0precision \\xa0and\\xa0IoU\\xa0both\\xa0reach\\xa0a\\xa0high\\xa0\\nlevel.\\xa0At\\xa0the\\xa0same\\xa0time,\\xa0the\\xa0developed \\xa0method\\xa0has\\xa0better\\xa0precision \\xa0and\\xa0faster\\xa0speed\\xa0than\\xa0\\nthe\\xa0semi‐automatic \\xa0method.\\xa0Additionally, \\xa0because\\xa0of\\xa0the\\xa0wide\\xa0universality, \\xa0the\\xa0proposed \\xa0\\nmethod\\xa0has\\xa0great\\xa0potential \\xa0in\\xa0data\\xa0labeling. \\xa0Lastly,\\xa0experiments \\xa0also\\xa0show\\xa0that\\xa0this\\xa0strat‐\\negy\\xa0does\\xa0not\\xa0consider \\xa0some\\xa0special\\xa0road\\xa0types\\xa0and\\xa0may\\xa0miss\\xa0extraction \\xa0of\\xa0shorter\\xa0roads,\\xa0\\nwhich\\xa0deserve\\xa0more\\xa0attention \\xa0in\\xa0future\\xa0work.\\xa0\\nAuthor\\xa0Contributions: \\xa0Conceptualization, \\xa0W.C.\\xa0and\\xa0K.Y.;\\xa0methodology, \\xa0K.Y.\\xa0and\\xa0W.C.;\\xa0software, \\xa0\\nK.Y.\\xa0and\\xa0S.S.;\\xa0investigation, \\xa0K.Y.,\\xa0Y.L.\\xa0(Yuanjin \\xa0Li),\\xa0and\\xa0Y.L.\\xa0(Yu\\xa0Liu);\\xa0writing,\\xa0K.Y.\\xa0and\\xa0W.C.;\\xa0writ‐\\ning—review \\xa0and\\xa0editing,\\xa0W.C.,\\xa0K.Y.,\\xa0and\\xa0M.G.;\\xa0funding\\xa0acquisition, \\xa0W.C.\\xa0All\\xa0authors\\xa0have\\xa0read\\xa0and\\xa0\\nagreed\\xa0to\\xa0the\\xa0published \\xa0version\\xa0of\\xa0the\\xa0manuscript. \\xa0\\nFigure 18. Comparison of intersections with auxiliary roads: ( a) original image; ( b) extraction result;\\n(c) ground truth.\\nWe selected four tested images with different resolutions and obtained them from\\ndifferent remote-sensing platforms. The experiments on rural, suburban, urban, and rural\\nresidential area images proved the universality of the proposed method. Compared with the\\nexisting Gu’s methods, the proposed method also showed better performance (Figure 17,\\nTable 3). This can provide an accurate and complete way to extract roads at different\\nscales, especially beneﬁcial for the remote-sensing images of some areas with shadows and\\nintersections. In addition, because of the wide universality, the proposed method has great\\npotential in data labeling. We realize that the proposed method reduces the shadow effect;\\nhowever, this proposed method still does not eliminate it (Figure 17b,e,h). The method also\\nignores auxiliary roads and is less effective when road widths are inconsistent, so further\\nresearch efforts will be focused on reﬁning the developed method.\\n5. Conclusions\\nIn this study, a full-ﬂow processing strategy including all steps from road extrac-\\ntion to road network generation is proposed, aiming at improving the efﬁciency of road\\nnetwork extraction and data labeling. To this end, a new framework with two main\\nsteps—single-road extraction and road network generation—is constructed by integrating\\nvarious algorithms. Among these algorithms, the implementation of new algorithms such\\nas endpoint modiﬁcations, road connections, and road network generation algorithms was\\ncrucial for establishing the whole road-extraction workﬂow. Four high-resolution imagesAppl. Sci. 2022 ,12, 4705 19 of 21\\nwith different terrains and resolutions are used to validate the proposed framework, and\\nthe results show that the strategy greatly improves the road network extraction effect.\\nIt has good accuracy and universality and can be used to perform road extraction and\\nroad network update with high-resolution remote-sensing images. The evaluations of the\\nextraction results of the four images show that the road precision and IoU both reach a high\\nlevel. At the same time, the developed method has better precision and faster speed than\\nthe semi-automatic method. Additionally, because of the wide universality, the proposed\\nmethod has great potential in data labeling. Lastly, experiments also show that this strategy\\ndoes not consider some special road types and may miss extraction of shorter roads, which\\ndeserve more attention in future work.\\nAuthor Contributions: Conceptualization, W.C. and K.Y.; methodology, K.Y. and W.C.; software,\\nK.Y. and S.S.; investigation, K.Y., Y.L. (Yuanjin Li) and Y.L. (Yu Liu); writing, K.Y. and W.C.; writing—\\nreview and editing, W.C., K.Y. and M.G.; funding acquisition, W.C. All authors have read and agreed\\nto the published version of the manuscript.\\nFunding: Research presented in this paper was funded by National Natural Science Foundation of\\nChina (NSFC: U2033216) and the Foundation of Key Laboratory of Aerospace Information Application\\nof CETC: No. SXX19629X060. The authors gratefully acknowledge this support.\\nInstitutional Review Board Statement: Not applicable.\\nInformed Consent Statement: Not applicable.\\nData Availability Statement: The aerial image data used to support the ﬁndings of this study were\\nsupplied by the High-Resolution Comprehensive Trafﬁc Remote-Sensing Application program under\\ngrant No. 07-Y30B10-9001-14/16, thus cannot be made freely available. Similarly, the GF-2 satellite\\nimage data used to support the ﬁndings of this study were provided by the Innovation Fund Project\\nof CETC key laboratory of aerospace information applications under grant No. SXX19629X060, and\\nso cannot be made freely available.\\nAcknowledgments: The authors are grateful to Rongrong Wu, Jia Li, Zhiwei Peng and Haoying Cui,\\nwho provided assistance and advice during various stages of this work.\\nConﬂicts of Interest: The authors declare no conﬂict of interest.\\nReferences\\n1. Zhu, Q.; Zhang, Y.; Wang, L.; Zhong, Y.; Guan, Q.; Lu, X.; Zhang, L.; Li, D. A global context-aware and batch-independent\\nnetwork for road extraction from VHR satellite imagery. ISPRS J. Photogramm. Remote Sens. 2021 ,175, 353–365. [CrossRef]\\n2. Gao, L.; Song, W.; Dai, J.; Chen, Y. Road extraction from high-resolution remote sensing imagery using reﬁned deep residual\\nconvolutional neural network. Remote Sens. 2019 ,11, 552. [CrossRef]\\n3. Yang, X.; Li, X.; Ye, Y.; Lau, R.Y.K.; Zhang, X.; Huang, X. Road detection and centerline extraction via deep recurrent convolutional\\nneural network U-Net. IEEE Trans. Geosci. Remote Sens. 2019 ,57, 7209–7220. [CrossRef]\\n4. Wang, C.; Zourlidou, S.; Golze, J.; Sester, M. Trajectory analysis at intersections for trafﬁc rule identiﬁcation. Geo-Spat. Inf. Sci.\\n2020 ,24, 75–84. [CrossRef]\\n5. Gurung, P . Challenging infrastructural orthodoxies: Political and economic geographies of a Himalayan road. Geoforum 2021 ,120,\\n103–112. [CrossRef]\\n6. Alamgir, M.; Campbell, M.J.; Sloan, S.; Goosem, M.; Clements, G.R.; Mahmoud, M.I.; Laurance, W.F. Economic, socio-political\\nand environmental risks of road development in the tropics. Curr. Biol. 2017 ,27, 1130–1140. [CrossRef]\\n7. Qi, Y.; Chodron Drolma, S.; Zhang, X.; Liang, J.; Jiang, H.; Xu, J.; Ni, T. An investigation of the visual features of urban street\\nvitality using a convolutional neural network. Geo-Spat. Inf. Sci. 2020 ,23, 341–351. [CrossRef]\\n8. Metz, D. Economic beneﬁts of road widening: Discrepancy between outturn and forecast. Transp. Res. Part A Policy Pract. 2021 ,\\n147, 312–319. [CrossRef]\\n9. Chaudhuri, D.; Kushwaha, N.K.; Samal, A. Semi-Automated road detection from high resolution satellite images by directional\\nmorphological enhancement and segmentation techniques. IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 2012 ,5, 1538–1544.\\n[CrossRef]\\n10. Wang, J.; Qin, Q.; Gao, Z.; Zhao, J.; Ye, X. A new approach to urban road extraction using high-resolution aerial image. ISPRS Int.\\nJ. Geo-Inf. 2016 ,5, 114. [CrossRef]\\n11. Wang, T.; Du, L.; Yi, W.; Hong, J.; Zhang, L.; Zheng, J.; Li, C.; Ma, X.; Zhang, D.; Fang, W.; et al. An adaptive atmospheric\\ncorrection algorithm for the effective adjacency effect correction of submeter-scale spatial resolution optical satellite images:\\nApplication to a WorldView-3 panchromatic image. Remote Sens. Environ. 2021 ,259, 112412. [CrossRef]Appl. Sci. 2022 ,12, 4705 20 of 21\\n12. Máttyus, G.; Luo, W.; Urtasun, R. DeepRoadMapper: Extracting Road Topology from Aerial Images. In Proceedings of the 2017\\nIEEE International Conference on Computer Vision (ICCV), Venice, Italy, 22–29 October 2017; pp. 3458–3466. [CrossRef]\\n13. Zhao, J.Q.; Yang, J.; Li, P .X.; Lu, J.M. Semi-automatic Road Extraction from SAR Images Using EKF and PF. Int. Arch. Photogramm.\\nRemote Sens. Spat. Inf. Sci. 2015 ,XL-7/W4 , 227–230. [CrossRef]\\n14. Guan, H.; Lei, X.; Yu, Y.; Zhao, H.; Peng, D.; Marcato Junior, J.; Li, J. Road marking extraction in UAV imagery using attentive\\ncapsule feature pyramid network. Int. J. Appl. Earth Obs. 2022 ,107, 102677. [CrossRef]\\n15. Ekim, B.; Sertel, E.; Kabadayı, M.E. Automatic road extraction from historical maps using deep learning techniques: A regional\\ncase study of turkey in a German World War II Map. ISPRS Int. J. Geo-Inf. 2021 ,10, 492. [CrossRef]\\n16. Kuo, C.-L.; Tsai, M.-H. Road characteristics detection based on joint convolutional neural networks with adaptive squares. ISPRS\\nInt. J. Geo-Inf. 2021 ,10, 377. [CrossRef]\\n17. Yang, M.; Yuan, Y.; Liu, G. SDUNet: Road extraction via spatial enhanced and densely connected UNet. Pattern Recognit. 2022 ,\\n126, 108549. [CrossRef]\\n18. Zhang, X.; Ma, W.; Li, C.; Wu, J.; Tang, X.; Jiao, L. Fully Convolutional Network-Based Ensemble Method for Road Extraction\\nfrom Aerial Images. IEEE Geosci. Remote Sens. Lett. 2020 ,17, 1777–1781. [CrossRef]\\n19. Heipke, C.; Rottensteiner, F. Deep learning for geometric and semantic tasks in photogrammetry and remote sensing. Geo-Spat.\\nInf. Sci. 2020 ,23, 10–19. [CrossRef]\\n20. Wu, S.; Du, C.; Chen, H.; Xu, Y.; Guo, N.; Jing, N. Road extraction from very high resolution images using weakly labeled\\nopenstreetmap centerline. ISPRS Int. J. Geo-Inf. 2019 ,8, 478. [CrossRef]\\n21. Bakhtiari, H.R.R.; Abdollahi, A.; Rezaeian, H. Semi automatic road extraction from digital images. Egypt. J. Remote Sens. Space Sci.\\n2017 ,20, 117–123. [CrossRef]\\n22. Miao, Z.; Wang, B.; Shi, W.; Zhang, H. A semi-automatic method for road centerline extraction from VHR images. IEEE Geosci.\\nRemote Sens. Lett. 2014 ,11, 1856–1860. [CrossRef]\\n23. Nunes, D.M.; Medeiros, N.D.G.; Santos, A.D.P .D. Semi-automatic road network extraction from digital images using object-based\\nclassiﬁcation and morphological operators. Bol. Ci ênc. Geod. 2018 ,24, 485–502. [CrossRef]\\n24. Chen, L.; Rottensteiner, F.; Heipke, C. Feature detection and description for image matching: From hand-crafted design to deep\\nlearning. Geo-Spat. Inf. Sci. 2020 ,24, 58–74. [CrossRef]\\n25. Yuan, X.; Shi, J.; Gu, L. A review of deep learning methods for semantic segmentation of remote sensing imagery. Expert Syst.\\nAppl. 2021 ,169, 114417. [CrossRef]\\n26. Arya, D.; Maeda, H.; Ghosh, S.K.; Toshniwal, D.; Sekimoto, Y. RDD2020: An annotated image dataset for automatic road damage\\ndetection using deep learning. Data Brief 2021 ,36, 107133. [CrossRef]\\n27. Li, P .; He, X.; Qiao, M.; Miao, D.; Cheng, X.; Song, D.; Chen, M.; Li, J.; Zhou, T.; Guo, X.; et al. Exploring multiple crowdsourced\\ndata to learn deep convolutional neural networks for road extraction. Int. J. Appl. Earth Obs. 2021 ,104, 102544. [CrossRef]\\n28. Yu, J.; Yu, F.; Zhang, J.; Liu, Z. High resolution remote sensing image road extraction combining region growing and road-unit.\\nGeomat. Inf. Sci. Wuhan Univ. 2013 ,38, 761–764. [CrossRef]\\n29. Li, J.; Wen, Z.Q.; Hu, Y.X.; Liu, Z.D. Road Extraction from Remote Sensing Images Based on Improved Regional Growth. Comput.\\nEng. Appl. 2016 ,209–213 , 238. [CrossRef]\\n30. Wang, Z.; Yang, L.; Sheng, Y.; Shen, M. Pole-like Objects Segmentation and Multiscale Classiﬁcation-Based Fusion from Mobile\\nPoint Clouds in Road Scenes. Remote Sens. 2021 ,13, 4382. [CrossRef]\\n31. Cao, F.; Xu, Y.; Zhu, B.; Li, R. Semi-automatic road centerline extraction from high-resolution remote sensing by image utilizing\\ndynamic programming. J. Geomat. Sci. Technol. 2015 ,32, 615–618. [CrossRef]\\n32. Gruen, A.; Li, H. Road extraction from aerial and satellite images by dynamic programming. ISPRS J. Photogramm. Remote Sens.\\n1995 ,50, 11–20. [CrossRef]\\n33. Lian, R.; Wang, W.; Mustafa, N.; Huang, L. Road Extraction Methods in High-Resolution Remote Sensing Images: A Comprehen-\\nsive Review. IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 2020 ,13, 5489–5507. [CrossRef]\\n34. Ghandorh, H.; Boulila, W.; Masood, S.; Koubaa, A.; Ahmed, F.; Ahmad, J. Semantic Segmentation and Edge Detection—Approach\\nto Road Detection in Very High Resolution Satellite Images. Remote Sens. 2022 ,14, 613. [CrossRef]\\n35. Hormese, J.; Saravanan, C. Automated road extraction from high resolution satellite images. Procedia Technol. 2016 ,24, 1460–1467.\\n[CrossRef]\\n36. Xiao, Y.; Tan, T.S.; Tay, S.C. Utilizing Edge to Extract Roads in High-Resolution Satellite Imagery. In Proceedings of the IEEE\\nInternational Conference on Image Processing, Genova, Italy, 14 September 2005; Volume 1, p. I-637. [CrossRef]\\n37. Chen, G.; Sui, H.; Tu, J.; Song, Z. Semi-automatic road extraction method from high resolution remote sensing images based on\\nP-N learning. Geomat. Inf. Sci. Wuhan Univ. 2017 ,42, 775–781. [CrossRef]\\n38. Tan, H.; Shen, Z.; Dai, J. Semi-automatic extraction of rural roads under the constraint of combined geometric and texture features.\\nISPRS Int. J. Geo-Inf. 2021 ,10, 754. [CrossRef]\\n39. Wang, F.; Wang, W.; Xue, B.; Cao, T.; Gao, T. Road extraction from high-spatial-resolution remote sensing image by combining\\nGVF snake with salient features. Acta Geod. Cartogr. Sin. 2017 ,46, 1978–1985. [CrossRef]\\n40. Abdelfattah, R.; Chokmani, K. A Semi Automatic off-roads and Trails Extraction Method from Sentinel-1 Data. In Proceedings\\nof the 2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS), Fort Worth, TX, USA, 23–28 July 2017;\\npp. 3728–3731. [CrossRef]Appl. Sci. 2022 ,12, 4705 21 of 21\\n41. Gu, D.; Wang, X. Road extraction in remote sensing images based on region growing and GVF-Snake. Comput. Eng. Appl. 2010 ,\\n46, 202–205. [CrossRef]\\n42. Wei, Y.; Wang, Z.; Xu, M. Road structure reﬁned CNN for road extraction in aerial image. IEEE Geosci. Remote. Sens. Lett. 2017 ,14,\\n709–713. [CrossRef]\\n43. Wang, W.; Yang, N.; Zhang, Y.; Wang, F.; Cao, T.; Eklund, P . A review of road extraction from remote sensing images. J. Trafﬁc\\nTransp. Eng. 2016 ,3, 271–282. [CrossRef]\\n44. Wan, Y.; Wang, D.; Xiao, J.; Lai, X.; Xu, J. Automatic determination of seamlines for aerial image mosaicking based on vector\\nroads alone. ISPRS J. Photogramm. Remote Sens. 2013 ,76, 1–10. [CrossRef]\\n45. Mnih, V . Machine Learning for Aerial Image Labeling. Ph.D. Thesis, University of Toronto, Toronto, ON, Canda, 2013.\\n46. Demir, I.; Koperski, K.; Lindenbaum, D.; Pang, G.; Huang, J.; Basu, S.; Hughes, F.; Tuia, D.; Raskar, R. DeepGlobe 2018: A\\nChallenge to Parse the Earth through Satellite Images. In Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and\\nPattern Recognition Workshops (CVPRW), Salt Lake City, UT, USA, 18–22 June 2018; pp. 172–181.\\n47. Zhang, T.Y.; Suen, C.Y. A fast parallel algorithm for thinning digital patterns. Commun. ACM 1984 ,27, 236–239. [CrossRef]\\n48. Cao, X.; Liu, D.; Ren, X. Detection method for auto guide vehicle’s walking deviation based on image thinning and Hough\\ntransform. Meas. Control 2019 ,52, 252–261. [CrossRef]\\n49. Saalfeld, A. Topologically consistent line simpliﬁcation with the Douglas-Peucker algorithm. Cartogr. Geogr. Inf. Sci. 1999 ,26,\\n7–18. [CrossRef]\\n50. Henry, C.; Azimi, S.M.; Merkle, N. Road segmentation in SAR satellite images with deep fully convolutional neural networks.\\nIEEE Geosci. Remote Sens. Lett. 2018 ,15, 1867–1871. [CrossRef]\\n51. Abdollahi, A.; Pradhan, B.; Alamri, A. SC-RoadDeepNet: A New Shape and Connectivity-Preserving Road Extraction Deep\\nLearning-Based Network from Remote Sensing Data. IEEE Trans. Geosci. Remote Sens. 2022 ,60, 5617815. [CrossRef]']\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdfs(pdf_dir):\n",
    "    \"\"\"\n",
    "    Extract text from all PDF files in the specified directory.\n",
    "    \n",
    "    Args:\n",
    "        pdf_dir (str): Directory containing PDF files.\n",
    "        \n",
    "    Returns:\n",
    "        List of strings: Extracted text from each PDF file.\n",
    "    \"\"\"\n",
    "    text_data = []\n",
    "    for pdf_file in os.listdir(pdf_dir):\n",
    "        if pdf_file.endswith('.pdf'):\n",
    "            with open(os.path.join(pdf_dir, pdf_file), 'rb') as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                text = ''\n",
    "                for page in range(len(reader.pages)):  # Iterate over pages\n",
    "                    text += reader.pages[page].extract_text()  # Extract text from each page\n",
    "                text_data.append(text)\n",
    "                print(f\"Extracted from {pdf_file}: {text}\")  # Print text from each file\n",
    "    return text_data\n",
    "\n",
    "# Directory containing PDF files\n",
    "pdf_texts = extract_text_from_pdfs('data')  # Store the return value\n",
    "print(pdf_texts)  # Print the final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d974eb2e-72fb-4ca6-830a-a11ae29f39b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the knowledge base\n",
    "\n",
    "# Load pre-trained sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Encode the extracted text data\n",
    "embeddings = model.encode(pdf_texts)\n",
    "\n",
    "# Initialize FAISS index\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(embeddings)\n",
    "\n",
    "# Save the FAISS index to disk\n",
    "faiss.write_index(index, 'faiss_index.idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3188148b-9b67-43ef-8c0f-c23954a92189",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalAgent:\n",
    "    def __init__(self, index, model):\n",
    "        \"\"\"\n",
    "        Initialize the Retrieval Agent.\n",
    "        \n",
    "        Args:\n",
    "            index: FAISS index for document retrieval.\n",
    "            model: Sentence transformer model for encoding queries.\n",
    "        \"\"\"\n",
    "        self.index = index\n",
    "        self.model = model\n",
    "\n",
    "    def retrieve(self, query, top_k=5):\n",
    "        \"\"\"\n",
    "        Retrieve top-k relevant documents for a given query.\n",
    "        \n",
    "        Args:\n",
    "            query (str): User query.\n",
    "            top_k (int): Number of top documents to retrieve.\n",
    "            \n",
    "        Returns:\n",
    "            List of indices of the top-k relevant documents.\n",
    "        \"\"\"\n",
    "        query_embedding = self.model.encode([query])\n",
    "        distances, indices = self.index.search(query_embedding, top_k)\n",
    "        return indices\n",
    "\n",
    "class GenerationAgent:\n",
    "    def __init__(self, openai_api_key):\n",
    "        \"\"\"\n",
    "        Initialize the Generation Agent.\n",
    "        \n",
    "        Args:\n",
    "            openai_api_key (str): API key for OpenAI.\n",
    "        \"\"\"\n",
    "        self.openai_api_key = openai_api_key\n",
    "\n",
    "    def generate(self, prompt):\n",
    "        \"\"\"\n",
    "        Generate a response based on the given prompt using OpenAI.\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): Input prompt for generation.\n",
    "            \n",
    "        Returns:\n",
    "            str: Generated response.\n",
    "        \"\"\"\n",
    "        openai.api_key = self.openai_api_key\n",
    "        response = openai.Completion.create(\n",
    "            engine=\"davinci-codex\",\n",
    "            prompt=prompt,\n",
    "            max_tokens=150\n",
    "        )\n",
    "        return response.choices[0].text.strip()\n",
    "\n",
    "# Initialize agents\n",
    "retrieval_agent = RetrievalAgent(faiss.read_index('faiss_index.idx'), model)\n",
    "generation_agent = GenerationAgent('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "55123486-d233-4f46-a171-9f416a952f78",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Test the system with a sample query\u001b[39;00m\n\u001b[0;32m     24\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are the latest advancements in AI?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 25\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43manswer_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "Cell \u001b[1;32mIn[41], line 13\u001b[0m, in \u001b[0;36manswer_query\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Retrieve relevant documents\u001b[39;00m\n\u001b[0;32m     12\u001b[0m doc_indices \u001b[38;5;241m=\u001b[39m retrieval_agent\u001b[38;5;241m.\u001b[39mretrieve(query)\n\u001b[1;32m---> 13\u001b[0m relevant_docs \u001b[38;5;241m=\u001b[39m [pdf_texts[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m doc_indices]\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Combine documents into a single prompt\u001b[39;00m\n\u001b[0;32m     16\u001b[0m combined_docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(relevant_docs)\n",
      "Cell \u001b[1;32mIn[41], line 13\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Retrieve relevant documents\u001b[39;00m\n\u001b[0;32m     12\u001b[0m doc_indices \u001b[38;5;241m=\u001b[39m retrieval_agent\u001b[38;5;241m.\u001b[39mretrieve(query)\n\u001b[1;32m---> 13\u001b[0m relevant_docs \u001b[38;5;241m=\u001b[39m [\u001b[43mpdf_texts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m doc_indices]\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Combine documents into a single prompt\u001b[39;00m\n\u001b[0;32m     16\u001b[0m combined_docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(relevant_docs)\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "def answer_query(query):\n",
    "    \"\"\"\n",
    "    Answer a user query by retrieving relevant documents and generating a response.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query.\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated answer.\n",
    "    \"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    doc_indices = retrieval_agent.retrieve(query)\n",
    "    relevant_docs = [pdf_texts[i] for i in doc_indices]\n",
    "\n",
    "    # Combine documents into a single prompt\n",
    "    combined_docs = \"\\n\\n\".join(relevant_docs)\n",
    "    prompt = f\"Answer the following query based on these documents:\\n\\n{query}\\n\\n{combined_docs}\"\n",
    "\n",
    "    # Generate answer\n",
    "    answer = generation_agent.generate(prompt)\n",
    "    return answer\n",
    "\n",
    "# Test the system with a sample query\n",
    "query = \"What are the latest advancements in AI?\"\n",
    "response = answer_query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9005d728-126e-47a8-8fb2-d6f1d3f28e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamlit app title\n",
    "st.title(\"AI Research Assistant\")\n",
    "\n",
    "# Input field for user query\n",
    "query = st.text_input(\"Enter your research query:\")\n",
    "\n",
    "# Display the generated response when a query is entered\n",
    "if query:\n",
    "    response = answer_query(query)\n",
    "    st.write(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "llm-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
